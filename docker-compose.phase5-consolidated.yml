# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# K2 Market Data Platform - Phase 5 Consolidated Stack
# Purpose: Complete platform for live data ingestion + Iceberg offload testing
# Version: v2.0-phase5
# Last Updated: 2026-02-11
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

#############################################################################
# NETWORKS
#############################################################################
networks:
  k2-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

#############################################################################
# VOLUMES
#############################################################################
volumes:
  # Streaming
  redpanda-data:
    driver: local

  # Storage
  clickhouse-data:
    driver: local
  clickhouse-logs:
    driver: local

  # Orchestration
  postgres-data:
    driver: local

  # Object Storage
  minio-data:
    driver: local

  # Iceberg
  iceberg-warehouse:
    driver: local

#############################################################################
# SERVICES - STREAMING BACKBONE
#############################################################################
services:

  # ───────────────────────────────────────────────────────────────────────
  # Redpanda - Kafka-compatible streaming platform
  # ───────────────────────────────────────────────────────────────────────
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:v25.3.4
    container_name: k2-redpanda
    hostname: redpanda
    networks:
      k2-net:
        ipv4_address: 172.28.0.10
    ports:
      - "9092:9092"     # Kafka API
      - "9644:9644"     # Admin API
      - "8081:8081"     # Schema Registry
    volumes:
      - redpanda-data:/var/lib/redpanda/data
    command:
      - redpanda
      - start
      - --kafka-addr=internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr=internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr=internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr=internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr=internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr=redpanda:33145
      - --advertise-rpc-addr=redpanda:33145
      - --mode=dev-container
      - --smp=1
      - --memory=1500M
    environment:
      REDPANDA_PANDAPROXY_CLIENT_RETRIES: 5
      REDPANDA_PANDAPROXY_CLIENT_RETRY_BASE_BACKOFF_MS: 100
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -q 'Healthy'"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "streaming"
      com.k2.tier: "hot"

#############################################################################
# SERVICES - STORAGE LAYER
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # ClickHouse - OLAP database for hot tier
  # ───────────────────────────────────────────────────────────────────────
  clickhouse:
    image: clickhouse/clickhouse-server:24.1-alpine
    container_name: k2-clickhouse
    hostname: clickhouse
    networks:
      k2-net:
        ipv4_address: 172.28.0.20
    ports:
      - "8123:8123"     # HTTP interface
      - "9002:9000"     # Native protocol (avoid MinIO port conflict)
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - clickhouse-logs:/var/log/clickhouse-server
      - ./docker/clickhouse/ddl:/docker-entrypoint-initdb.d:ro
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "storage"
      com.k2.tier: "hot"

  # ───────────────────────────────────────────────────────────────────────
  # MinIO - S3-compatible object storage for Iceberg
  # ───────────────────────────────────────────────────────────────────────
  minio:
    image: minio/minio:RELEASE.2024-01-18T22-51-28Z
    container_name: k2-minio
    hostname: minio
    networks:
      k2-net:
        ipv4_address: 172.28.0.21
    ports:
      - "9000:9000"     # API
      - "9001:9001"     # Console
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    labels:
      com.k2.service: "storage"
      com.k2.tier: "cold"

#############################################################################
# SERVICES - ORCHESTRATION LAYER
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # PostgreSQL - Prefect metadata + watermarks
  # ───────────────────────────────────────────────────────────────────────
  prefect-db:
    image: postgres:15-alpine
    container_name: k2-prefect-db
    hostname: prefect-db
    networks:
      k2-net:
        ipv4_address: 172.28.0.30
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres/ddl:/docker-entrypoint-initdb.d:ro
    environment:
      POSTGRES_USER: prefect
      POSTGRES_PASSWORD: prefect
      POSTGRES_DB: prefect
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U prefect"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    labels:
      com.k2.service: "orchestration"
      com.k2.tier: "control-plane"

  # ───────────────────────────────────────────────────────────────────────
  # Prefect Server - Workflow orchestration UI/API
  # ───────────────────────────────────────────────────────────────────────
  prefect-server:
    image: prefecthq/prefect:2.14.9-python3.10
    container_name: k2-prefect-server
    hostname: prefect-server
    networks:
      k2-net:
        ipv4_address: 172.28.0.31
    ports:
      - "4200:4200"
    volumes:
      - ./docker/prefect:/root/.prefect
    environment:
      PREFECT_UI_API_URL: http://localhost:4200/api
      PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@prefect-db:5432/prefect
      PREFECT_SERVER_API_HOST: 0.0.0.0
      PREFECT_API_URL: http://prefect-server:4200/api
    command: prefect server start --host 0.0.0.0
    depends_on:
      prefect-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4200/api/health')"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "orchestration"
      com.k2.tier: "control-plane"

  # ───────────────────────────────────────────────────────────────────────
  # Prefect Agent - Workflow executor
  # ───────────────────────────────────────────────────────────────────────
  prefect-agent:
    image: prefecthq/prefect:2.14.9-python3.10
    container_name: k2-prefect-agent
    hostname: prefect-agent
    networks:
      k2-net:
        ipv4_address: 172.28.0.32
    volumes:
      - ./docker/prefect:/root/.prefect
    environment:
      PREFECT_API_URL: http://prefect-server:4200/api
    command: prefect agent start -q iceberg-offload
    depends_on:
      prefect-server:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped
    labels:
      com.k2.service: "orchestration"
      com.k2.tier: "control-plane"

#############################################################################
# SERVICES - ETL / OFFLOAD LAYER
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # Spark + Iceberg - ETL engine for cold tier offload
  # ───────────────────────────────────────────────────────────────────────
  spark-iceberg:
    image: tabulario/spark-iceberg:3.5.0_1.4.2
    container_name: k2-spark-iceberg
    hostname: spark-iceberg
    networks:
      k2-net:
        ipv4_address: 172.28.0.40
    ports:
      - "8080:8080"     # Spark Master UI
      - "4040:4040"     # Spark Application UI
      - "8888:8888"     # Jupyter (if needed)
    volumes:
      - iceberg-warehouse:/home/iceberg/warehouse
      - ./docker/iceberg/warehouse:/home/iceberg/warehouse
      - ./docker/offload:/home/iceberg/offload
      - ./docker/offload/flows:/home/iceberg/offload/flows
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_REGION=us-east-1
      - AWS_S3_ENDPOINT=http://minio:9000
    depends_on:
      - minio
      - clickhouse
      - prefect-db
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    restart: unless-stopped
    labels:
      com.k2.service: "etl"
      com.k2.tier: "cold"

#############################################################################
# SERVICES - FEED HANDLERS (DATA INGESTION)
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # Binance Feed Handler
  # ───────────────────────────────────────────────────────────────────────
  feed-handler-binance:
    build:
      context: .
      dockerfile: services/feed-handler-kotlin/Dockerfile
    container_name: k2-feed-handler-binance
    hostname: feed-handler-binance
    networks:
      k2-net:
        ipv4_address: 172.28.0.50
    depends_on:
      redpanda:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      # JVM settings
      - JAVA_OPTS=-Xmx512m -Xms256m
      # Feed handler configuration
      - K2_EXCHANGE=binance
      - K2_SYMBOLS=BTCUSDT,ETHUSDT,BNBUSDT
      - K2_SCHEMA_PATH=/app/schemas
      # Kafka/Redpanda configuration
      - K2_KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
      - K2_KAFKA_SCHEMA_REGISTRY_URL=http://redpanda:8081
    volumes:
      - ./logs/binance:/app/logs
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped
    labels:
      com.k2.service: "feed-handler"
      com.k2.exchange: "binance"

  # ───────────────────────────────────────────────────────────────────────
  # Kraken Feed Handler
  # ───────────────────────────────────────────────────────────────────────
  feed-handler-kraken:
    build:
      context: .
      dockerfile: services/feed-handler-kotlin/Dockerfile
    container_name: k2-feed-handler-kraken
    hostname: feed-handler-kraken
    networks:
      k2-net:
        ipv4_address: 172.28.0.51
    depends_on:
      redpanda:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      # JVM settings
      - JAVA_OPTS=-Xmx512m -Xms256m
      # Feed handler configuration
      - K2_EXCHANGE=kraken
      - K2_SYMBOLS=XBT/USD,ETH/USD
      - K2_SCHEMA_PATH=/app/schemas
      # Kafka/Redpanda configuration
      - K2_KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
      - K2_KAFKA_SCHEMA_REGISTRY_URL=http://redpanda:8081
    volumes:
      - ./logs/kraken:/app/logs
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped
    labels:
      com.k2.service: "feed-handler"
      com.k2.exchange: "kraken"

#############################################################################
# RESOURCE SUMMARY
#############################################################################
# Service                  | CPU Limit | Memory Limit | Purpose
# -------------------------|-----------|--------------|---------------------------
# redpanda                 | 2.0       | 2G           | Message streaming
# clickhouse               | 2.0       | 4G           | Hot tier storage
# minio                    | 1.0       | 1G           | S3-compatible storage
# prefect-db               | 1.0       | 1G           | Metadata + watermarks
# prefect-server           | 1.0       | 1G           | Orchestration UI/API
# prefect-agent            | 0.5       | 512M         | Workflow executor
# spark-iceberg            | 2.0       | 4G           | ETL engine
# feed-handler-binance     | 0.5       | 512M         | Binance ingestion
# feed-handler-kraken      | 0.5       | 512M         | Kraken ingestion
# -------------------------|-----------|--------------|---------------------------
# TOTAL                    | 11.5 CPU  | 14.5 GB      |
#############################################################################
