{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K2 Market Data Platform - Demo Notebook\n",
    "\n",
    "**K2 Platform** is a distributed market data processing platform designed for high-frequency trading environments.\n",
    "\n",
    "This notebook demonstrates:\n",
    "- Platform architecture and data flow\n",
    "- Sample data exploration (ASX trades)\n",
    "- Data ingestion pipeline\n",
    "- Query engine capabilities\n",
    "- Time-travel queries with Iceberg\n",
    "- REST API usage\n",
    "\n",
    "**Prerequisites**:\n",
    "- Docker services running (`make docker-up`)\n",
    "- Infrastructure initialized (`make init-infra`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                       K2 Platform Architecture                      │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  CSV Files → Kafka (Avro) → Iceberg → DuckDB → REST API            │\n",
    "│                                                                     │\n",
    "│  • CSV batch ingestion with schema validation                       │\n",
    "│  • Kafka streaming with at-least-once delivery                      │\n",
    "│  • Iceberg ACID transactions with time-travel                       │\n",
    "│  • DuckDB sub-second analytical queries                             │\n",
    "│  • FastAPI REST endpoints with OpenAPI docs                         │\n",
    "│                                                                     │\n",
    "│  Key Technologies:                                                  │\n",
    "│  • Apache Kafka + Schema Registry                                   │\n",
    "│  • Apache Iceberg (table format)                                    │\n",
    "│  • DuckDB (embedded analytics)                                      │\n",
    "│  • FastAPI + Prometheus + Grafana                                   │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from decimal import Decimal\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Add src to path for k2 imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 15)\n",
    "pd.set_option('display.width', 150)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SAMPLE_DATA_DIR = Path.cwd().parent / \"data\" / \"sample\"\n",
    "TRADES_DIR = SAMPLE_DATA_DIR / \"trades\"\n",
    "QUOTES_DIR = SAMPLE_DATA_DIR / \"quotes\"\n",
    "\n",
    "# Company ID to Symbol mapping\n",
    "COMPANY_MAPPING = {\n",
    "    \"7181\": {\"symbol\": \"DVN\", \"name\": \"Devine Ltd\"},\n",
    "    \"3153\": {\"symbol\": \"MWR\", \"name\": \"MGM Wireless\"},\n",
    "    \"7078\": {\"symbol\": \"BHP\", \"name\": \"BHP Billiton\"},\n",
    "    \"7458\": {\"symbol\": \"RIO\", \"name\": \"Rio Tinto\"},\n",
    "}\n",
    "\n",
    "# Verify sample data exists\n",
    "if SAMPLE_DATA_DIR.exists():\n",
    "    print(f\"Sample data directory: {SAMPLE_DATA_DIR}\")\n",
    "    print(f\"Available trade files: {list(TRADES_DIR.glob('*.csv'))}\")\n",
    "else:\n",
    "    print(\"Warning: Sample data not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sample Data Exploration\n",
    "\n",
    "The sample data contains real ASX (Australian Securities Exchange) market data from March 10-14, 2014.\n",
    "\n",
    "| Symbol | Company | Trades | Notes |\n",
    "|--------|---------|--------|-------|\n",
    "| DVN | Devine Ltd | 231 | Low volume, good for demos |\n",
    "| MWR | MGM Wireless | 10 | Very low volume |\n",
    "| BHP | BHP Billiton | 91,630 | High volume mining stock |\n",
    "| RIO | Rio Tinto | 108,670 | High volume mining stock |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DVN trades (small dataset for demo)\n",
    "dvn_file = TRADES_DIR / \"7181.csv\"\n",
    "\n",
    "# Sample data has no header, so we specify column names\n",
    "df_dvn = pd.read_csv(\n",
    "    dvn_file,\n",
    "    names=[\"Date\", \"Time\", \"Price\", \"Volume\", \"Qualifiers\", \"Venue\", \"BuyerID\"],\n",
    ")\n",
    "\n",
    "print(f\"DVN Trades: {len(df_dvn)} records\")\n",
    "print(f\"Date range: {df_dvn['Date'].iloc[0]} to {df_dvn['Date'].iloc[-1]}\")\n",
    "print(\"\\nFirst 10 trades:\")\n",
    "df_dvn.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data statistics\n",
    "print(\"=== DVN Trade Statistics ===\")\n",
    "print(f\"\\nPrice Range: ${df_dvn['Price'].min():.2f} - ${df_dvn['Price'].max():.2f}\")\n",
    "print(f\"Average Price: ${df_dvn['Price'].mean():.2f}\")\n",
    "print(f\"Total Volume: {df_dvn['Volume'].sum():,}\")\n",
    "print(f\"Average Trade Size: {df_dvn['Volume'].mean():,.0f}\")\n",
    "print(f\"\\nVenues: {df_dvn['Venue'].unique().tolist()}\")\n",
    "print(f\"Qualifiers: {df_dvn['Qualifiers'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse timestamps for time-series analysis\n",
    "def parse_timestamp(row):\n",
    "    \"\"\"Parse sample data timestamp format to datetime.\"\"\"\n",
    "    dt_str = f\"{row['Date']} {row['Time']}\"\n",
    "    try:\n",
    "        return datetime.strptime(dt_str, \"%m/%d/%Y %H:%M:%S.%f\")\n",
    "    except ValueError:\n",
    "        return datetime.strptime(dt_str, \"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "df_dvn['Timestamp'] = df_dvn.apply(parse_timestamp, axis=1)\n",
    "df_dvn['DateParsed'] = pd.to_datetime(df_dvn['Date'], format='%m/%d/%Y')\n",
    "\n",
    "print(\"Timestamps parsed successfully!\")\n",
    "df_dvn[['Date', 'Time', 'Timestamp', 'Price', 'Volume']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intraday price chart\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Price scatter plot\n",
    "ax1 = axes[0]\n",
    "scatter = ax1.scatter(\n",
    "    df_dvn['Timestamp'], \n",
    "    df_dvn['Price'], \n",
    "    c=df_dvn['Volume'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.7,\n",
    "    s=df_dvn['Volume'] / 1000,  # Size by volume\n",
    ")\n",
    "ax1.set_ylabel('Price ($)', fontsize=12)\n",
    "ax1.set_title('DVN Intraday Trades - March 10-14, 2014', fontsize=14)\n",
    "plt.colorbar(scatter, ax=ax1, label='Volume')\n",
    "\n",
    "# Volume bar chart\n",
    "ax2 = axes[1]\n",
    "ax2.bar(\n",
    "    df_dvn['Timestamp'], \n",
    "    df_dvn['Volume'],\n",
    "    width=0.001,\n",
    "    color='steelblue',\n",
    "    alpha=0.7,\n",
    ")\n",
    "ax2.set_ylabel('Volume', fontsize=12)\n",
    "ax2.set_xlabel('Time', fontsize=12)\n",
    "\n",
    "# Format x-axis\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%m/%d %H:%M'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily OHLCV summary\n",
    "daily_ohlcv = df_dvn.groupby('DateParsed').agg({\n",
    "    'Price': ['first', 'max', 'min', 'last', 'mean'],\n",
    "    'Volume': 'sum',\n",
    "    'Timestamp': 'count',\n",
    "}).round(2)\n",
    "\n",
    "daily_ohlcv.columns = ['Open', 'High', 'Low', 'Close', 'VWAP', 'Volume', 'Trades']\n",
    "\n",
    "# Calculate VWAP properly\n",
    "for date in daily_ohlcv.index:\n",
    "    day_data = df_dvn[df_dvn['DateParsed'] == date]\n",
    "    vwap = (day_data['Price'] * day_data['Volume']).sum() / day_data['Volume'].sum()\n",
    "    daily_ohlcv.loc[date, 'VWAP'] = round(vwap, 2)\n",
    "\n",
    "print(\"=== Daily OHLCV Summary ===\")\n",
    "daily_ohlcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Transformation for Ingestion\n",
    "\n",
    "The sample data format differs from our Avro schema. We need to transform:\n",
    "- Add `symbol` from company_id mapping\n",
    "- Combine `Date` + `Time` → `exchange_timestamp`\n",
    "- Generate `sequence_number` from row order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_sample_trades(company_id: str, limit: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Transform sample trade data to BatchLoader-compatible format.\"\"\"\n",
    "    csv_path = TRADES_DIR / f\"{company_id}.csv\"\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        csv_path,\n",
    "        names=[\"Date\", \"Time\", \"Price\", \"Volume\", \"Qualifiers\", \"Venue\", \"BuyerID\"],\n",
    "    )\n",
    "    \n",
    "    if limit:\n",
    "        df = df.head(limit)\n",
    "    \n",
    "    company_info = COMPANY_MAPPING[company_id]\n",
    "    \n",
    "    # Transform to schema format\n",
    "    result = pd.DataFrame({\n",
    "        \"symbol\": company_info[\"symbol\"],\n",
    "        \"company_id\": int(company_id),\n",
    "        \"exchange\": \"ASX\",\n",
    "        \"exchange_timestamp\": df.apply(\n",
    "            lambda row: parse_timestamp(row).isoformat(),\n",
    "            axis=1,\n",
    "        ),\n",
    "        \"price\": df[\"Price\"],\n",
    "        \"volume\": df[\"Volume\"],\n",
    "        \"qualifiers\": df[\"Qualifiers\"],\n",
    "        \"venue\": df[\"Venue\"].fillna(\"X\"),\n",
    "        \"buyer_id\": df[\"BuyerID\"].fillna(\"\"),\n",
    "        \"sequence_number\": range(1, len(df) + 1),\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Transform DVN data\n",
    "df_transformed = transform_sample_trades(\"7181\", limit=20)\n",
    "print(\"Transformed data (first 10 rows):\")\n",
    "df_transformed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation\n",
    "required_columns = [\n",
    "    \"symbol\", \"company_id\", \"exchange\", \"exchange_timestamp\",\n",
    "    \"price\", \"volume\", \"qualifiers\", \"venue\", \"sequence_number\",\n",
    "]\n",
    "\n",
    "print(\"Schema Validation:\")\n",
    "for col in required_columns:\n",
    "    status = \"\" if col in df_transformed.columns else \"\"\n",
    "    print(f\"  {status} {col}: {df_transformed[col].dtype if col in df_transformed.columns else 'MISSING'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query Engine Demo\n",
    "\n",
    "The K2 QueryEngine uses DuckDB with Iceberg extension to query market data.\n",
    "\n",
    "**Note**: This requires Docker services to be running and data loaded into Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import and connect to QueryEngine\n",
    "try:\n",
    "    from k2.query.engine import QueryEngine\n",
    "    \n",
    "    engine = QueryEngine()\n",
    "    print(\"QueryEngine connected successfully!\")\n",
    "    \n",
    "    # Get statistics\n",
    "    stats = engine.get_stats()\n",
    "    print(\"\\nDatabase Statistics:\")\n",
    "    for key, value in stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"QueryEngine not available: {e}\")\n",
    "    print(\"\\nTo use QueryEngine, run:\")\n",
    "    print(\"  make docker-up\")\n",
    "    print(\"  make init-infra\")\n",
    "    engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query trades if engine is available\n",
    "if engine:\n",
    "    symbols = engine.get_symbols()\n",
    "    print(f\"Available symbols: {symbols}\")\n",
    "    \n",
    "    if symbols:\n",
    "        # Query first symbol\n",
    "        symbol = symbols[0]\n",
    "        print(f\"\\nQuerying trades for {symbol}...\")\n",
    "        \n",
    "        trades = engine.query_trades(symbol=symbol, limit=10)\n",
    "        print(f\"Found {len(trades)} trades:\")\n",
    "        display(pd.DataFrame(trades))\n",
    "else:\n",
    "    print(\"Skipping query demo - QueryEngine not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Market summary example\n",
    "if engine:\n",
    "    symbols = engine.get_symbols()\n",
    "    if symbols:\n",
    "        symbol = symbols[0]\n",
    "        date_range = engine.get_date_range(symbol)\n",
    "        \n",
    "        if date_range and date_range.get('min_date'):\n",
    "            date = date_range['min_date'].strftime('%Y-%m-%d')\n",
    "            print(f\"Market Summary for {symbol} on {date}:\")\n",
    "            \n",
    "            summary = engine.get_market_summary(symbol, date)\n",
    "            for key, value in summary.items():\n",
    "                print(f\"  {key}: {value}\")\n",
    "else:\n",
    "    print(\"Skipping market summary - QueryEngine not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Time-Travel Demo\n",
    "\n",
    "Apache Iceberg maintains a snapshot history of tables. This enables:\n",
    "- Query historical data as it existed at any point\n",
    "- Audit changes over time\n",
    "- Rollback to previous states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import ReplayEngine\n",
    "try:\n",
    "    from k2.query.replay import ReplayEngine\n",
    "    \n",
    "    replay = ReplayEngine()\n",
    "    print(\"ReplayEngine connected successfully!\")\n",
    "    \n",
    "    # List snapshots\n",
    "    snapshots = replay.list_snapshots(table_type=\"trades\", limit=5)\n",
    "    print(f\"\\nFound {len(snapshots)} snapshots:\")\n",
    "    \n",
    "    if snapshots:\n",
    "        df_snapshots = pd.DataFrame(snapshots)\n",
    "        display(df_snapshots)\n",
    "    else:\n",
    "        print(\"No snapshots found (table may be empty)\")\n",
    "        \n",
    "    replay.close()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ReplayEngine not available: {e}\")\n",
    "    print(\"\\nTime-travel requires Iceberg tables with data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. REST API Demo\n",
    "\n",
    "The K2 API provides REST endpoints for querying market data.\n",
    "\n",
    "**Base URL**: http://localhost:8000\n",
    "\n",
    "**Endpoints**:\n",
    "- `GET /health` - Health check\n",
    "- `GET /v1/trades` - Query trades\n",
    "- `GET /v1/quotes` - Query quotes\n",
    "- `GET /v1/symbols` - List symbols\n",
    "- `GET /v1/summary/{symbol}/{date}` - OHLCV summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_BASE = \"http://localhost:8000\"\n",
    "API_KEY = \"k2-dev-api-key-2026\"\n",
    "HEADERS = {\"X-API-Key\": API_KEY}\n",
    "\n",
    "def api_get(endpoint, params=None, auth=True):\n",
    "    \"\"\"Make API GET request.\"\"\"\n",
    "    headers = HEADERS if auth else {}\n",
    "    try:\n",
    "        response = requests.get(f\"{API_BASE}{endpoint}\", headers=headers, params=params, timeout=5)\n",
    "        return response.status_code, response.json()\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return None, {\"error\": \"API not running. Start with: make api\"}\n",
    "    except Exception as e:\n",
    "        return None, {\"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health check (no auth required)\n",
    "status, data = api_get(\"/health\", auth=False)\n",
    "print(f\"Health Check: {status}\")\n",
    "print(f\"Response: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trades (requires auth)\n",
    "status, data = api_get(\"/v1/trades\", params={\"limit\": 5})\n",
    "print(f\"Trades Endpoint: {status}\")\n",
    "\n",
    "if status == 200 and \"data\" in data:\n",
    "    df_trades = pd.DataFrame(data[\"data\"])\n",
    "    display(df_trades)\n",
    "else:\n",
    "    print(f\"Response: {data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List symbols\n",
    "status, data = api_get(\"/v1/symbols\")\n",
    "print(f\"Symbols Endpoint: {status}\")\n",
    "\n",
    "if status == 200 and \"data\" in data:\n",
    "    print(f\"Available symbols: {data['data']}\")\n",
    "else:\n",
    "    print(f\"Response: {data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### What We Demonstrated\n",
    "\n",
    "| Component | Description | Status |\n",
    "|-----------|-------------|--------|\n",
    "| Sample Data | Real ASX market data (March 2014) | Explored |\n",
    "| Data Transformation | CSV → Avro schema format | Demonstrated |\n",
    "| Visualization | Intraday charts, OHLCV | Created |\n",
    "| Query Engine | DuckDB + Iceberg queries | Tested |\n",
    "| Time-Travel | Iceberg snapshots | Explored |\n",
    "| REST API | FastAPI endpoints | Tested |\n",
    "\n",
    "### Key Commands\n",
    "\n",
    "```bash\n",
    "make docker-up      # Start all services\n",
    "make init-infra     # Initialize Kafka topics and Iceberg tables\n",
    "make api            # Start REST API server\n",
    "k2-query --help     # Query CLI usage\n",
    "```\n",
    "\n",
    "### Links\n",
    "\n",
    "- **API Docs**: http://localhost:8000/docs\n",
    "- **Grafana**: http://localhost:3000 (admin/admin)\n",
    "- **Kafka UI**: http://localhost:8080\n",
    "- **MinIO Console**: http://localhost:9001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if 'engine' in dir() and engine:\n",
    "    engine.close()\n",
    "    print(\"QueryEngine closed.\")\n",
    "\n",
    "print(\"\\nDemo complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
