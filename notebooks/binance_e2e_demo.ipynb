{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# K2 Platform - Binance Live Streaming E2E Demo\n",
    "\n",
    "**Phase 2 Prep Achievement**: Complete Binance Cryptocurrency Streaming Pipeline\n",
    "\n",
    "This notebook demonstrates the complete end-to-end data pipeline for live cryptocurrency market data streaming, showcasing the achievements of Phase 2 Prep:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                  K2 Binance Streaming Architecture                  │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                     │\n",
    "│  Binance WebSocket → Kafka (Avro) → Consumer → Iceberg → Query     │\n",
    "│      (Live)            (Stream)      (Batch)    (ACID)    (DuckDB) │\n",
    "│                                                                     │\n",
    "│  • Real-time crypto trades (BTC, ETH, BNB)                          │\n",
    "│  • V2 hybrid schema with vendor_data map                            │\n",
    "│  • Multi-source capability (ASX batch + Binance streaming)          │\n",
    "│  • Multi-asset-class platform (equities + crypto)                   │\n",
    "│  • Production-grade resilience (SSL, metrics, circuit breakers)     │\n",
    "│  • 138 msg/s throughput, sub-second queries                         │\n",
    "│                                                                     │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## What This Demo Covers\n",
    "\n",
    "1. **Infrastructure Setup** - Docker services validation and initialization\n",
    "2. **Live Streaming** - Connect to Binance WebSocket, stream real trades\n",
    "3. **Kafka Pipeline** - Message broker with partitioning and Avro serialization\n",
    "4. **Consumer Processing** - Batch processing from Kafka to Iceberg (ACID)\n",
    "5. **Iceberg Lakehouse** - Query 5,000+ trades with DuckDB\n",
    "6. **Real-Time Visualizations** - Price charts, volume analysis, metrics\n",
    "7. **Data Quality** - Schema validation, sequence tracking, duplicate detection\n",
    "8. **Vendor Data** - Exchange-specific fields preserved in JSON\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to connect to live cryptocurrency exchanges via WebSocket\n",
    "- How to build a production-ready streaming data pipeline\n",
    "- How v2 hybrid schemas enable multi-source compatibility\n",
    "- How to achieve ACID guarantees with Apache Iceberg\n",
    "- How to query streaming data with sub-second latency\n",
    "- How to validate data quality in real-time systems\n",
    "\n",
    "**Estimated Time**: 30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 2. Prerequisites & Setup Overview\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "- ✓ **Docker** (with Docker Compose) - Running all infrastructure services\n",
    "- ✓ **Python 3.12+** - Managed via `uv` package manager\n",
    "- ✓ **4GB RAM minimum** - For Kafka, Iceberg, MinIO, PostgreSQL\n",
    "- ✓ **Internet connection** - For Binance WebSocket streaming\n",
    "- ✓ **Disk space** - ~2GB for Docker images and data\n",
    "\n",
    "### Docker Services Required\n",
    "\n",
    "| Service | Purpose | Port |\n",
    "|---------|---------|------|\n",
    "| Kafka | Message broker (KRaft mode) | 9092 |\n",
    "| Schema Registry | Avro schema management | 8081 |\n",
    "| MinIO | S3-compatible object storage | 9000, 9001 |\n",
    "| PostgreSQL | Iceberg catalog metadata | 5432 |\n",
    "| Iceberg REST Catalog | Table management | 8181 |\n",
    "| Prometheus | Metrics collection | 9090 |\n",
    "| Grafana | Dashboards and visualization | 3000 |\n",
    "\n",
    "### Python Dependencies\n",
    "\n",
    "All dependencies are managed via `uv` and installed automatically:\n",
    "- `confluent-kafka` - Kafka client with Avro support\n",
    "- `pyiceberg` - Iceberg Python SDK\n",
    "- `duckdb` - Embedded analytics engine\n",
    "- `pandas`, `matplotlib` - Data analysis and visualization\n",
    "- `websockets` - Async WebSocket client for Binance\n",
    "- `rich` - Beautiful terminal output\n",
    "\n",
    "### Network Requirements\n",
    "\n",
    "- Access to `wss://stream.binance.com:9443` (Binance WebSocket)\n",
    "- No VPN restrictions on cryptocurrency exchange APIs\n",
    "\n",
    "### Setup Time Estimate\n",
    "\n",
    "- **First time**: 10-15 minutes (Docker image download + init)\n",
    "- **Subsequent runs**: 2-3 minutes (start services + validation)\n",
    "\n",
    "### Quick Start Commands\n",
    "\n",
    "```bash\n",
    "# Start all Docker services\n",
    "docker compose up -d\n",
    "\n",
    "# Initialize infrastructure (schemas, topics, tables)\n",
    "python scripts/init_e2e_demo.py\n",
    "\n",
    "# Run this notebook!\n",
    "jupyter lab notebooks/binance_e2e_demo.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 3. Environment Setup & Validation\n",
    "\n",
    "Let's start by validating that all required services are running and healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "\n",
    "# Add src to path for k2 imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Rich console for beautiful output\n",
    "from rich.console import Console\n",
    "from rich.progress import Progress, SpinnerColumn, TextColumn\n",
    "from rich.table import Table\n",
    "\n",
    "console = Console()\n",
    "\n",
    "# Pandas display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "print(\"✓ Imports loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Docker services status\n",
    "console.print(\"\\n[bold cyan]Checking Docker services...[/bold cyan]\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=Path.cwd().parent,\n",
    "        timeout=10\n",
    "    )\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        console.print(\"[bold red]✗ Docker Compose is not running![/bold red]\")\n",
    "        console.print(\"\\nPlease start services with: [bold]docker compose up -d[/bold]\")\n",
    "        raise Exception(\"Docker services not running\")\n",
    "\n",
    "    # Parse Docker Compose output\n",
    "    import json\n",
    "    services = []\n",
    "    for line in result.stdout.strip().split('\\n'):\n",
    "        if line:\n",
    "            try:\n",
    "                service = json.loads(line)\n",
    "                services.append(service)\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "    # Create status table\n",
    "    table = Table(title=\"Docker Services Status\", show_header=True)\n",
    "    table.add_column(\"Service\", style=\"cyan\")\n",
    "    table.add_column(\"State\", style=\"green\")\n",
    "    table.add_column(\"Ports\", style=\"yellow\")\n",
    "\n",
    "    required_services = [\n",
    "        \"k2-kafka\", \"k2-schema-registry-1\", \"k2-minio\",\n",
    "        \"k2-postgres\", \"k2-iceberg-rest\", \"k2-prometheus\"\n",
    "    ]\n",
    "\n",
    "    running_services = {}\n",
    "    for service in services:\n",
    "        name = service.get(\"Name\", service.get(\"Service\", \"unknown\"))\n",
    "        state = service.get(\"State\", \"unknown\")\n",
    "        publishers = service.get(\"Publishers\", [])\n",
    "\n",
    "        # Format ports\n",
    "        ports = \", \".join([f\"{p.get('PublishedPort', '')}\" for p in publishers if p.get('PublishedPort')]) or \"N/A\"\n",
    "\n",
    "        running_services[name] = state\n",
    "\n",
    "        # Color based on state\n",
    "        state_style = \"green\" if state == \"running\" else \"red\"\n",
    "        table.add_row(name, f\"[{state_style}]{state}[/{state_style}]\", ports)\n",
    "\n",
    "    console.print(table)\n",
    "\n",
    "    # Check if all required services are running\n",
    "    missing_services = []\n",
    "    for svc in required_services:\n",
    "        if svc not in running_services or running_services[svc] != \"running\":\n",
    "            missing_services.append(svc)\n",
    "\n",
    "    if missing_services:\n",
    "        console.print(f\"\\n[bold red]✗ Missing or stopped services: {', '.join(missing_services)}[/bold red]\")\n",
    "        console.print(\"\\nPlease start services with: [bold]docker compose up -d[/bold]\")\n",
    "    else:\n",
    "        console.print(\"\\n[bold green]✓ All required services are running![/bold green]\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    console.print(\"[bold red]✗ Docker is not installed or not in PATH[/bold red]\")\n",
    "    console.print(\"\\nPlease install Docker: https://docs.docker.com/get-docker/\")\n",
    "except subprocess.TimeoutExpired:\n",
    "    console.print(\"[bold red]✗ Docker command timed out[/bold red]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error checking Docker: {e}[/bold red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate service health\n",
    "console.print(\"\\n[bold cyan]Validating service health...[/bold cyan]\\n\")\n",
    "\n",
    "import requests\n",
    "\n",
    "health_checks = []\n",
    "\n",
    "# 1. Check Schema Registry\n",
    "try:\n",
    "    start = time.time()\n",
    "    response = requests.get(\"http://localhost:8081/subjects\", timeout=5)\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        subjects = response.json()\n",
    "        health_checks.append({\n",
    "            \"service\": \"Schema Registry\",\n",
    "            \"status\": \"✓ Healthy\",\n",
    "            \"latency\": f\"{latency:.0f}ms\",\n",
    "            \"detail\": f\"{len(subjects)} schemas\"\n",
    "        })\n",
    "    else:\n",
    "        health_checks.append({\n",
    "            \"service\": \"Schema Registry\",\n",
    "            \"status\": \"✗ Unhealthy\",\n",
    "            \"latency\": f\"{latency:.0f}ms\",\n",
    "            \"detail\": f\"Status {response.status_code}\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    health_checks.append({\n",
    "        \"service\": \"Schema Registry\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"latency\": \"N/A\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# 2. Check MinIO\n",
    "try:\n",
    "    import boto3\n",
    "\n",
    "    start = time.time()\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=\"http://localhost:9000\",\n",
    "        aws_access_key_id=\"admin\",\n",
    "        aws_secret_access_key=\"password\",\n",
    "    )\n",
    "    buckets = s3_client.list_buckets()\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    health_checks.append({\n",
    "        \"service\": \"MinIO (S3)\",\n",
    "        \"status\": \"✓ Healthy\",\n",
    "        \"latency\": f\"{latency:.0f}ms\",\n",
    "        \"detail\": f\"{len(buckets['Buckets'])} buckets\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    health_checks.append({\n",
    "        \"service\": \"MinIO (S3)\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"latency\": \"N/A\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# 3. Check Iceberg REST Catalog\n",
    "try:\n",
    "    start = time.time()\n",
    "    response = requests.get(\"http://localhost:8181/v1/config\", timeout=5)\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        health_checks.append({\n",
    "            \"service\": \"Iceberg REST\",\n",
    "            \"status\": \"✓ Healthy\",\n",
    "            \"latency\": f\"{latency:.0f}ms\",\n",
    "            \"detail\": \"Catalog ready\"\n",
    "        })\n",
    "    else:\n",
    "        health_checks.append({\n",
    "            \"service\": \"Iceberg REST\",\n",
    "            \"status\": \"✗ Unhealthy\",\n",
    "            \"latency\": f\"{latency:.0f}ms\",\n",
    "            \"detail\": f\"Status {response.status_code}\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    health_checks.append({\n",
    "        \"service\": \"Iceberg REST\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"latency\": \"N/A\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# 4. Check Prometheus\n",
    "try:\n",
    "    start = time.time()\n",
    "    response = requests.get(\"http://localhost:9090/-/healthy\", timeout=5)\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        health_checks.append({\n",
    "            \"service\": \"Prometheus\",\n",
    "            \"status\": \"✓ Healthy\",\n",
    "            \"latency\": f\"{latency:.0f}ms\",\n",
    "            \"detail\": \"Metrics ready\"\n",
    "        })\n",
    "    else:\n",
    "        health_checks.append({\n",
    "            \"service\": \"Prometheus\",\n",
    "            \"status\": \"✗ Unhealthy\",\n",
    "            \"latency\": f\"{latency:.0f}ms\",\n",
    "            \"detail\": f\"Status {response.status_code}\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    health_checks.append({\n",
    "        \"service\": \"Prometheus\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"latency\": \"N/A\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# 5. Check Kafka (via AdminClient)\n",
    "try:\n",
    "    from confluent_kafka.admin import AdminClient\n",
    "\n",
    "    start = time.time()\n",
    "    admin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "    metadata = admin.list_topics(timeout=5)\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    health_checks.append({\n",
    "        \"service\": \"Kafka\",\n",
    "        \"status\": \"✓ Healthy\",\n",
    "        \"latency\": f\"{latency:.0f}ms\",\n",
    "        \"detail\": f\"{len(metadata.topics)} topics\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    health_checks.append({\n",
    "        \"service\": \"Kafka\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"latency\": \"N/A\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# Create health check table\n",
    "table = Table(title=\"Service Health Checks\", show_header=True)\n",
    "table.add_column(\"Service\", style=\"cyan\")\n",
    "table.add_column(\"Status\", style=\"green\")\n",
    "table.add_column(\"Latency\", style=\"yellow\")\n",
    "table.add_column(\"Detail\", style=\"white\")\n",
    "\n",
    "all_healthy = True\n",
    "for check in health_checks:\n",
    "    status_style = \"green\" if \"✓\" in check[\"status\"] else \"red\"\n",
    "    table.add_row(\n",
    "        check[\"service\"],\n",
    "        f\"[{status_style}]{check['status']}[/{status_style}]\",\n",
    "        check[\"latency\"],\n",
    "        check[\"detail\"]\n",
    "    )\n",
    "    if \"✗\" in check[\"status\"]:\n",
    "        all_healthy = False\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "if all_healthy:\n",
    "    console.print(\"\\n[bold green]✓ All services are healthy and ready![/bold green]\")\n",
    "else:\n",
    "    console.print(\"\\n[bold red]✗ Some services are unhealthy. Please check Docker logs.[/bold red]\")\n",
    "    console.print(\"\\nDebug with: [bold]docker compose logs <service-name>[/bold]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. Infrastructure Initialization\n",
    "\n",
    "Now that services are running and healthy, let's initialize the infrastructure:\n",
    "1. Register v2 Avro schemas with Schema Registry\n",
    "2. Create Kafka topics for crypto trades\n",
    "3. Create Iceberg tables with v2 schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run infrastructure initialization script\n",
    "console.print(\"\\n[bold cyan]Initializing infrastructure...[/bold cyan]\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"python\", \"scripts/init_e2e_demo.py\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        cwd=Path.cwd().parent,\n",
    "        timeout=60\n",
    "    )\n",
    "\n",
    "    # Print output\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        console.print(\"\\n[bold green]✓ Infrastructure initialized successfully![/bold green]\")\n",
    "    else:\n",
    "        console.print(f\"\\n[bold red]✗ Initialization failed with code {result.returncode}[/bold red]\")\n",
    "        if result.stderr:\n",
    "            print(\"Error output:\")\n",
    "            print(result.stderr)\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    console.print(\"\\n[bold red]✗ Initialization timed out (>60s)[/bold red]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[bold red]✗ Error during initialization: {e}[/bold red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify infrastructure\n",
    "console.print(\"\\n[bold cyan]Verifying infrastructure components...[/bold cyan]\\n\")\n",
    "\n",
    "verification_results = []\n",
    "\n",
    "# 1. Check Schema Registry subjects\n",
    "try:\n",
    "    from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "\n",
    "    client = SchemaRegistryClient({\"url\": \"http://localhost:8081\"})\n",
    "    subjects = client.get_subjects()\n",
    "\n",
    "    # Check for required v2 subjects\n",
    "    required_subjects = [\n",
    "        \"market.crypto.trades-value\",\n",
    "        \"market.equities.trades-value\",\n",
    "    ]\n",
    "\n",
    "    found_subjects = [s for s in required_subjects if s in subjects]\n",
    "\n",
    "    verification_results.append({\n",
    "        \"component\": \"Schema Registry\",\n",
    "        \"status\": \"✓ Ready\" if len(found_subjects) >= 2 else \"⚠ Partial\",\n",
    "        \"detail\": f\"{len(subjects)} schemas registered\"\n",
    "    })\n",
    "except Exception as e:\n",
    "    verification_results.append({\n",
    "        \"component\": \"Schema Registry\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# 2. Check Kafka topics\n",
    "try:\n",
    "    from confluent_kafka.admin import AdminClient\n",
    "\n",
    "    admin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "    metadata = admin.list_topics(timeout=10)\n",
    "\n",
    "    # Check for crypto trades topic\n",
    "    crypto_topic = \"market.crypto.trades.binance\"\n",
    "    topic_exists = crypto_topic in metadata.topics\n",
    "\n",
    "    if topic_exists:\n",
    "        topic_meta = metadata.topics[crypto_topic]\n",
    "        partition_count = len(topic_meta.partitions)\n",
    "        verification_results.append({\n",
    "            \"component\": \"Kafka Topics\",\n",
    "            \"status\": \"✓ Ready\",\n",
    "            \"detail\": f\"{crypto_topic} ({partition_count} partitions)\"\n",
    "        })\n",
    "    else:\n",
    "        verification_results.append({\n",
    "            \"component\": \"Kafka Topics\",\n",
    "            \"status\": \"✗ Missing\",\n",
    "            \"detail\": f\"{crypto_topic} not found\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    verification_results.append({\n",
    "        \"component\": \"Kafka Topics\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# 3. Check Iceberg tables\n",
    "try:\n",
    "    from pyiceberg.catalog import load_catalog\n",
    "\n",
    "    catalog = load_catalog(\n",
    "        \"k2\",\n",
    "        **{\n",
    "            \"uri\": \"http://localhost:8181\",\n",
    "            \"s3.endpoint\": \"http://localhost:9000\",\n",
    "            \"s3.access-key-id\": \"admin\",\n",
    "            \"s3.secret-access-key\": \"password\",\n",
    "            \"s3.path-style-access\": \"true\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Try to load trades_v2 table\n",
    "    try:\n",
    "        table = catalog.load_table(\"market_data.trades_v2\")\n",
    "        field_count = len(table.schema().fields)\n",
    "\n",
    "        verification_results.append({\n",
    "            \"component\": \"Iceberg Tables\",\n",
    "            \"status\": \"✓ Ready\",\n",
    "            \"detail\": f\"market_data.trades_v2 ({field_count} fields)\"\n",
    "        })\n",
    "    except Exception:\n",
    "        verification_results.append({\n",
    "            \"component\": \"Iceberg Tables\",\n",
    "            \"status\": \"✗ Not Found\",\n",
    "            \"detail\": \"market_data.trades_v2 missing\"\n",
    "        })\n",
    "except Exception as e:\n",
    "    verification_results.append({\n",
    "        \"component\": \"Iceberg Tables\",\n",
    "        \"status\": \"✗ Error\",\n",
    "        \"detail\": str(e)[:50]\n",
    "    })\n",
    "\n",
    "# Create verification table\n",
    "table = Table(title=\"Infrastructure Verification\", show_header=True)\n",
    "table.add_column(\"Component\", style=\"cyan\")\n",
    "table.add_column(\"Status\", style=\"green\")\n",
    "table.add_column(\"Detail\", style=\"white\")\n",
    "\n",
    "all_ready = True\n",
    "for result in verification_results:\n",
    "    if \"✓\" in result[\"status\"]:\n",
    "        status_style = \"green\"\n",
    "    elif \"⚠\" in result[\"status\"]:\n",
    "        status_style = \"yellow\"\n",
    "    else:\n",
    "        status_style = \"red\"\n",
    "        all_ready = False\n",
    "\n",
    "    table.add_row(\n",
    "        result[\"component\"],\n",
    "        f\"[{status_style}]{result['status']}[/{status_style}]\",\n",
    "        result[\"detail\"]\n",
    "    )\n",
    "\n",
    "console.print(table)\n",
    "\n",
    "if all_ready:\n",
    "    console.print(\"\\n[bold green]✓ All infrastructure components are ready for streaming![/bold green]\")\n",
    "else:\n",
    "    console.print(\"\\n[bold red]✗ Some components are not ready. Run init_e2e_demo.py again.[/bold red]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Binance WebSocket Client Demo\n",
    "\n",
    "Now for the exciting part - let's connect to Binance's live WebSocket stream and watch real cryptocurrency trades flow in!\n",
    "\n",
    "We'll stream trades for:\n",
    "- **BTCUSDT** - Bitcoin vs USDT\n",
    "- **ETHUSDT** - Ethereum vs USDT\n",
    "\n",
    "The stream will run for **30 seconds** so you can see real-time data flowing through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Binance client and producer\n",
    "import asyncio\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from k2.ingestion.binance_client import BinanceWebSocketClient\n",
    "from k2.ingestion.producer import MarketDataProducer\n",
    "\n",
    "# Create producer (will send to Kafka)\n",
    "producer = MarketDataProducer(schema_version=\"v2\")\n",
    "\n",
    "# Create Binance WebSocket client\n",
    "symbols = [\"BTCUSDT\", \"ETHUSDT\"]\n",
    "client = BinanceWebSocketClient(\n",
    "    symbols=symbols,\n",
    "    producer=producer,\n",
    ")\n",
    "\n",
    "console.print(f\"\\n[bold green]✓ Binance client created for symbols: {', '.join(symbols)}[/bold green]\")\n",
    "console.print(\"\\nReady to start streaming!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream trades for 30 seconds\n",
    "console.print(\"\\n[bold cyan]Starting Binance WebSocket stream...[/bold cyan]\\n\")\n",
    "console.print(\"[yellow]Streaming for 30 seconds. Watch real trades flow in![/yellow]\\n\")\n",
    "\n",
    "# Track streaming stats\n",
    "streaming_stats = {\n",
    "    \"start_time\": None,\n",
    "    \"total_trades\": 0,\n",
    "    \"trades_by_symbol\": {},\n",
    "    \"latest_prices\": {},\n",
    "}\n",
    "\n",
    "async def demo_stream():\n",
    "    \"\"\"Stream trades from Binance for 30 seconds.\"\"\"\n",
    "    try:\n",
    "        # Connect to Binance\n",
    "        await client.connect()\n",
    "        console.print(\"[green]✓ Connected to Binance WebSocket![/green]\\n\")\n",
    "\n",
    "        streaming_stats[\"start_time\"] = time.time()\n",
    "\n",
    "        # Stream for 30 seconds\n",
    "        duration = 30\n",
    "        while time.time() - streaming_stats[\"start_time\"] < duration:\n",
    "            await asyncio.sleep(1)\n",
    "\n",
    "            # Get current metrics from producer\n",
    "            elapsed = int(time.time() - streaming_stats[\"start_time\"])\n",
    "\n",
    "            # Update display\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            console.print(f\"[bold cyan]Streaming... ({elapsed}s / {duration}s)[/bold cyan]\\n\")\n",
    "            console.print(\"Connected to: wss://stream.binance.com:9443/stream\")\n",
    "            console.print(f\"Symbols: {', '.join(symbols)}\")\n",
    "            console.print(\"\\n[yellow]Live trades are being sent to Kafka topic: market.crypto.trades.binance[/yellow]\")\n",
    "            console.print(f\"\\nTrades received: [bold green]~{elapsed * 10}[/bold green] (estimated)\")\n",
    "\n",
    "        # Disconnect\n",
    "        await client.disconnect()\n",
    "\n",
    "        console.print(\"\\n[bold green]✓ Streaming complete![/bold green]\")\n",
    "        console.print(f\"\\nTotal time: {duration}s\")\n",
    "        console.print(f\"Estimated trades received: [bold]{duration * 10}[/bold] (BTC + ETH)\")\n",
    "        console.print(\"\\n[cyan]Trades are now in Kafka and ready to be consumed to Iceberg![/cyan]\")\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"\\n[bold red]✗ Streaming error: {e}[/bold red]\")\n",
    "        console.print(\"\\n[yellow]This is usually due to network issues or Binance API limits.[/yellow]\")\n",
    "        console.print(\"[yellow]Don't worry - you can continue with existing Kafka data![/yellow]\")\n",
    "\n",
    "# Run the streaming demo\n",
    "await demo_stream()\n",
    "\n",
    "# Flush producer to ensure all messages are sent\n",
    "producer.flush()\n",
    "console.print(\"\\n[green]✓ Producer flushed - all messages sent to Kafka[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example Binance WebSocket message format\n",
    "console.print(\"\\n[bold cyan]Example Binance WebSocket Message:[/bold cyan]\\n\")\n",
    "\n",
    "import json\n",
    "\n",
    "example_binance_message = {\n",
    "    \"e\": \"trade\",              # Event type\n",
    "    \"E\": 1704067800000,        # Event time (milliseconds)\n",
    "    \"s\": \"BTCUSDT\",            # Symbol\n",
    "    \"t\": 123456789,            # Trade ID\n",
    "    \"p\": \"65000.00\",           # Price\n",
    "    \"q\": \"0.05\",               # Quantity\n",
    "    \"T\": 1704067800000,        # Trade time (milliseconds)\n",
    "    \"m\": False,                # Is buyer maker?\n",
    "    \"M\": True                  # Is best match?\n",
    "}\n",
    "\n",
    "console.print(\"[yellow]Raw Binance WebSocket JSON:[/yellow]\")\n",
    "print(json.dumps(example_binance_message, indent=2))\n",
    "\n",
    "console.print(\"\\n[bold cyan]Converted to K2 V2 Schema:[/bold cyan]\\n\")\n",
    "\n",
    "# Show v2 conversion (simplified example)\n",
    "example_v2_trade = {\n",
    "    \"message_id\": \"123e4567-e89b-12d3-a456-426614174000\",  # Generated UUID\n",
    "    \"trade_id\": \"BINANCE-123456789\",                       # Exchange trade ID\n",
    "    \"symbol\": \"BTCUSDT\",                                   # Trading pair\n",
    "    \"exchange\": \"BINANCE\",                                 # Exchange name\n",
    "    \"asset_class\": \"crypto\",                               # Asset class\n",
    "    \"timestamp\": \"2024-01-01T00:00:00.000000Z\",            # Microseconds\n",
    "    \"price\": \"65000.00000000\",                             # Decimal(18,8)\n",
    "    \"quantity\": \"0.05000000\",                              # Decimal(18,8)\n",
    "    \"currency\": \"USDT\",                                    # Quote currency\n",
    "    \"side\": \"BUY\",                                         # Aggressor side\n",
    "    \"source_sequence\": 123456789,                          # Exchange sequence\n",
    "    \"vendor_data\": {                                        # Binance-specific fields\n",
    "        \"event_type\": \"trade\",\n",
    "        \"event_time\": \"1704067800000\",\n",
    "        \"trade_time\": \"1704067800000\",\n",
    "        \"is_buyer_maker\": \"False\",\n",
    "        \"is_best_match\": \"True\",\n",
    "        \"base_asset\": \"BTC\",\n",
    "        \"quote_asset\": \"USDT\"\n",
    "    }\n",
    "}\n",
    "\n",
    "console.print(\"[yellow]K2 V2 Schema (with vendor_data):[/yellow]\")\n",
    "print(json.dumps(example_v2_trade, indent=2))\n",
    "\n",
    "console.print(\"\\n[bold green]Key Points:[/bold green]\")\n",
    "console.print(\"  • Industry-standard core fields (symbol, price, quantity, etc.)\")\n",
    "console.print(\"  • Binance-specific fields preserved in vendor_data JSON\")\n",
    "console.print(\"  • UUID message_id for deduplication\")\n",
    "console.print(\"  • Microsecond precision timestamps\")\n",
    "console.print(\"  • Decimal(18,8) precision for prices/quantities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3lvsw5ih4e4",
   "metadata": {},
   "source": "## 6. Kafka Topic Inspection\n\nLet's verify that the trades we just streamed are actually in Kafka. We'll check:\n1. Topic metadata (partitions, leaders)\n2. Total message count per partition\n3. Sample a few messages to see the Avro-serialized data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w4uyfi6uwj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Kafka topic metadata\n",
    "console.print(\"\\n[bold cyan]Kafka Topic Metadata:[/bold cyan]\\n\")\n",
    "\n",
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "admin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n",
    "metadata = admin.list_topics(topic=\"market.crypto.trades.binance\", timeout=10)\n",
    "\n",
    "topic_meta = metadata.topics[\"market.crypto.trades.binance\"]\n",
    "\n",
    "console.print(f\"[yellow]Topic:[/yellow] {topic_meta.topic}\")\n",
    "console.print(f\"[yellow]Partitions:[/yellow] {len(topic_meta.partitions)}\")\n",
    "\n",
    "# Create partition table\n",
    "table = Table(title=\"Partition Distribution\", show_header=True)\n",
    "table.add_column(\"Partition\", style=\"cyan\")\n",
    "table.add_column(\"Leader\", style=\"green\")\n",
    "table.add_column(\"Replicas\", style=\"yellow\")\n",
    "table.add_column(\"ISR\", style=\"magenta\")\n",
    "\n",
    "for partition_id, partition in sorted(topic_meta.partitions.items()):\n",
    "    table.add_row(\n",
    "        str(partition_id),\n",
    "        str(partition.leader),\n",
    "        str(len(partition.replicas)),\n",
    "        str(len(partition.isrs))\n",
    "    )\n",
    "\n",
    "console.print(table)\n",
    "console.print(\"\\n[green]✓ Topic exists and has healthy partition distribution[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dy5zb3i7n2p",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count messages in topic\n",
    "console.print(\"\\n[bold cyan]Counting messages in topic...[/bold cyan]\\n\")\n",
    "\n",
    "from confluent_kafka import Consumer\n",
    "\n",
    "consumer = Consumer({\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"notebook-inspector\",\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "})\n",
    "\n",
    "# Get partition assignments\n",
    "consumer.subscribe([\"market.crypto.trades.binance\"])\n",
    "consumer.poll(timeout=1.0)  # Trigger partition assignment\n",
    "assignment = consumer.assignment()\n",
    "\n",
    "if not assignment:\n",
    "    console.print(\"[yellow]⚠ No partitions assigned yet. Topic might be empty.[/yellow]\")\n",
    "else:\n",
    "    # Get watermark offsets (low = earliest, high = latest)\n",
    "    table = Table(title=\"Message Count by Partition\", show_header=True)\n",
    "    table.add_column(\"Partition\", style=\"cyan\")\n",
    "    table.add_column(\"Low Offset\", style=\"yellow\")\n",
    "    table.add_column(\"High Offset\", style=\"green\")\n",
    "    table.add_column(\"Messages\", style=\"magenta\")\n",
    "\n",
    "    total_messages = 0\n",
    "    for tp in sorted(assignment, key=lambda x: x.partition):\n",
    "        low, high = consumer.get_watermark_offsets(tp, timeout=5.0)\n",
    "        messages = high - low\n",
    "        total_messages += messages\n",
    "\n",
    "        table.add_row(\n",
    "            str(tp.partition),\n",
    "            f\"{low:,}\",\n",
    "            f\"{high:,}\",\n",
    "            f\"{messages:,}\"\n",
    "        )\n",
    "\n",
    "    console.print(table)\n",
    "    console.print(f\"\\n[bold green]Total messages in topic: {total_messages:,}[/bold green]\")\n",
    "\n",
    "    if total_messages == 0:\n",
    "        console.print(\"\\n[yellow]⚠ No messages found. The streaming demo might have failed or not run yet.[/yellow]\")\n",
    "    else:\n",
    "        console.print(f\"[cyan]Average messages per partition: {total_messages / len(assignment):,.1f}[/cyan]\")\n",
    "\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cy1b5eds6k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 5 messages from the topic\n",
    "console.print(\"\\n[bold cyan]Sampling messages from Kafka...[/bold cyan]\\n\")\n",
    "\n",
    "from confluent_kafka import Consumer\n",
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer\n",
    "\n",
    "# Create Schema Registry client\n",
    "schema_registry_client = SchemaRegistryClient({\"url\": \"http://localhost:8081\"})\n",
    "\n",
    "# Create consumer\n",
    "consumer = Consumer({\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "    \"group.id\": \"notebook-sampler\",\n",
    "    \"auto.offset.reset\": \"earliest\",\n",
    "    \"enable.auto.commit\": False,\n",
    "})\n",
    "\n",
    "consumer.subscribe([\"market.crypto.trades.binance\"])\n",
    "\n",
    "# Consume 5 messages\n",
    "messages = []\n",
    "attempts = 0\n",
    "max_attempts = 20\n",
    "\n",
    "console.print(\"[yellow]Consuming messages...[/yellow]\")\n",
    "\n",
    "while len(messages) < 5 and attempts < max_attempts:\n",
    "    msg = consumer.poll(timeout=1.0)\n",
    "    attempts += 1\n",
    "\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        console.print(f\"[red]Consumer error: {msg.error()}[/red]\")\n",
    "        continue\n",
    "\n",
    "    # Deserialize Avro value\n",
    "    try:\n",
    "        # Get schema ID from message\n",
    "        schema_id = int.from_bytes(msg.value()[1:5], byteorder='big')\n",
    "        schema = schema_registry_client.get_schema(schema_id)\n",
    "\n",
    "        deserializer = AvroDeserializer(\n",
    "            schema_registry_client,\n",
    "            schema.schema_str\n",
    "        )\n",
    "\n",
    "        value = deserializer(msg.value(), None)\n",
    "        messages.append(value)\n",
    "\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]Deserialization error: {e}[/red]\")\n",
    "        continue\n",
    "\n",
    "consumer.close()\n",
    "\n",
    "if messages:\n",
    "    console.print(f\"\\n[green]✓ Successfully sampled {len(messages)} messages[/green]\\n\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_kafka = pd.DataFrame(messages)\n",
    "\n",
    "    # Display sample\n",
    "    display_columns = [\"symbol\", \"timestamp\", \"price\", \"quantity\", \"side\", \"exchange\"]\n",
    "    if all(col in df_kafka.columns for col in display_columns):\n",
    "        console.print(\"[bold]Sample Messages:[/bold]\")\n",
    "        display(df_kafka[display_columns].head())\n",
    "\n",
    "        console.print(f\"\\n[cyan]Message structure: {len(df_kafka.columns)} fields[/cyan]\")\n",
    "        console.print(f\"[cyan]Fields: {', '.join(df_kafka.columns[:10])}...[/cyan]\")\n",
    "    else:\n",
    "        console.print(\"[bold]Sample Messages (all fields):[/bold]\")\n",
    "        display(df_kafka.head())\n",
    "else:\n",
    "    console.print(\"\\n[yellow]⚠ No messages found. The topic might be empty.[/yellow]\")\n",
    "    console.print(\"[yellow]Run the Binance streaming demo above to populate the topic.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9h9a55wuh",
   "metadata": {},
   "source": "## 7. Consumer Pipeline (Kafka → Iceberg)\n\nNow let's consume messages from Kafka and write them to Iceberg. The consumer will:\n1. Read messages in batches (500 records)\n2. Deserialize Avro to Python objects\n3. Write to Iceberg with ACID guarantees\n4. Track metrics (throughput, latency, errors)\n\nWe'll consume **1,000 messages** to demonstrate the pipeline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "igq255o8hu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run consumer to write messages to Iceberg\n",
    "console.print(\"\\n[bold cyan]Starting consumer pipeline...[/bold cyan]\\n\")\n",
    "\n",
    "from k2.ingestion.consumer import MarketDataConsumer\n",
    "\n",
    "# Create consumer\n",
    "consumer = MarketDataConsumer(\n",
    "    topics=[\"market.crypto.trades.binance\"],\n",
    "    group_id=\"notebook-consumer\",\n",
    "    table_version=\"v2\",\n",
    ")\n",
    "\n",
    "console.print(\"[yellow]Consumer Configuration:[/yellow]\")\n",
    "console.print(f\"  Topics: {consumer.topics}\")\n",
    "console.print(f\"  Group ID: {consumer.group_id}\")\n",
    "console.print(f\"  Batch size: {consumer.batch_size}\")\n",
    "console.print(\"  Table version: v2\")\n",
    "console.print(\"\\n[green]✓ Consumer initialized[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8hhxl2g3t46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consume and write messages\n",
    "console.print(\"\\n[bold cyan]Consuming messages and writing to Iceberg...[/bold cyan]\\n\")\n",
    "\n",
    "max_messages = 1000\n",
    "messages_consumed = 0\n",
    "messages_written = 0\n",
    "start_time = time.time()\n",
    "\n",
    "console.print(f\"[yellow]Target: {max_messages} messages[/yellow]\")\n",
    "console.print(\"[yellow]This may take 30-60 seconds...[/yellow]\\n\")\n",
    "\n",
    "try:\n",
    "    with Progress(\n",
    "        SpinnerColumn(),\n",
    "        TextColumn(\"[progress.description]{task.description}\"),\n",
    "        console=console,\n",
    "    ) as progress:\n",
    "        task = progress.add_task(\"Consuming messages...\", total=max_messages)\n",
    "\n",
    "        # Start consuming\n",
    "        for message in consumer.consume(max_messages=max_messages):\n",
    "            messages_consumed += 1\n",
    "            progress.update(task, advance=1)\n",
    "\n",
    "            # Update every 100 messages\n",
    "            if messages_consumed % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                throughput = messages_consumed / elapsed if elapsed > 0 else 0\n",
    "                progress.update(\n",
    "                    task,\n",
    "                    description=f\"Consuming messages... ({messages_consumed}/{max_messages}, {throughput:.1f} msg/s)\"\n",
    "                )\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    throughput = messages_consumed / elapsed_time if elapsed_time > 0 else 0\n",
    "\n",
    "    console.print(\"\\n[bold green]✓ Consumption complete![/bold green]\")\n",
    "    console.print(\"\\nResults:\")\n",
    "    console.print(f\"  Messages consumed: [bold]{messages_consumed:,}[/bold]\")\n",
    "    console.print(f\"  Time elapsed: [bold]{elapsed_time:.2f}s[/bold]\")\n",
    "    console.print(f\"  Throughput: [bold]{throughput:.2f} msg/s[/bold]\")\n",
    "\n",
    "    # Note: Messages are written in batches, so written count may differ\n",
    "    console.print(\"\\n[cyan]Messages have been written to Iceberg table: market_data.trades_v2[/cyan]\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    console.print(\"\\n[yellow]⚠ Consumption interrupted by user[/yellow]\")\n",
    "except Exception as e:\n",
    "    console.print(f\"\\n[bold red]✗ Consumer error: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    # Close consumer\n",
    "    consumer.close()\n",
    "    console.print(\"\\n[green]✓ Consumer closed[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dwsnm13zfi",
   "metadata": {},
   "source": "## 8. Iceberg Table Inspection\n\nLet's verify that data was successfully written to the Iceberg lakehouse. We'll:\n1. Load the table and inspect its schema\n2. Count total rows\n3. Sample recent data\n4. Examine vendor_data fields"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4i9wu3c8xb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Iceberg table and inspect schema\n",
    "console.print(\"\\n[bold cyan]Loading Iceberg table...[/bold cyan]\\n\")\n",
    "\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "catalog = load_catalog(\n",
    "    \"k2\",\n",
    "    **{\n",
    "        \"uri\": \"http://localhost:8181\",\n",
    "        \"s3.endpoint\": \"http://localhost:9000\",\n",
    "        \"s3.access-key-id\": \"admin\",\n",
    "        \"s3.secret-access-key\": \"password\",\n",
    "        \"s3.path-style-access\": \"true\",\n",
    "    },\n",
    ")\n",
    "\n",
    "try:\n",
    "    table = catalog.load_table(\"market_data.trades_v2\")\n",
    "\n",
    "    console.print(f\"[green]✓ Table loaded: {table.name()}[/green]\")\n",
    "    console.print(\"\\n[yellow]Table Schema (V2):[/yellow]\\n\")\n",
    "\n",
    "    # Create schema table\n",
    "    schema_table = Table(title=\"trades_v2 Schema\", show_header=True)\n",
    "    schema_table.add_column(\"Field ID\", style=\"cyan\")\n",
    "    schema_table.add_column(\"Field Name\", style=\"green\")\n",
    "    schema_table.add_column(\"Type\", style=\"yellow\")\n",
    "    schema_table.add_column(\"Required\", style=\"magenta\")\n",
    "\n",
    "    for field in table.schema().fields:\n",
    "        required = \"✓\" if field.required else \"✗\"\n",
    "        schema_table.add_row(\n",
    "            str(field.field_id),\n",
    "            field.name,\n",
    "            str(field.field_type),\n",
    "            required\n",
    "        )\n",
    "\n",
    "    console.print(schema_table)\n",
    "    console.print(f\"\\n[cyan]Total fields: {len(table.schema().fields)}[/cyan]\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error loading table: {e}[/bold red]\")\n",
    "    console.print(\"\\n[yellow]Make sure the consumer has written data to the table.[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9h9d86c8dz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows using DuckDB\n",
    "console.print(\"\\n[bold cyan]Counting rows in Iceberg table...[/bold cyan]\\n\")\n",
    "\n",
    "import duckdb\n",
    "\n",
    "# Create DuckDB connection\n",
    "conn = duckdb.connect(\":memory:\")\n",
    "conn.install_extension(\"iceberg\")\n",
    "conn.load_extension(\"iceberg\")\n",
    "\n",
    "# Configure S3 access for DuckDB\n",
    "conn.execute(\"\"\"\n",
    "    CREATE SECRET secret1 (\n",
    "        TYPE S3,\n",
    "        KEY_ID 'admin',\n",
    "        SECRET 'password',\n",
    "        ENDPOINT 'localhost:9000',\n",
    "        URL_STYLE 'path',\n",
    "        REGION 'us-east-1',\n",
    "        USE_SSL false\n",
    "    );\n",
    "\"\"\")\n",
    "\n",
    "try:\n",
    "    # Count rows\n",
    "    result = conn.execute(\"\"\"\n",
    "        SELECT COUNT(*) as count\n",
    "        FROM iceberg_scan('http://localhost:8181/v1/market_data/trades_v2', allow_moved_paths=true)\n",
    "    \"\"\").fetchone()\n",
    "\n",
    "    row_count = result[0]\n",
    "\n",
    "    console.print(f\"[bold green]Total rows in trades_v2: {row_count:,}[/bold green]\")\n",
    "\n",
    "    if row_count == 0:\n",
    "        console.print(\"\\n[yellow]⚠ No rows found. The consumer might not have written data yet.[/yellow]\")\n",
    "        console.print(\"[yellow]Run the consumer pipeline above to write data to Iceberg.[/yellow]\")\n",
    "    else:\n",
    "        # Get data distribution by symbol\n",
    "        result = conn.execute(\"\"\"\n",
    "            SELECT \n",
    "                symbol,\n",
    "                COUNT(*) as trade_count,\n",
    "                MIN(timestamp) as first_trade,\n",
    "                MAX(timestamp) as last_trade\n",
    "            FROM iceberg_scan('http://localhost:8181/v1/market_data/trades_v2', allow_moved_paths=true)\n",
    "            GROUP BY symbol\n",
    "            ORDER BY trade_count DESC\n",
    "        \"\"\").fetchdf()\n",
    "\n",
    "        console.print(\"\\n[yellow]Data distribution by symbol:[/yellow]\\n\")\n",
    "        display(result)\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error querying table: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onxfoyk7i2q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample recent data\n",
    "console.print(\"\\n[bold cyan]Sampling recent trades...[/bold cyan]\\n\")\n",
    "\n",
    "try:\n",
    "    # Query 10 most recent trades\n",
    "    df_recent = conn.execute(\"\"\"\n",
    "        SELECT *\n",
    "        FROM iceberg_scan('http://localhost:8181/v1/market_data/trades_v2', allow_moved_paths=true)\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 10\n",
    "    \"\"\").fetchdf()\n",
    "\n",
    "    if len(df_recent) > 0:\n",
    "        console.print(f\"[green]✓ Found {len(df_recent)} recent trades[/green]\\n\")\n",
    "\n",
    "        # Display key columns\n",
    "        display_columns = [\"symbol\", \"exchange\", \"timestamp\", \"price\", \"quantity\", \"side\", \"currency\"]\n",
    "        console.print(\"[bold]Recent Trades (Key Fields):[/bold]\")\n",
    "        display(df_recent[display_columns])\n",
    "\n",
    "        # Show sample trade details\n",
    "        console.print(\"\\n[bold yellow]Sample Trade Details:[/bold yellow]\")\n",
    "        sample = df_recent.iloc[0]\n",
    "        console.print(f\"  Symbol: {sample['symbol']}\")\n",
    "        console.print(f\"  Exchange: {sample['exchange']}\")\n",
    "        console.print(f\"  Asset Class: {sample['asset_class']}\")\n",
    "        console.print(f\"  Timestamp: {sample['timestamp']}\")\n",
    "        console.print(f\"  Price: {sample['price']} {sample['currency']}\")\n",
    "        console.print(f\"  Quantity: {sample['quantity']}\")\n",
    "        console.print(f\"  Side: {sample['side']}\")\n",
    "        console.print(f\"  Trade ID: {sample['trade_id']}\")\n",
    "        console.print(f\"  Message ID: {sample['message_id']}\")\n",
    "\n",
    "    else:\n",
    "        console.print(\"[yellow]⚠ No trades found in table[/yellow]\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error sampling data: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmtzvxvrm9q",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine vendor_data (Binance-specific fields)\n",
    "console.print(\"\\n[bold cyan]Examining vendor_data...[/bold cyan]\\n\")\n",
    "\n",
    "import json\n",
    "\n",
    "if len(df_recent) > 0:\n",
    "    sample = df_recent.iloc[0]\n",
    "\n",
    "    console.print(\"[bold yellow]Vendor Data (Binance-specific fields):[/bold yellow]\\n\")\n",
    "\n",
    "    # Parse vendor_data JSON\n",
    "    if pd.notna(sample['vendor_data']):\n",
    "        vendor_data = json.loads(sample['vendor_data'])\n",
    "\n",
    "        # Create vendor data table\n",
    "        vendor_table = Table(title=\"Binance Vendor Data\", show_header=True)\n",
    "        vendor_table.add_column(\"Field\", style=\"cyan\")\n",
    "        vendor_table.add_column(\"Value\", style=\"green\")\n",
    "        vendor_table.add_column(\"Description\", style=\"yellow\")\n",
    "\n",
    "        field_descriptions = {\n",
    "            \"event_type\": \"Type of event (trade, aggTrade, etc.)\",\n",
    "            \"event_time\": \"Event timestamp from Binance (milliseconds)\",\n",
    "            \"trade_time\": \"Trade execution timestamp (milliseconds)\",\n",
    "            \"is_buyer_maker\": \"Was the buyer the maker? (true/false)\",\n",
    "            \"is_best_match\": \"Was this the best price match? (true/false)\",\n",
    "            \"base_asset\": \"Base asset symbol (e.g., BTC in BTCUSDT)\",\n",
    "            \"quote_asset\": \"Quote asset symbol (e.g., USDT in BTCUSDT)\",\n",
    "        }\n",
    "\n",
    "        for key, value in vendor_data.items():\n",
    "            description = field_descriptions.get(key, \"N/A\")\n",
    "            vendor_table.add_row(key, str(value), description)\n",
    "\n",
    "        console.print(vendor_table)\n",
    "\n",
    "        console.print(\"\\n[bold green]Key Points:[/bold green]\")\n",
    "        console.print(\"  • vendor_data preserves all Binance-specific fields\")\n",
    "        console.print(\"  • Core fields (price, quantity, side) are normalized to v2 schema\")\n",
    "        console.print(\"  • Exchange-specific data enables advanced analysis\")\n",
    "        console.print(\"  • Same v2 schema works across ASX (equities) and Binance (crypto)\")\n",
    "\n",
    "    else:\n",
    "        console.print(\"[yellow]⚠ No vendor_data found in sample trade[/yellow]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available to examine vendor_data[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cttj14bkvon",
   "metadata": {},
   "source": "## 9. Query Engine Demo\n\nNow let's demonstrate the K2 Query Engine, which provides a high-level Python API for querying trades. The QueryEngine:\n- Abstracts DuckDB complexity\n- Supports time-range queries\n- Filters by symbol, exchange, asset class\n- Returns data as Python dictionaries or DataFrames\n- Provides sub-second query performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acmisi6tiw",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize QueryEngine\n",
    "console.print(\"\\n[bold cyan]Initializing Query Engine...[/bold cyan]\\n\")\n",
    "\n",
    "from k2.query.engine import QueryEngine\n",
    "\n",
    "engine = QueryEngine(table_version=\"v2\")\n",
    "\n",
    "console.print(\"[green]✓ QueryEngine initialized[/green]\")\n",
    "console.print(f\"  Iceberg Catalog: {engine.iceberg_catalog_uri}\")\n",
    "console.print(\"  Table: market_data.trades_v2\")\n",
    "console.print(\"  Query Engine: DuckDB with Iceberg extension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byfs6rk0pxi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic queries\n",
    "console.print(\"\\n[bold cyan]Running basic queries...[/bold cyan]\\n\")\n",
    "\n",
    "# Query 1: Get all available symbols\n",
    "try:\n",
    "    symbols = engine.get_symbols(exchange=\"BINANCE\")\n",
    "    console.print(f\"[yellow]Available symbols:[/yellow] {', '.join(symbols)}\")\n",
    "except Exception as e:\n",
    "    console.print(f\"[red]Error getting symbols: {e}[/red]\")\n",
    "    symbols = []\n",
    "\n",
    "# Query 2: Query trades for each symbol\n",
    "if symbols:\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            trades = engine.query_trades(\n",
    "                symbol=symbol,\n",
    "                exchange=\"BINANCE\",\n",
    "                limit=100,\n",
    "            )\n",
    "            console.print(f\"[green]✓[/green] {symbol}: {len(trades)} trades (limit 100)\")\n",
    "        except Exception as e:\n",
    "            console.print(f\"[red]✗[/red] {symbol}: Error - {e}\")\n",
    "else:\n",
    "    console.print(\"\\n[yellow]⚠ No symbols found. Query all trades instead:[/yellow]\")\n",
    "    try:\n",
    "        all_trades = engine.query_trades(limit=100)\n",
    "        console.print(f\"[green]✓[/green] Found {len(all_trades)} trades (limit 100)\")\n",
    "\n",
    "        # Get unique symbols from results\n",
    "        if all_trades:\n",
    "            unique_symbols = set(t['symbol'] for t in all_trades)\n",
    "            console.print(f\"[yellow]Symbols in data:[/yellow] {', '.join(unique_symbols)}\")\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]✗ Error querying trades: {e}[/red]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ct58js5185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-range query\n",
    "console.print(\"\\n[bold cyan]Time-range query (last 5 minutes)...[/bold cyan]\\n\")\n",
    "\n",
    "from datetime import UTC\n",
    "\n",
    "# Query last 5 minutes\n",
    "end_time = datetime.now(UTC)\n",
    "start_time = end_time - timedelta(minutes=5)\n",
    "\n",
    "try:\n",
    "    recent_trades = engine.query_trades(\n",
    "        start_time=start_time,\n",
    "        end_time=end_time,\n",
    "        limit=1000,\n",
    "    )\n",
    "\n",
    "    console.print(f\"[green]✓ Found {len(recent_trades)} trades in last 5 minutes[/green]\")\n",
    "\n",
    "    if recent_trades:\n",
    "        # Convert to DataFrame for analysis\n",
    "        df_trades = pd.DataFrame(recent_trades)\n",
    "\n",
    "        console.print(\"\\n[yellow]Time range:[/yellow]\")\n",
    "        console.print(f\"  Start: {start_time}\")\n",
    "        console.print(f\"  End: {end_time}\")\n",
    "        console.print(\"  Duration: 5 minutes\")\n",
    "\n",
    "        console.print(\"\\n[yellow]Sample trades:[/yellow]\")\n",
    "        display(df_trades[[\"symbol\", \"timestamp\", \"price\", \"quantity\", \"side\"]].head(10))\n",
    "\n",
    "        # Show trade distribution\n",
    "        console.print(\"\\n[yellow]Trade distribution:[/yellow]\")\n",
    "        symbol_counts = df_trades[\"symbol\"].value_counts()\n",
    "        for symbol, count in symbol_counts.items():\n",
    "            console.print(f\"  {symbol}: {count} trades\")\n",
    "    else:\n",
    "        console.print(\"\\n[yellow]⚠ No trades found in the specified time range[/yellow]\")\n",
    "        console.print(\"[yellow]This is normal if data is older than 5 minutes.[/yellow]\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error in time-range query: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7omqhrysp7u",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OHLCV aggregation query\n",
    "console.print(\"\\n[bold cyan]OHLCV (Open, High, Low, Close, Volume) Aggregation...[/bold cyan]\\n\")\n",
    "\n",
    "try:\n",
    "    # Query all trades\n",
    "    all_trades = engine.query_trades(limit=5000)\n",
    "\n",
    "    if all_trades:\n",
    "        df_all = pd.DataFrame(all_trades)\n",
    "\n",
    "        # Convert price and quantity to numeric\n",
    "        df_all[\"price\"] = pd.to_numeric(df_all[\"price\"])\n",
    "        df_all[\"quantity\"] = pd.to_numeric(df_all[\"quantity\"])\n",
    "\n",
    "        # Calculate OHLCV by symbol\n",
    "        ohlcv = df_all.groupby(\"symbol\").agg({\n",
    "            \"price\": [\"first\", \"max\", \"min\", \"last\"],\n",
    "            \"quantity\": \"sum\",\n",
    "            \"trade_id\": \"count\",\n",
    "        })\n",
    "\n",
    "        # Flatten column names\n",
    "        ohlcv.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n",
    "\n",
    "        # Calculate price range and spread\n",
    "        ohlcv[\"range\"] = ohlcv[\"high\"] - ohlcv[\"low\"]\n",
    "        ohlcv[\"spread_pct\"] = (ohlcv[\"range\"] / ohlcv[\"open\"] * 100).round(2)\n",
    "\n",
    "        console.print(\"[bold]OHLCV Summary by Symbol:[/bold]\\n\")\n",
    "        display(ohlcv)\n",
    "\n",
    "        console.print(\"\\n[bold green]Analysis:[/bold green]\")\n",
    "        for symbol in ohlcv.index:\n",
    "            row = ohlcv.loc[symbol]\n",
    "            console.print(f\"\\n[cyan]{symbol}:[/cyan]\")\n",
    "            console.print(f\"  Open: {row['open']:.2f}, Close: {row['close']:.2f}\")\n",
    "            console.print(f\"  High: {row['high']:.2f}, Low: {row['low']:.2f}\")\n",
    "            console.print(f\"  Range: {row['range']:.2f} ({row['spread_pct']:.2f}%)\")\n",
    "            console.print(f\"  Volume: {row['volume']:.4f}\")\n",
    "            console.print(f\"  Trades: {row['trades']:.0f}\")\n",
    "    else:\n",
    "        console.print(\"[yellow]⚠ No trades available for OHLCV calculation[/yellow]\")\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error in OHLCV aggregation: {e}[/bold red]\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zbh09cc11ya",
   "metadata": {},
   "source": "## 10. Real-Time Price Visualization\n\nLet's visualize the live cryptocurrency trade data with interactive charts:\n1. **Price Scatter Plot** - BTC and ETH prices over time (colored by trade size)\n2. **Volume Bar Chart** - Trading volume aggregated in 30-second buckets\n3. **Price Distribution** - Histogram showing price ranges"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ya83x42f6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "console.print(\"\\n[bold cyan]Preparing data for visualization...[/bold cyan]\\n\")\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Enable matplotlib inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "try:\n",
    "    # Query recent trades for visualization\n",
    "    viz_trades = engine.query_trades(limit=2000)\n",
    "\n",
    "    if viz_trades:\n",
    "        df_viz = pd.DataFrame(viz_trades)\n",
    "\n",
    "        # Convert data types\n",
    "        df_viz[\"timestamp\"] = pd.to_datetime(df_viz[\"timestamp\"])\n",
    "        df_viz[\"price\"] = pd.to_numeric(df_viz[\"price\"])\n",
    "        df_viz[\"quantity\"] = pd.to_numeric(df_viz[\"quantity\"])\n",
    "\n",
    "        # Sort by timestamp\n",
    "        df_viz = df_viz.sort_values(\"timestamp\")\n",
    "\n",
    "        console.print(f\"[green]✓ Prepared {len(df_viz)} trades for visualization[/green]\")\n",
    "        console.print(f\"[yellow]Time range:[/yellow] {df_viz['timestamp'].min()} to {df_viz['timestamp'].max()}\")\n",
    "        console.print(f\"[yellow]Symbols:[/yellow] {', '.join(df_viz['symbol'].unique())}\")\n",
    "\n",
    "        # Separate by symbol for plotting\n",
    "        symbols = df_viz[\"symbol\"].unique()\n",
    "        df_by_symbol = {symbol: df_viz[df_viz[\"symbol\"] == symbol] for symbol in symbols}\n",
    "\n",
    "        console.print(\"\\n[green]✓ Data ready for plotting[/green]\")\n",
    "\n",
    "    else:\n",
    "        console.print(\"[yellow]⚠ No trades available for visualization[/yellow]\")\n",
    "        df_viz = None\n",
    "        df_by_symbol = {}\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error preparing data: {e}[/bold red]\")\n",
    "    df_viz = None\n",
    "    df_by_symbol = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l30br5u4b4t",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price scatter plot by symbol\n",
    "if df_by_symbol:\n",
    "    console.print(\"\\n[bold cyan]Plotting price charts...[/bold cyan]\\n\")\n",
    "\n",
    "    num_symbols = len(df_by_symbol)\n",
    "    fig, axes = plt.subplots(num_symbols, 1, figsize=(14, 5 * num_symbols), sharex=True)\n",
    "\n",
    "    # Handle single symbol case\n",
    "    if num_symbols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Color maps for different symbols\n",
    "    color_maps = [\"viridis\", \"plasma\", \"inferno\", \"magma\", \"cividis\"]\n",
    "\n",
    "    for idx, (symbol, df_symbol) in enumerate(df_by_symbol.items()):\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Scatter plot with quantity as color\n",
    "        scatter = ax.scatter(\n",
    "            df_symbol[\"timestamp\"],\n",
    "            df_symbol[\"price\"],\n",
    "            c=df_symbol[\"quantity\"],\n",
    "            cmap=color_maps[idx % len(color_maps)],\n",
    "            alpha=0.6,\n",
    "            s=50,\n",
    "            edgecolors='black',\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "        # Labels and title\n",
    "        ax.set_ylabel(f\"Price ({df_symbol['currency'].iloc[0]})\", fontsize=12, fontweight='bold')\n",
    "        ax.set_title(f\"{symbol} Live Trades - Price Over Time\", fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label(\"Trade Quantity\", fontsize=10)\n",
    "\n",
    "        # Add price statistics\n",
    "        mean_price = df_symbol[\"price\"].mean()\n",
    "        std_price = df_symbol[\"price\"].std()\n",
    "        ax.axhline(y=mean_price, color='red', linestyle='--', linewidth=1, alpha=0.7, label=f'Mean: {mean_price:.2f}')\n",
    "        ax.axhline(y=mean_price + std_price, color='orange', linestyle=':', linewidth=1, alpha=0.5, label=f'+1σ: {mean_price + std_price:.2f}')\n",
    "        ax.axhline(y=mean_price - std_price, color='orange', linestyle=':', linewidth=1, alpha=0.5, label=f'-1σ: {mean_price - std_price:.2f}')\n",
    "        ax.legend(loc='upper right', fontsize=8)\n",
    "\n",
    "    # Format x-axis (time)\n",
    "    axes[-1].set_xlabel(\"Time (UTC)\", fontsize=12, fontweight='bold')\n",
    "    axes[-1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    console.print(\"[green]✓ Price charts rendered[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for plotting[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uwv03yjrdy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume bar chart (30-second buckets)\n",
    "if df_viz is not None and len(df_viz) > 0:\n",
    "    console.print(\"\\n[bold cyan]Plotting volume chart...[/bold cyan]\\n\")\n",
    "\n",
    "    # Create time buckets (30 seconds)\n",
    "    df_viz[\"time_bucket\"] = df_viz[\"timestamp\"].dt.floor(\"30S\")\n",
    "\n",
    "    # Aggregate volume by bucket and symbol\n",
    "    volume_agg = df_viz.groupby([\"time_bucket\", \"symbol\"])[\"quantity\"].sum().reset_index()\n",
    "\n",
    "    # Create volume chart\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    # Plot bars for each symbol\n",
    "    symbols = volume_agg[\"symbol\"].unique()\n",
    "    bar_width = pd.Timedelta(seconds=30) / (len(symbols) + 1)\n",
    "\n",
    "    for idx, symbol in enumerate(symbols):\n",
    "        symbol_data = volume_agg[volume_agg[\"symbol\"] == symbol]\n",
    "\n",
    "        # Offset bars for each symbol\n",
    "        offset = bar_width * (idx - len(symbols) / 2)\n",
    "\n",
    "        ax.bar(\n",
    "            symbol_data[\"time_bucket\"] + offset,\n",
    "            symbol_data[\"quantity\"],\n",
    "            width=bar_width,\n",
    "            label=symbol,\n",
    "            alpha=0.7,\n",
    "            edgecolor='black',\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Time (UTC)\", fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Trading Volume\", fontsize=12, fontweight='bold')\n",
    "    ax.set_title(\"Trading Volume by Symbol (30-second buckets)\", fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    # Format x-axis\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    console.print(\"[green]✓ Volume chart rendered[/green]\")\n",
    "\n",
    "    # Print volume statistics\n",
    "    console.print(\"\\n[bold yellow]Volume Statistics:[/bold yellow]\")\n",
    "    for symbol in symbols:\n",
    "        symbol_trades = df_viz[df_viz[\"symbol\"] == symbol]\n",
    "        total_volume = symbol_trades[\"quantity\"].sum()\n",
    "        avg_trade_size = symbol_trades[\"quantity\"].mean()\n",
    "        console.print(f\"\\n[cyan]{symbol}:[/cyan]\")\n",
    "        console.print(f\"  Total volume: {total_volume:.4f}\")\n",
    "        console.print(f\"  Average trade size: {avg_trade_size:.6f}\")\n",
    "        console.print(f\"  Number of trades: {len(symbol_trades):,}\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for volume chart[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ueafj2wpd6s",
   "metadata": {},
   "source": "## 11. Pipeline Metrics Visualization\n\nThe K2 platform exposes comprehensive metrics through Prometheus. Let's query and visualize:\n1. **Consumer throughput** - Messages consumed per second over time\n2. **Binance connection status** - WebSocket connection health\n3. **Message lag** - Consumer lag behind Kafka\n4. **Error rates** - Any errors in the pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfvgnw04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query current metrics from Prometheus\n",
    "console.print(\"\\n[bold cyan]Querying Prometheus metrics...[/bold cyan]\\n\")\n",
    "\n",
    "import requests\n",
    "\n",
    "prom_url = \"http://localhost:9090/api/v1/query\"\n",
    "\n",
    "def query_prometheus(metric_name):\n",
    "    \"\"\"Query Prometheus for a metric.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(prom_url, params={\"query\": metric_name}, timeout=5)\n",
    "        data = response.json()\n",
    "\n",
    "        if data[\"status\"] == \"success\" and data[\"data\"][\"result\"]:\n",
    "            # Return all results\n",
    "            return data[\"data\"][\"result\"]\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]Error querying {metric_name}: {e}[/red]\")\n",
    "        return []\n",
    "\n",
    "# Query key metrics\n",
    "metrics_to_check = [\n",
    "    (\"k2_kafka_messages_consumed_total\", \"Total messages consumed\"),\n",
    "    (\"k2_kafka_messages_produced_total\", \"Total messages produced\"),\n",
    "    (\"k2_binance_messages_received_total\", \"Total Binance messages received\"),\n",
    "    (\"k2_binance_connection_status\", \"Binance connection status\"),\n",
    "]\n",
    "\n",
    "# Create metrics summary table\n",
    "metrics_table = Table(title=\"Current Pipeline Metrics\", show_header=True)\n",
    "metrics_table.add_column(\"Metric\", style=\"cyan\")\n",
    "metrics_table.add_column(\"Description\", style=\"yellow\")\n",
    "metrics_table.add_column(\"Value\", style=\"green\")\n",
    "\n",
    "for metric_name, description in metrics_to_check:\n",
    "    results = query_prometheus(metric_name)\n",
    "\n",
    "    if results:\n",
    "        # Sum values if multiple series\n",
    "        total = sum(float(r[\"value\"][1]) for r in results)\n",
    "        metrics_table.add_row(metric_name, description, f\"{total:,.0f}\")\n",
    "    else:\n",
    "        metrics_table.add_row(metric_name, description, \"N/A\")\n",
    "\n",
    "console.print(metrics_table)\n",
    "console.print(\"\\n[green]✓ Current metrics retrieved[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rcml4lx25vs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query time-series metrics for visualization\n",
    "console.print(\"\\n[bold cyan]Querying time-series metrics (last 10 minutes)...[/bold cyan]\\n\")\n",
    "\n",
    "prom_range_url = \"http://localhost:9090/api/v1/query_range\"\n",
    "\n",
    "def query_prometheus_range(query, duration_minutes=10):\n",
    "    \"\"\"Query Prometheus for a time-series metric.\"\"\"\n",
    "    try:\n",
    "        end_time = datetime.now(UTC)\n",
    "        start_time = end_time - timedelta(minutes=duration_minutes)\n",
    "\n",
    "        params = {\n",
    "            \"query\": query,\n",
    "            \"start\": int(start_time.timestamp()),\n",
    "            \"end\": int(end_time.timestamp()),\n",
    "            \"step\": \"15s\",\n",
    "        }\n",
    "\n",
    "        response = requests.get(prom_range_url, params=params, timeout=10)\n",
    "        data = response.json()\n",
    "\n",
    "        if data[\"status\"] == \"success\" and data[\"data\"][\"result\"]:\n",
    "            # Parse time-series data\n",
    "            all_series = []\n",
    "            for series in data[\"data\"][\"result\"]:\n",
    "                values = series[\"values\"]\n",
    "                timestamps = [datetime.fromtimestamp(v[0], tz=UTC) for v in values]\n",
    "                vals = [float(v[1]) for v in values]\n",
    "                labels = series[\"metric\"]\n",
    "\n",
    "                all_series.append({\n",
    "                    \"timestamps\": timestamps,\n",
    "                    \"values\": vals,\n",
    "                    \"labels\": labels,\n",
    "                })\n",
    "\n",
    "            return all_series\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        console.print(f\"[red]Error querying range: {e}[/red]\")\n",
    "        return []\n",
    "\n",
    "# Query consumer throughput (rate of messages consumed)\n",
    "throughput_series = query_prometheus_range(\"rate(k2_kafka_messages_consumed_total[1m])\", duration_minutes=10)\n",
    "\n",
    "if throughput_series:\n",
    "    console.print(f\"[green]✓ Found {len(throughput_series)} throughput series[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No throughput data available (might need to wait for metrics to populate)[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff560dgxxk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot throughput metrics\n",
    "if throughput_series:\n",
    "    console.print(\"\\n[bold cyan]Plotting consumer throughput...[/bold cyan]\\n\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    for series in throughput_series:\n",
    "        # Get label for legend\n",
    "        topic = series[\"labels\"].get(\"topic\", \"unknown\")\n",
    "        label = f\"Topic: {topic}\"\n",
    "\n",
    "        # Plot line\n",
    "        ax.plot(\n",
    "            series[\"timestamps\"],\n",
    "            series[\"values\"],\n",
    "            linewidth=2,\n",
    "            marker=\"o\",\n",
    "            markersize=4,\n",
    "            label=label,\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "    ax.set_xlabel(\"Time (UTC)\", fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Messages/second\", fontsize=12, fontweight='bold')\n",
    "    ax.set_title(\"Consumer Throughput (Last 10 Minutes)\", fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "    # Format x-axis\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate statistics\n",
    "    all_values = []\n",
    "    for series in throughput_series:\n",
    "        all_values.extend(series[\"values\"])\n",
    "\n",
    "    if all_values:\n",
    "        avg_throughput = np.mean(all_values)\n",
    "        max_throughput = np.max(all_values)\n",
    "        min_throughput = np.min(all_values)\n",
    "\n",
    "        console.print(\"\\n[bold yellow]Throughput Statistics:[/bold yellow]\")\n",
    "        console.print(f\"  Average: {avg_throughput:.2f} msg/s\")\n",
    "        console.print(f\"  Peak: {max_throughput:.2f} msg/s\")\n",
    "        console.print(f\"  Minimum: {min_throughput:.2f} msg/s\")\n",
    "\n",
    "    console.print(\"\\n[green]✓ Throughput chart rendered[/green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No throughput data to plot[/yellow]\")\n",
    "    console.print(\"[yellow]Metrics may not be available yet. Run the consumer pipeline and wait 1-2 minutes.[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6lugc9nlia",
   "metadata": {},
   "source": "## 12. Data Quality Checks\n\nLet's validate data integrity and quality across the pipeline:\n1. **Schema validation** - All required fields present and non-null\n2. **Sequence gap detection** - Check for missing sequence numbers\n3. **Duplicate detection** - Find duplicate message_ids or trade_ids\n4. **Data quality score** - Overall quality assessment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0euf6525mr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema validation\n",
    "console.print(\"\\n[bold cyan]Validating schema compliance...[/bold cyan]\\n\")\n",
    "\n",
    "try:\n",
    "    # Query trades for validation\n",
    "    validation_trades = engine.query_trades(limit=1000)\n",
    "\n",
    "    if validation_trades:\n",
    "        df_validation = pd.DataFrame(validation_trades)\n",
    "\n",
    "        # Required fields according to v2 schema\n",
    "        required_fields = [\n",
    "            \"message_id\", \"trade_id\", \"symbol\", \"exchange\", \"asset_class\",\n",
    "            \"timestamp\", \"price\", \"quantity\", \"currency\", \"side\",\n",
    "            \"source_sequence\", \"ingestion_timestamp\", \"platform_sequence\",\n",
    "        ]\n",
    "\n",
    "        # Create validation table\n",
    "        validation_table = Table(title=\"Schema Validation Results\", show_header=True)\n",
    "        validation_table.add_column(\"Field\", style=\"cyan\")\n",
    "        validation_table.add_column(\"Present\", style=\"green\")\n",
    "        validation_table.add_column(\"Null Count\", style=\"yellow\")\n",
    "        validation_table.add_column(\"Status\", style=\"magenta\")\n",
    "\n",
    "        all_valid = True\n",
    "        for field in required_fields:\n",
    "            present = field in df_validation.columns\n",
    "\n",
    "            if present:\n",
    "                null_count = df_validation[field].isnull().sum()\n",
    "                status = \"✓ Valid\" if null_count == 0 else \"⚠ Has nulls\"\n",
    "\n",
    "                if null_count > 0:\n",
    "                    all_valid = False\n",
    "\n",
    "                validation_table.add_row(\n",
    "                    field,\n",
    "                    \"✓ Yes\" if present else \"✗ No\",\n",
    "                    str(null_count),\n",
    "                    status\n",
    "                )\n",
    "            else:\n",
    "                validation_table.add_row(field, \"✗ No\", \"N/A\", \"✗ Missing\")\n",
    "                all_valid = False\n",
    "\n",
    "        console.print(validation_table)\n",
    "\n",
    "        if all_valid:\n",
    "            console.print(\"\\n[bold green]✓ All required fields present with no nulls[/bold green]\")\n",
    "        else:\n",
    "            console.print(\"\\n[bold yellow]⚠ Some fields have issues (see table above)[/bold yellow]\")\n",
    "    else:\n",
    "        console.print(\"[yellow]⚠ No data available for schema validation[/yellow]\")\n",
    "        df_validation = None\n",
    "\n",
    "except Exception as e:\n",
    "    console.print(f\"[bold red]✗ Error in schema validation: {e}[/bold red]\")\n",
    "    df_validation = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3rqzd15uhm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence gap detection\n",
    "console.print(\"\\n[bold cyan]Detecting sequence gaps...[/bold cyan]\\n\")\n",
    "\n",
    "if df_validation is not None and len(df_validation) > 0:\n",
    "    gaps_detected = []\n",
    "\n",
    "    # Check for gaps per symbol\n",
    "    for symbol in df_validation[\"symbol\"].unique():\n",
    "        df_symbol = df_validation[df_validation[\"symbol\"] == symbol].copy()\n",
    "\n",
    "        # Sort by source_sequence\n",
    "        df_symbol = df_symbol.sort_values(\"source_sequence\")\n",
    "\n",
    "        # Check for gaps in source_sequence\n",
    "        if \"source_sequence\" in df_symbol.columns:\n",
    "            sequences = df_symbol[\"source_sequence\"].dropna().astype(int).values\n",
    "\n",
    "            if len(sequences) > 1:\n",
    "                # Find gaps\n",
    "                min_seq = int(sequences.min())\n",
    "                max_seq = int(sequences.max())\n",
    "                expected = set(range(min_seq, max_seq + 1))\n",
    "                actual = set(sequences)\n",
    "                gaps = expected - actual\n",
    "\n",
    "                if gaps:\n",
    "                    gaps_detected.append({\n",
    "                        \"symbol\": symbol,\n",
    "                        \"gap_count\": len(gaps),\n",
    "                        \"sequence_range\": f\"{min_seq} - {max_seq}\",\n",
    "                        \"total_trades\": len(sequences),\n",
    "                    })\n",
    "\n",
    "    if gaps_detected:\n",
    "        console.print(\"[yellow]⚠ Sequence gaps detected:[/yellow]\\n\")\n",
    "\n",
    "        gaps_table = Table(title=\"Sequence Gaps by Symbol\", show_header=True)\n",
    "        gaps_table.add_column(\"Symbol\", style=\"cyan\")\n",
    "        gaps_table.add_column(\"Gap Count\", style=\"yellow\")\n",
    "        gaps_table.add_column(\"Sequence Range\", style=\"green\")\n",
    "        gaps_table.add_column(\"Total Trades\", style=\"magenta\")\n",
    "\n",
    "        for gap in gaps_detected:\n",
    "            gaps_table.add_row(\n",
    "                gap[\"symbol\"],\n",
    "                str(gap[\"gap_count\"]),\n",
    "                gap[\"sequence_range\"],\n",
    "                str(gap[\"total_trades\"])\n",
    "            )\n",
    "\n",
    "        console.print(gaps_table)\n",
    "        console.print(\"\\n[yellow]Note: Sequence gaps are normal in live streaming (network delays, filtering, etc.)[/yellow]\")\n",
    "    else:\n",
    "        console.print(\"[bold green]✓ No sequence gaps detected[/bold green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for sequence gap detection[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeqoq4hh6qf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicate detection\n",
    "console.print(\"\\n[bold cyan]Detecting duplicates...[/bold cyan]\\n\")\n",
    "\n",
    "if df_validation is not None and len(df_validation) > 0:\n",
    "    # Check for duplicate message_ids\n",
    "    duplicate_msgs = df_validation[df_validation.duplicated(subset=[\"message_id\"], keep=False)]\n",
    "\n",
    "    console.print(f\"[yellow]Duplicate message_ids:[/yellow] {len(duplicate_msgs)}\")\n",
    "\n",
    "    if len(duplicate_msgs) > 0:\n",
    "        console.print(\"[yellow]⚠ Found duplicate message IDs:[/yellow]\")\n",
    "        display(duplicate_msgs[[\"message_id\", \"symbol\", \"timestamp\", \"trade_id\"]].head(10))\n",
    "    else:\n",
    "        console.print(\"[green]✓ No duplicate message_ids[/green]\")\n",
    "\n",
    "    # Check for duplicate trade_ids\n",
    "    duplicate_trades = df_validation[df_validation.duplicated(subset=[\"trade_id\"], keep=False)]\n",
    "\n",
    "    console.print(f\"\\n[yellow]Duplicate trade_ids:[/yellow] {len(duplicate_trades)}\")\n",
    "\n",
    "    if len(duplicate_trades) > 0:\n",
    "        console.print(\"[yellow]⚠ Found duplicate trade IDs:[/yellow]\")\n",
    "        display(duplicate_trades[[\"trade_id\", \"symbol\", \"timestamp\", \"message_id\"]].head(10))\n",
    "    else:\n",
    "        console.print(\"[green]✓ No duplicate trade_ids[/green]\")\n",
    "\n",
    "    # Store for quality score\n",
    "    has_duplicates = len(duplicate_msgs) > 0 or len(duplicate_trades) > 0\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for duplicate detection[/yellow]\")\n",
    "    has_duplicates = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3xaisggigzv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality summary\n",
    "console.print(\"\\n\" + \"=\" * 60)\n",
    "console.print(\"[bold cyan]DATA QUALITY REPORT[/bold cyan]\")\n",
    "console.print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "if df_validation is not None and len(df_validation) > 0:\n",
    "    console.print(\"[yellow]Dataset:[/yellow]\")\n",
    "    console.print(f\"  Total trades analyzed: {len(df_validation):,}\")\n",
    "    console.print(f\"  Unique symbols: {df_validation['symbol'].nunique()}\")\n",
    "    console.print(f\"  Time range: {df_validation['timestamp'].min()} to {df_validation['timestamp'].max()}\")\n",
    "\n",
    "    console.print(\"\\n[yellow]Quality Checks:[/yellow]\")\n",
    "\n",
    "    # Schema validation\n",
    "    schema_pass = all_valid if 'all_valid' in locals() else True\n",
    "    console.print(f\"  {'✓' if schema_pass else '✗'} Schema validation: {'All fields present and valid' if schema_pass else 'Some fields have issues'}\")\n",
    "\n",
    "    # Sequence gaps\n",
    "    gaps_pass = len(gaps_detected) == 0 if 'gaps_detected' in locals() else True\n",
    "    console.print(f\"  {'✓' if gaps_pass else '⚠'} Sequence gaps: {len(gaps_detected) if 'gaps_detected' in locals() else 0} symbols with gaps\")\n",
    "\n",
    "    # Duplicates\n",
    "    dup_pass = not has_duplicates\n",
    "    console.print(f\"  {'✓' if dup_pass else '✗'} Duplicate detection: {'No duplicates' if dup_pass else 'Duplicates found'}\")\n",
    "\n",
    "    # Calculate quality score\n",
    "    score = 100\n",
    "    if not schema_pass:\n",
    "        score -= 30\n",
    "    if not gaps_pass:\n",
    "        score -= 10  # Gaps are normal in streaming\n",
    "    if not dup_pass:\n",
    "        score -= 40\n",
    "\n",
    "    # Display score with color\n",
    "    if score >= 90:\n",
    "        score_color = \"green\"\n",
    "        rating = \"EXCELLENT\"\n",
    "    elif score >= 70:\n",
    "        score_color = \"yellow\"\n",
    "        rating = \"GOOD\"\n",
    "    elif score >= 50:\n",
    "        score_color = \"yellow\"\n",
    "        rating = \"FAIR\"\n",
    "    else:\n",
    "        score_color = \"red\"\n",
    "        rating = \"POOR\"\n",
    "\n",
    "    console.print(f\"\\n[bold {score_color}]Data Quality Score: {score}/100 ({rating})[/bold {score_color}]\")\n",
    "\n",
    "    # Recommendations\n",
    "    if score < 100:\n",
    "        console.print(\"\\n[yellow]Recommendations:[/yellow]\")\n",
    "        if not schema_pass:\n",
    "            console.print(\"  • Fix schema validation issues (missing or null fields)\")\n",
    "        if not dup_pass:\n",
    "            console.print(\"  • Investigate duplicate detection logic in consumer\")\n",
    "        if not gaps_pass:\n",
    "            console.print(\"  • Sequence gaps are normal but monitor gap frequency\")\n",
    "    else:\n",
    "        console.print(\"\\n[bold green]✓ Data quality is excellent! Pipeline is operating correctly.[/bold green]\")\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for quality report[/yellow]\")\n",
    "\n",
    "console.print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w4otl2o6kt",
   "metadata": {},
   "source": "## 13. Vendor Data Analysis\n\nThe v2 schema's `vendor_data` field is key to multi-source compatibility. Let's analyze how Binance-specific fields are preserved:\n1. **Field coverage** - Which vendor fields are present\n2. **Field statistics** - Coverage percentage across trades\n3. **Example data** - Sample vendor_data entries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltba4ic4ceq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse vendor_data and analyze field coverage\n",
    "console.print(\"\\n[bold cyan]Analyzing vendor_data fields...[/bold cyan]\\n\")\n",
    "\n",
    "import json\n",
    "\n",
    "if df_validation is not None and len(df_validation) > 0:\n",
    "    # Parse vendor_data JSON for all trades\n",
    "    df_validation[\"vendor_parsed\"] = df_validation[\"vendor_data\"].apply(\n",
    "        lambda x: json.loads(x) if pd.notna(x) and x else {}\n",
    "    )\n",
    "\n",
    "    # Collect all unique vendor fields\n",
    "    all_vendor_fields = set()\n",
    "    for vendor_dict in df_validation[\"vendor_parsed\"]:\n",
    "        all_vendor_fields.update(vendor_dict.keys())\n",
    "\n",
    "    console.print(f\"[green]✓ Found {len(all_vendor_fields)} unique Binance-specific fields[/green]\\n\")\n",
    "    console.print(\"[yellow]Binance vendor_data fields:[/yellow]\")\n",
    "    for field in sorted(all_vendor_fields):\n",
    "        console.print(f\"  • {field}\")\n",
    "\n",
    "    # Calculate field coverage\n",
    "    field_counts = {}\n",
    "    for field in all_vendor_fields:\n",
    "        count = sum(1 for vd in df_validation[\"vendor_parsed\"] if field in vd)\n",
    "        field_counts[field] = count\n",
    "\n",
    "    # Create coverage table\n",
    "    coverage_table = Table(title=\"Vendor Field Coverage Statistics\", show_header=True)\n",
    "    coverage_table.add_column(\"Field\", style=\"cyan\")\n",
    "    coverage_table.add_column(\"Count\", style=\"green\")\n",
    "    coverage_table.add_column(\"Coverage %\", style=\"yellow\")\n",
    "\n",
    "    total_trades = len(df_validation)\n",
    "    for field in sorted(field_counts.keys(), key=lambda x: field_counts[x], reverse=True):\n",
    "        count = field_counts[field]\n",
    "        percentage = (count / total_trades) * 100\n",
    "        coverage_table.add_row(\n",
    "            field,\n",
    "            f\"{count:,}\",\n",
    "            f\"{percentage:.1f}%\"\n",
    "        )\n",
    "\n",
    "    console.print(\"\\n\")\n",
    "    console.print(coverage_table)\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for vendor_data analysis[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aovu7yauu8k",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example vendor_data entries\n",
    "console.print(\"\\n[bold cyan]Example vendor_data entries:[/bold cyan]\\n\")\n",
    "\n",
    "if df_validation is not None and len(df_validation) > 0:\n",
    "    # Show 3 example vendor_data entries\n",
    "    num_examples = min(3, len(df_validation))\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        trade = df_validation.iloc[i]\n",
    "        vendor_data = trade[\"vendor_parsed\"]\n",
    "\n",
    "        console.print(f\"[bold yellow]Trade {i+1} ({trade['symbol']}):[/bold yellow]\")\n",
    "        console.print(\"  [cyan]Core fields:[/cyan]\")\n",
    "        console.print(f\"    • Price: {trade['price']} {trade['currency']}\")\n",
    "        console.print(f\"    • Quantity: {trade['quantity']}\")\n",
    "        console.print(f\"    • Side: {trade['side']}\")\n",
    "        console.print(f\"    • Timestamp: {trade['timestamp']}\")\n",
    "\n",
    "        console.print(\"  [cyan]Vendor data (Binance-specific):[/cyan]\")\n",
    "        for key, value in vendor_data.items():\n",
    "            console.print(f\"    • {key}: {value}\")\n",
    "        console.print()\n",
    "\n",
    "    console.print(\"[bold green]Key Insight:[/bold green]\")\n",
    "    console.print(\"  • Core fields (price, quantity, side) are normalized across all exchanges\")\n",
    "    console.print(\"  • vendor_data preserves exchange-specific fields without modification\")\n",
    "    console.print(\"  • Same v2 schema works for ASX (equities), Binance (crypto), and future exchanges\")\n",
    "    console.print(\"  • Enables advanced analysis using exchange-specific metadata\")\n",
    "\n",
    "else:\n",
    "    console.print(\"[yellow]⚠ No data available for vendor_data examples[/yellow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z1a5i3emkv",
   "metadata": {},
   "source": "## 14. Architecture Summary & Links\n\n### What We Demonstrated\n\nThis notebook showcased the complete end-to-end Binance cryptocurrency streaming pipeline:\n\n| Component | Description | Status |\n|-----------|-------------|--------|\n| **Binance WebSocket** | Live crypto trades streaming (BTC, ETH) | ✓ Validated |\n| **Kafka Broker** | Message queueing with 4 partitions | ✓ Validated |\n| **Schema Registry** | V2 Avro schema management | ✓ Validated |\n| **Consumer Pipeline** | Batch processing (Kafka → Iceberg) | ✓ Validated |\n| **Iceberg Lakehouse** | ACID transactions, Parquet storage | ✓ Validated |\n| **Query Engine** | DuckDB analytical queries | ✓ Validated |\n| **V2 Hybrid Schema** | Standard fields + vendor_data map | ✓ Validated |\n| **Multi-Asset-Class** | Crypto + Equities support | ✓ Validated |\n| **Prometheus Metrics** | Real-time pipeline monitoring | ✓ Validated |\n| **Data Quality** | Schema, sequence, duplicate checks | ✓ Validated |\n\n---\n\n### Key Achievements (Phase 2 Prep)\n\n**V2 Schema Evolution:**\n- Industry-standard core fields (symbol, price, quantity, side, etc.)\n- `vendor_data` JSON map for exchange-specific fields\n- Works across ASX (equities) and Binance (crypto)\n- Future-proof for Coinbase, Kraken, and other exchanges\n\n**Multi-Source Capability:**\n- Batch ingestion: ASX CSV files\n- Streaming ingestion: Binance WebSocket\n- Same v2 schema for both sources\n- Unified query interface\n\n**Production-Grade Features:**\n- SSL/TLS support\n- Exponential backoff and circuit breakers\n- Prometheus metrics (7 Binance-specific metrics)\n- Sub-second query performance\n- 138+ msg/s consumer throughput\n\n**Data Guarantees:**\n- ACID transactions via Apache Iceberg\n- Exactly-once semantics with idempotent producers\n- Schema evolution support\n- Time-travel queries\n\n---\n\n### Performance Metrics\n\nFrom E2E validation session (2026-01-13):\n\n- **Messages Received**: 69,666+ trades from Binance\n- **Messages Written**: 5,000+ trades to Iceberg\n- **Consumer Throughput**: 138 msg/s\n- **Query Latency**: Sub-second for 5,000 records\n- **Uptime**: 0 connection errors during demo\n\n---\n\n### Links & Resources\n\n**Local Services:**\n- **Prometheus Metrics**: [http://localhost:9090](http://localhost:9090)\n- **Grafana Dashboards**: [http://localhost:3000](http://localhost:3000) (admin/admin)\n- **Kafka UI**: [http://localhost:8080](http://localhost:8080)\n- **MinIO Console**: [http://localhost:9001](http://localhost:9001) (minioadmin/minioadmin)\n- **Schema Registry**: [http://localhost:8081](http://localhost:8081)\n\n**Documentation:**\n- Phase 2 Prep README: `docs/phases/phase-2-prep/README.md`\n- E2E Demo Success Summary: `docs/operations/e2e-demo-success-summary.md`\n- V2 Schema Design: `docs/architecture/schema-design-v2.md`\n- Streaming Architecture: `docs/architecture/streaming-architecture.md`\n\n**Scripts:**\n- Binance Streaming: `scripts/binance_stream.py`\n- Infrastructure Init: `scripts/init_e2e_demo.py`\n- Consumer: `src/k2/ingestion/consumer.py`\n- Query Engine: `src/k2/query/engine.py`\n\n---\n\n### Next Steps\n\n**Explore the Platform:**\n1. View metrics in Prometheus: [http://localhost:9090](http://localhost:9090)\n2. Create Grafana dashboards for real-time monitoring\n3. Query trades via the REST API (coming in Phase 2 Demo Enhancements)\n4. Try the ASX historical data demo: `notebooks/demo.ipynb`\n\n**Extend the Pipeline:**\n1. Add more crypto symbols (BNB, SOL, ADA, etc.)\n2. Stream from multiple exchanges (see Section 15 TODO)\n3. Build custom analytics queries\n4. Set up alerting rules in Prometheus\n\n**Production Deployment:**\n1. Enable SSL certificate verification\n2. Add Kafka broker replication (3+ brokers)\n3. Deploy distributed Iceberg catalog\n4. Set up authentication and authorization\n5. Configure data retention policies\n\n---\n\n### Architecture Highlights\n\n**Why This Architecture?**\n\n1. **Apache Kafka**: Industry-standard streaming platform, horizontal scalability, fault tolerance\n2. **Apache Iceberg**: ACID guarantees, time-travel queries, schema evolution, S3 compatibility\n3. **DuckDB**: Embedded analytics, Parquet-native, sub-second queries, no separate server\n4. **Avro + Schema Registry**: Schema evolution, compact serialization, version management\n5. **Prometheus + Grafana**: Real-time metrics, alerting, visualization\n\n**Design Principles:**\n\n- **Separation of Concerns**: Ingestion → Storage → Query\n- **Idempotency**: Safe retries, exactly-once semantics\n- **Schema Evolution**: Forward and backward compatibility\n- **Multi-Tenancy**: Asset classes, exchanges, symbols\n- **Observability**: Metrics, logs, traces\n\n---\n\n**Congratulations!** You've completed the Binance E2E streaming pipeline demo.\n\nThis platform demonstrates **Principal/Staff-level data engineering** with production-grade architecture, multi-source compatibility, and sub-second query performance."
  },
  {
   "cell_type": "markdown",
   "id": "39xvyyrv9vl",
   "metadata": {},
   "source": "## 15. TODO - Cross-Exchange Comparison (Future Enhancement)\n\n### Goal\n\nCompare cryptocurrency prices across multiple exchanges (Binance, Coinbase, Kraken) to:\n- Demonstrate the platform's multi-source capability\n- Identify arbitrage opportunities\n- Validate v2 schema works across exchanges\n- Show value of vendor_data for exchange-specific analysis\n\n---\n\n### Implementation Plan\n\n#### 1. Add Coinbase WebSocket Client\n\n**File**: `src/k2/ingestion/coinbase_client.py`\n\nSimilar to `BinanceWebSocketClient`, implement:\n- WebSocket connection to `wss://ws-feed.exchange.coinbase.com`\n- Subscribe to `matches` channel for BTC-USD, ETH-USD\n- Convert Coinbase messages to v2 schema\n- Store Coinbase-specific fields in `vendor_data`:\n  - `maker_order_id`, `taker_order_id`\n  - `sequence`, `product_id`\n  - `maker_fee`, `taker_fee`\n\n**Example Coinbase Message:**\n```json\n{\n  \"type\": \"match\",\n  \"trade_id\": 12345678,\n  \"maker_order_id\": \"abc123\",\n  \"taker_order_id\": \"def456\",\n  \"side\": \"buy\",\n  \"size\": \"0.05\",\n  \"price\": \"65000.00\",\n  \"product_id\": \"BTC-USD\",\n  \"sequence\": 987654321,\n  \"time\": \"2024-01-01T00:00:00.000000Z\"\n}\n```\n\n**V2 Conversion:**\n- `symbol`: \"BTCUSD\" (normalized)\n- `exchange`: \"COINBASE\"\n- `asset_class`: \"crypto\"\n- `vendor_data`: Coinbase-specific fields\n\n---\n\n#### 2. Add Kraken WebSocket Client\n\n**File**: `src/k2/ingestion/kraken_client.py`\n\nSimilar implementation for Kraken:\n- WebSocket connection to `wss://ws.kraken.com`\n- Subscribe to `trade` channel for XBT/USD, ETH/USD\n- Convert Kraken messages to v2 schema\n- Store Kraken-specific fields in `vendor_data`:\n  - `order_type` (market, limit)\n  - `misc` (additional flags)\n\n---\n\n#### 3. Create Multi-Exchange Streaming Service\n\n**File**: `scripts/multi_exchange_stream.py`\n\nConnect to multiple exchanges simultaneously:\n\n```python\nimport asyncio\nfrom k2.ingestion.binance_client import BinanceWebSocketClient\nfrom k2.ingestion.coinbase_client import CoinbaseWebSocketClient\nfrom k2.ingestion.kraken_client import KrakenWebSocketClient\nfrom k2.ingestion.producer import MarketDataProducer\n\nasync def stream_all_exchanges():\n    producer = MarketDataProducer(schema_version=\"v2\")\n    \n    # Create clients\n    binance = BinanceWebSocketClient(symbols=[\"BTCUSDT\", \"ETHUSDT\"], producer=producer)\n    coinbase = CoinbaseWebSocketClient(symbols=[\"BTC-USD\", \"ETH-USD\"], producer=producer)\n    kraken = KrakenWebSocketClient(symbols=[\"XBT/USD\", \"ETH/USD\"], producer=producer)\n    \n    # Connect all\n    await asyncio.gather(\n        binance.connect(),\n        coinbase.connect(),\n        kraken.connect(),\n    )\n    \n    # Stream indefinitely\n    await asyncio.Event().wait()\n\nif __name__ == \"__main__\":\n    asyncio.run(stream_all_exchanges())\n```\n\n**Kafka Topics:**\n- Option 1: Single topic `market.crypto.trades` (all exchanges)\n- Option 2: Per-exchange topics:\n  - `market.crypto.trades.binance`\n  - `market.crypto.trades.coinbase`\n  - `market.crypto.trades.kraken`\n\n---\n\n#### 4. Notebook Enhancements\n\nAdd new cells to this notebook:\n\n**Cell: Query trades from all exchanges**\n```python\n# Query BTCUSDT from all exchanges\ntrades_binance = engine.query_trades(symbol=\"BTCUSDT\", exchange=\"BINANCE\", limit=1000)\ntrades_coinbase = engine.query_trades(symbol=\"BTCUSD\", exchange=\"COINBASE\", limit=1000)\ntrades_kraken = engine.query_trades(symbol=\"XBTUSD\", exchange=\"KRAKEN\", limit=1000)\n\n# Normalize symbol names for comparison\ndf_binance = pd.DataFrame(trades_binance)\ndf_coinbase = pd.DataFrame(trades_coinbase)\ndf_kraken = pd.DataFrame(trades_kraken)\n```\n\n**Cell: Calculate price spreads**\n```python\n# Calculate average prices\navg_binance = df_binance[\"price\"].mean()\navg_coinbase = df_coinbase[\"price\"].mean()\navg_kraken = df_kraken[\"price\"].mean()\n\n# Calculate spreads (arbitrage opportunities)\nspread_binance_coinbase = abs(avg_binance - avg_coinbase)\nspread_binance_kraken = abs(avg_binance - avg_kraken)\nspread_coinbase_kraken = abs(avg_coinbase - avg_kraken)\n\nprint(f\"Price Spreads:\")\nprint(f\"  Binance-Coinbase: ${spread_binance_coinbase:.2f}\")\nprint(f\"  Binance-Kraken: ${spread_binance_kraken:.2f}\")\nprint(f\"  Coinbase-Kraken: ${spread_coinbase_kraken:.2f}\")\n```\n\n**Cell: Visualize price comparison**\n```python\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Plot prices from all exchanges\nax.plot(df_binance[\"timestamp\"], df_binance[\"price\"], label=\"Binance\", alpha=0.7)\nax.plot(df_coinbase[\"timestamp\"], df_coinbase[\"price\"], label=\"Coinbase\", alpha=0.7)\nax.plot(df_kraken[\"timestamp\"], df_kraken[\"price\"], label=\"Kraken\", alpha=0.7)\n\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"Price (USD)\")\nax.set_title(\"BTC Price Comparison Across Exchanges\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n```\n\n**Cell: Heatmap of spreads over time**\n```python\n# Aggregate prices by 1-minute buckets\ndf_binance[\"bucket\"] = df_binance[\"timestamp\"].dt.floor(\"1min\")\ndf_coinbase[\"bucket\"] = df_coinbase[\"timestamp\"].dt.floor(\"1min\")\ndf_kraken[\"bucket\"] = df_kraken[\"timestamp\"].dt.floor(\"1min\")\n\n# Calculate mean price per bucket\nprice_binance = df_binance.groupby(\"bucket\")[\"price\"].mean()\nprice_coinbase = df_coinbase.groupby(\"bucket\")[\"price\"].mean()\nprice_kraken = df_kraken.groupby(\"bucket\")[\"price\"].mean()\n\n# Calculate spread matrix\nspread_matrix = pd.DataFrame({\n    \"Binance-Coinbase\": abs(price_binance - price_coinbase),\n    \"Binance-Kraken\": abs(price_binance - price_kraken),\n    \"Coinbase-Kraken\": abs(price_coinbase - price_kraken),\n})\n\n# Plot heatmap\nimport seaborn as sns\nsns.heatmap(spread_matrix.T, cmap=\"YlOrRd\", cbar_kws={\"label\": \"Spread (USD)\"})\nplt.title(\"Price Spreads Over Time (1-minute buckets)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Exchange Pair\")\nplt.show()\n```\n\n---\n\n### Benefits\n\n**Demonstrates Platform Capabilities:**\n- Multi-source data ingestion (3+ exchanges)\n- V2 schema flexibility (works across all exchanges)\n- vendor_data preserves exchange-specific fields\n- Unified query interface for cross-exchange analysis\n\n**Real-World Use Cases:**\n- **Arbitrage Detection**: Identify profitable price differences\n- **Market Microstructure**: Study price formation across venues\n- **Liquidity Analysis**: Compare trade volumes and spreads\n- **Best Execution**: Route orders to best-priced exchange\n\n**Technical Validation:**\n- Schema evolution: Same v2 schema for Binance, Coinbase, Kraken\n- Query performance: Sub-second queries across millions of trades\n- Data quality: Validate consistency across exchanges\n- Vendor data: Preserve exchange-specific fields for advanced analysis\n\n---\n\n### Example Output\n\n**Price Comparison Table:**\n\n| Exchange | Avg Price | Min Price | Max Price | Spread % | Volume |\n|----------|-----------|-----------|-----------|----------|--------|\n| Binance  | 65,123.45 | 65,000.00 | 65,250.00 | 0.38% | 150.25 BTC |\n| Coinbase | 65,145.20 | 65,020.00 | 65,270.00 | 0.38% | 98.50 BTC |\n| Kraken   | 65,110.80 | 64,990.00 | 65,230.00 | 0.37% | 75.30 BTC |\n\n**Arbitrage Opportunities:**\n\n- Binance → Coinbase: $21.75 spread (0.03%)\n- Kraken → Coinbase: $34.40 spread (0.05%) ← **Best opportunity**\n- Binance → Kraken: $12.65 spread (0.02%)\n\n**Transaction costs**: ~0.1% (typical exchange fees)\n**Profitable if**: Spread > 0.1%\n**Result**: Kraken → Coinbase arbitrage is profitable (0.05% > 0.1%)\n\n---\n\n### Next Steps to Implement\n\n1. **Research APIs**: Study Coinbase and Kraken WebSocket APIs\n2. **Implement clients**: Build CoinbaseWebSocketClient and KrakenWebSocketClient\n3. **Test conversion**: Validate v2 schema conversion for each exchange\n4. **Deploy streaming**: Run multi_exchange_stream.py in production\n5. **Create notebook**: Add cross-exchange comparison cells\n6. **Document findings**: Write report on arbitrage opportunities\n\n**Estimated Time**: 8-12 hours\n**Priority**: Medium (nice-to-have, demonstrates platform flexibility)\n**Dependencies**: Phase 2 Prep complete (v2 schema + Binance streaming)\n\n---\n\n**Note**: This is documented as future work. The platform's v2 schema and architecture are already designed to support multiple exchanges - implementation is straightforward once exchange clients are built."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}