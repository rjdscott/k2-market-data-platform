{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# K2 Platform - Binance Live Streaming E2E Demo\n",
    "\n",
    "**Phase 2 Prep Achievement**: Complete Binance Cryptocurrency Streaming Pipeline\n",
    "\n",
    "This notebook demonstrates the complete end-to-end data pipeline for live cryptocurrency market data streaming, showcasing the achievements of Phase 2 Prep:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  K2 Binance Streaming Architecture                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  Binance WebSocket \u2192 Kafka (Avro) \u2192 Consumer \u2192 Iceberg \u2192 Query     \u2502\n",
    "\u2502      (Live)            (Stream)      (Batch)    (ACID)    (DuckDB) \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2502  \u2022 Real-time crypto trades (BTC, ETH, BNB)                          \u2502\n",
    "\u2502  \u2022 V2 hybrid schema with vendor_data map                            \u2502\n",
    "\u2502  \u2022 Multi-source capability (ASX batch + Binance streaming)          \u2502\n",
    "\u2502  \u2022 Multi-asset-class platform (equities + crypto)                   \u2502\n",
    "\u2502  \u2022 Production-grade resilience (SSL, metrics, circuit breakers)     \u2502\n",
    "\u2502  \u2022 138 msg/s throughput, sub-second queries                         \u2502\n",
    "\u2502                                                                     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "## What This Demo Covers\n",
    "\n",
    "1. **Infrastructure Setup** - Docker services validation and initialization\n",
    "2. **Live Streaming** - Connect to Binance WebSocket, stream real trades\n",
    "3. **Kafka Pipeline** - Message broker with partitioning and Avro serialization\n",
    "4. **Consumer Processing** - Batch processing from Kafka to Iceberg (ACID)\n",
    "5. **Iceberg Lakehouse** - Query 5,000+ trades with DuckDB\n",
    "6. **Real-Time Visualizations** - Price charts, volume analysis, metrics\n",
    "7. **Data Quality** - Schema validation, sequence tracking, duplicate detection\n",
    "8. **Vendor Data** - Exchange-specific fields preserved in JSON\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "- How to connect to live cryptocurrency exchanges via WebSocket\n",
    "- How to build a production-ready streaming data pipeline\n",
    "- How v2 hybrid schemas enable multi-source compatibility\n",
    "- How to achieve ACID guarantees with Apache Iceberg\n",
    "- How to query streaming data with sub-second latency\n",
    "- How to validate data quality in real-time systems\n",
    "\n",
    "**Estimated Time**: 30-45 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 2. Prerequisites & Setup Overview\n",
    "\n",
    "### System Requirements\n",
    "\n",
    "- \u2713 **Docker** (with Docker Compose) - Running all infrastructure services\n",
    "- \u2713 **Python 3.12+** - Managed via `uv` package manager\n",
    "- \u2713 **4GB RAM minimum** - For Kafka, Iceberg, MinIO, PostgreSQL\n",
    "- \u2713 **Internet connection** - For Binance WebSocket streaming\n",
    "- \u2713 **Disk space** - ~2GB for Docker images and data\n",
    "\n",
    "### Docker Services Required\n",
    "\n",
    "| Service | Purpose | Port |\n",
    "|---------|---------|------|\n",
    "| Kafka | Message broker (KRaft mode) | 9092 |\n",
    "| Schema Registry | Avro schema management | 8081 |\n",
    "| MinIO | S3-compatible object storage | 9000, 9001 |\n",
    "| PostgreSQL | Iceberg catalog metadata | 5432 |\n",
    "| Iceberg REST Catalog | Table management | 8181 |\n",
    "| Prometheus | Metrics collection | 9090 |\n",
    "| Grafana | Dashboards and visualization | 3000 |\n",
    "\n",
    "### Python Dependencies\n",
    "\n",
    "All dependencies are managed via `uv` and installed automatically:\n",
    "- `confluent-kafka` - Kafka client with Avro support\n",
    "- `pyiceberg` - Iceberg Python SDK\n",
    "- `duckdb` - Embedded analytics engine\n",
    "- `pandas`, `matplotlib` - Data analysis and visualization\n",
    "- `websockets` - Async WebSocket client for Binance\n",
    "- `rich` - Beautiful terminal output\n",
    "\n",
    "### Network Requirements\n",
    "\n",
    "- Access to `wss://stream.binance.com:9443` (Binance WebSocket)\n",
    "- No VPN restrictions on cryptocurrency exchange APIs\n",
    "\n",
    "### Setup Time Estimate\n",
    "\n",
    "- **First time**: 10-15 minutes (Docker image download + init)\n",
    "- **Subsequent runs**: 2-3 minutes (start services + validation)\n",
    "\n",
    "### Quick Start Commands\n",
    "\n",
    "```bash\n",
    "# Start all Docker services\n",
    "docker compose up -d\n",
    "\n",
    "# Initialize infrastructure (schemas, topics, tables)\n",
    "python scripts/init_e2e_demo.py\n",
    "\n",
    "# Run this notebook!\n",
    "jupyter lab notebooks/binance_e2e_demo.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 3. Environment Setup & Validation\n",
    "\n",
    "Let's start by validating that all required services are running and healthy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library imports\nimport subprocess\nimport sys\nimport time\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\n\nimport numpy as np\n\n# Data processing\nimport pandas as pd\n\n# Add src to path for k2 imports\nsys.path.insert(0, str(Path.cwd().parent / \"src\"))\n\n# Pandas display settings\npd.set_option('display.max_columns', 20)\npd.set_option('display.width', 200)\n\nprint(\"Imports loaded successfully!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# Check Docker services status\nprint(\"\\nChecking Docker services...\\n\")\n\ntry:\n    result = subprocess.run(\n        [\"docker\", \"compose\", \"ps\", \"--format\", \"json\"],\n        capture_output=True,\n        text=True,\n        cwd=Path.cwd().parent,\n        timeout=10\n    )\n\n    if result.returncode != 0:\n        print(\"Docker Compose is not running!\")\n        print(\"\\nPlease start services with: docker compose up -d\")\n        raise Exception(\"Docker services not running\")\n\n    # Parse Docker Compose output\n    import json\n    services = []\n    for line in result.stdout.strip().split('\\n'):\n        if line:\n            try:\n                service = json.loads(line)\n                services.append(service)\n            except json.JSONDecodeError:\n                pass\n\n    # Display status\n    print(\"Docker Services Status\\n\")\n    print(f\"{'Service':<30} {'State':<15} {'Ports':<20}\")\n    print(\"-\" * 65)\n\n    required_services = [\n        \"k2-kafka\", \"k2-schema-registry-1\", \"k2-minio\",\n        \"k2-postgres\", \"k2-iceberg-rest\", \"k2-prometheus\"\n    ]\n\n    running_services = {}\n    for service in services:\n        name = service.get(\"Name\", service.get(\"Service\", \"unknown\"))\n        state = service.get(\"State\", \"unknown\")\n        publishers = service.get(\"Publishers\", [])\n\n        # Format ports\n        ports = \", \".join([f\"{p.get('PublishedPort', '')}\" for p in publishers if p.get('PublishedPort')]) or \"N/A\"\n\n        running_services[name] = state\n        print(f\"{name:<30} {state:<15} {ports:<20}\")\n\n    # Check if all required services are running\n    missing_services = []\n    for svc in required_services:\n        if svc not in running_services or running_services[svc] != \"running\":\n            missing_services.append(svc)\n\n    if missing_services:\n        print(f\"\\nMissing or stopped services: {', '.join(missing_services)}\")\n        print(\"\\nPlease start services with: docker compose up -d\")\n    else:\n        print(\"\\nAll required services are running!\")\n\nexcept FileNotFoundError:\n    print(\"Docker is not installed or not in PATH\")\n    print(\"\\nPlease install Docker: https://docs.docker.com/get-docker/\")\nexcept subprocess.TimeoutExpired:\n    print(\"Docker command timed out\")\nexcept Exception as e:\n    print(f\"Error checking Docker: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Validate service health\nprint(\"\\nValidating service health...\\n\")\n\nimport requests\n\nhealth_checks = []\n\n# 1. Check Schema Registry\ntry:\n    start = time.time()\n    response = requests.get(\"http://localhost:8081/subjects\", timeout=5)\n    latency = (time.time() - start) * 1000\n\n    if response.status_code == 200:\n        subjects = response.json()\n        health_checks.append({\n            \"service\": \"Schema Registry\",\n            \"status\": \"Healthy\",\n            \"latency\": f\"{latency:.0f}ms\",\n            \"detail\": f\"{len(subjects)} schemas\"\n        })\n    else:\n        health_checks.append({\n            \"service\": \"Schema Registry\",\n            \"status\": \"Unhealthy\",\n            \"latency\": f\"{latency:.0f}ms\",\n            \"detail\": f\"Status {response.status_code}\"\n        })\nexcept Exception as e:\n    health_checks.append({\n        \"service\": \"Schema Registry\",\n        \"status\": \"Error\",\n        \"latency\": \"N/A\",\n        \"detail\": str(e)[:50]\n    })\n\n# 2. Check MinIO\ntry:\n    import boto3\n\n    start = time.time()\n    s3_client = boto3.client(\n        \"s3\",\n        endpoint_url=\"http://localhost:9000\",\n        aws_access_key_id=\"admin\",\n        aws_secret_access_key=\"password\",\n    )\n    buckets = s3_client.list_buckets()\n    latency = (time.time() - start) * 1000\n\n    health_checks.append({\n        \"service\": \"MinIO (S3)\",\n        \"status\": \"Healthy\",\n        \"latency\": f\"{latency:.0f}ms\",\n        \"detail\": f\"{len(buckets['Buckets'])} buckets\"\n    })\nexcept Exception as e:\n    health_checks.append({\n        \"service\": \"MinIO (S3)\",\n        \"status\": \"Error\",\n        \"latency\": \"N/A\",\n        \"detail\": str(e)[:50]\n    })\n\n# 3. Check Iceberg REST Catalog\ntry:\n    start = time.time()\n    response = requests.get(\"http://localhost:8181/v1/config\", timeout=5)\n    latency = (time.time() - start) * 1000\n\n    if response.status_code == 200:\n        health_checks.append({\n            \"service\": \"Iceberg REST\",\n            \"status\": \"Healthy\",\n            \"latency\": f\"{latency:.0f}ms\",\n            \"detail\": \"Catalog ready\"\n        })\n    else:\n        health_checks.append({\n            \"service\": \"Iceberg REST\",\n            \"status\": \"Unhealthy\",\n            \"latency\": f\"{latency:.0f}ms\",\n            \"detail\": f\"Status {response.status_code}\"\n        })\nexcept Exception as e:\n    health_checks.append({\n        \"service\": \"Iceberg REST\",\n        \"status\": \"Error\",\n        \"latency\": \"N/A\",\n        \"detail\": str(e)[:50]\n    })\n\n# 4. Check Prometheus\ntry:\n    start = time.time()\n    response = requests.get(\"http://localhost:9090/-/healthy\", timeout=5)\n    latency = (time.time() - start) * 1000\n\n    if response.status_code == 200:\n        health_checks.append({\n            \"service\": \"Prometheus\",\n            \"status\": \"Healthy\",\n            \"latency\": f\"{latency:.0f}ms\",\n            \"detail\": \"Metrics ready\"\n        })\n    else:\n        health_checks.append({\n            \"service\": \"Prometheus\",\n            \"status\": \"Unhealthy\",\n            \"latency\": f\"{latency:.0f}ms\",\n            \"detail\": f\"Status {response.status_code}\"\n        })\nexcept Exception as e:\n    health_checks.append({\n        \"service\": \"Prometheus\",\n        \"status\": \"Error\",\n        \"latency\": \"N/A\",\n        \"detail\": str(e)[:50]\n    })\n\n# 5. Check Kafka (via AdminClient)\ntry:\n    from confluent_kafka.admin import AdminClient\n\n    start = time.time()\n    admin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n    metadata = admin.list_topics(timeout=5)\n    latency = (time.time() - start) * 1000\n\n    health_checks.append({\n        \"service\": \"Kafka\",\n        \"status\": \"Healthy\",\n        \"latency\": f\"{latency:.0f}ms\",\n        \"detail\": f\"{len(metadata.topics)} topics\"\n    })\nexcept Exception as e:\n    health_checks.append({\n        \"service\": \"Kafka\",\n        \"status\": \"Error\",\n        \"latency\": \"N/A\",\n        \"detail\": str(e)[:50]\n    })\n\n# Display health check results\nprint(\"Service Health Checks\\n\")\nprint(f\"{'Service':<20} {'Status':<12} {'Latency':<12} {'Detail':<30}\")\nprint(\"-\" * 74)\n\nall_healthy = True\nfor check in health_checks:\n    print(f\"{check['service']:<20} {check['status']:<12} {check['latency']:<12} {check['detail']:<30}\")\n    if \"Error\" in check[\"status\"] or \"Unhealthy\" in check[\"status\"]:\n        all_healthy = False\n\nif all_healthy:\n    print(\"\\nAll services are healthy and ready!\")\nelse:\n    print(\"\\nSome services are unhealthy. Please check Docker logs.\")\n    print(\"\\nDebug with: docker compose logs <service-name>\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 4. Infrastructure Initialization\n",
    "\n",
    "Now that services are running and healthy, let's initialize the infrastructure:\n",
    "1. Register v2 Avro schemas with Schema Registry\n",
    "2. Create Kafka topics for crypto trades\n",
    "3. Create Iceberg tables with v2 schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Run infrastructure initialization script\nprint(\"\\nInitializing infrastructure...\\n\")\n\ntry:\n    result = subprocess.run(\n        [\"python\", \"scripts/init_e2e_demo.py\"],\n        capture_output=True,\n        text=True,\n        cwd=Path.cwd().parent,\n        timeout=60\n    )\n\n    # Print output\n    if result.stdout:\n        print(result.stdout)\n\n    if result.returncode == 0:\n        print(\"\\n Infrastructure initialized successfully!\")\n    else:\n        print(f\"\\n Initialization failed with code {result.returncode}\")\n        if result.stderr:\n            print(\"Error output:\")\n            print(result.stderr)\n\nexcept subprocess.TimeoutExpired:\n    print(\"\\n Initialization timed out (>60s)\")\nexcept Exception as e:\n    print(f\"\\n Error during initialization: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Verify infrastructure\nprint(\"\\nVerifying infrastructure components...\\n\")\n\nverification_results = []\n\n# 1. Check Schema Registry subjects\ntry:\n    from confluent_kafka.schema_registry import SchemaRegistryClient\n\n    client = SchemaRegistryClient({\"url\": \"http://localhost:8081\"})\n    subjects = client.get_subjects()\n\n    # Check for required v2 subjects\n    required_subjects = [\n        \"market.crypto.trades-value\",\n        \"market.equities.trades-value\",\n    ]\n\n    found_subjects = [s for s in required_subjects if s in subjects]\n\n    verification_results.append({\n        \"component\": \"Schema Registry\",\n        \"status\": \"Ready\" if len(found_subjects) >= 2 else \"Partial\",\n        \"detail\": f\"{len(subjects)} schemas registered\"\n    })\nexcept Exception as e:\n    verification_results.append({\n        \"component\": \"Schema Registry\",\n        \"status\": \"Error\",\n        \"detail\": str(e)[:50]\n    })\n\n# 2. Check Kafka topics\ntry:\n    from confluent_kafka.admin import AdminClient\n\n    admin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\n    metadata = admin.list_topics(timeout=10)\n\n    # Check for crypto trades topic\n    crypto_topic = \"market.crypto.trades.binance\"\n    topic_exists = crypto_topic in metadata.topics\n\n    if topic_exists:\n        topic_meta = metadata.topics[crypto_topic]\n        partition_count = len(topic_meta.partitions)\n        verification_results.append({\n            \"component\": \"Kafka Topics\",\n            \"status\": \"Ready\",\n            \"detail\": f\"{crypto_topic} ({partition_count} partitions)\"\n        })\n    else:\n        verification_results.append({\n            \"component\": \"Kafka Topics\",\n            \"status\": \"Missing\",\n            \"detail\": f\"{crypto_topic} not found\"\n        })\nexcept Exception as e:\n    verification_results.append({\n        \"component\": \"Kafka Topics\",\n        \"status\": \"Error\",\n        \"detail\": str(e)[:50]\n    })\n\n# 3. Check Iceberg tables\ntry:\n    from pyiceberg.catalog import load_catalog\n\n    catalog = load_catalog(\n        \"k2\",\n        **{\n            \"uri\": \"http://localhost:8181\",\n            \"s3.endpoint\": \"http://localhost:9000\",\n            \"s3.access-key-id\": \"admin\",\n            \"s3.secret-access-key\": \"password\",\n            \"s3.path-style-access\": \"true\",\n        },\n    )\n\n    # Try to load trades_v2 table\n    try:\n        table = catalog.load_table(\"market_data.trades_v2\")\n        field_count = len(table.schema().fields)\n\n        verification_results.append({\n            \"component\": \"Iceberg Tables\",\n            \"status\": \"Ready\",\n            \"detail\": f\"market_data.trades_v2 ({field_count} fields)\"\n        })\n    except Exception:\n        verification_results.append({\n            \"component\": \"Iceberg Tables\",\n            \"status\": \"Not Found\",\n            \"detail\": \"market_data.trades_v2 missing\"\n        })\nexcept Exception as e:\n    verification_results.append({\n        \"component\": \"Iceberg Tables\",\n        \"status\": \"Error\",\n        \"detail\": str(e)[:50]\n    })\n\n# Display verification results\nprint(\"Infrastructure Verification\\n\")\nprint(f\"{'Component':<20} {'Status':<15} {'Detail':<50}\")\nprint(\"-\" * 85)\n\nall_ready = True\nfor result in verification_results:\n    print(f\"{result['component']:<20} {result['status']:<15} {result['detail']:<50}\")\n    if result['status'] not in ['Ready', 'Partial']:\n        all_ready = False\n\nif all_ready:\n    print(\"\\nAll infrastructure components are ready for streaming!\")\nelse:\n    print(\"\\nSome components are not ready. Run init_e2e_demo.py again.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Binance WebSocket Client Demo\n",
    "\n",
    "Now for the exciting part - let's connect to Binance's live WebSocket stream and watch real cryptocurrency trades flow in!\n",
    "\n",
    "We'll stream trades for:\n",
    "- **BTCUSDT** - Bitcoin vs USDT\n",
    "- **ETHUSDT** - Ethereum vs USDT\n",
    "\n",
    "The stream will run for **30 seconds** so you can see real-time data flowing through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Import Binance client and producer\nimport asyncio\n\nfrom IPython.display import clear_output\n\nfrom k2.ingestion.binance_client import BinanceWebSocketClient\nfrom k2.ingestion.producer import MarketDataProducer\n\n# Create producer (will send to Kafka)\nproducer = MarketDataProducer(schema_version=\"v2\")\n\n# Create callback function to send trades to Kafka\ndef handle_trade(trade_data: dict) -> None:\n    \"\"\"Callback to send trades to Kafka.\"\"\"\n    producer.send_trade(trade_data)\n\n# Create Binance WebSocket client\nsymbols = [\"BTCUSDT\", \"ETHUSDT\"]\nclient = BinanceWebSocketClient(\n    symbols=symbols,\n    on_message=handle_trade,\n)\n\nprint(f\"\\nBinance client created for symbols: {', '.join(symbols)}\")\nprint(\"\\nReady to start streaming!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Stream trades for 30 seconds\nprint(\"\\nStarting Binance WebSocket stream...\\n\")\nprint(\"Streaming for 30 seconds. Watch real trades flow in!\\n\")\n\n# Track streaming stats\nstreaming_stats = {\n    \"start_time\": None,\n    \"total_trades\": 0,\n    \"trades_by_symbol\": {},\n    \"latest_prices\": {},\n}\n\nasync def demo_stream():\n    \"\"\"Stream trades from Binance for 30 seconds.\"\"\"\n    try:\n        # Connect to Binance\n        await client.connect()\n        print(\"Connected to Binance WebSocket!\\n\")\n\n        streaming_stats[\"start_time\"] = time.time()\n\n        # Stream for 30 seconds\n        duration = 30\n        while time.time() - streaming_stats[\"start_time\"] < duration:\n            await asyncio.sleep(1)\n\n            # Get current metrics from producer\n            elapsed = int(time.time() - streaming_stats[\"start_time\"])\n\n            # Update display\n            clear_output(wait=True)\n\n            print(f\"Streaming... ({elapsed}s / {duration}s)\\n\")\n            print(\"Connected to: wss://stream.binance.com:9443/stream\")\n            print(f\"Symbols: {', '.join(symbols)}\")\n            print(\"\\nLive trades are being sent to Kafka topic: market.crypto.trades.binance\")\n            print(f\"\\nTrades received: ~{elapsed * 10} (estimated)\")\n\n        # Disconnect\n        await client.disconnect()\n\n        print(\"\\nStreaming complete!\")\n        print(f\"\\nTotal time: {duration}s\")\n        print(f\"Estimated trades received: {duration * 10} (BTC + ETH)\")\n        print(\"\\nTrades are now in Kafka and ready to be consumed to Iceberg!\")\n\n    except Exception as e:\n        print(f\"\\nStreaming error: {e}\")\n        print(\"\\nThis is usually due to network issues or Binance API limits.\")\n        print(\"Don't worry - you can continue with existing Kafka data!\")\n\n# Run the streaming demo\nawait demo_stream()\n\n# Flush producer to ensure all messages are sent\nproducer.flush()\nprint(\"\\nProducer flushed - all messages sent to Kafka\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Show example Binance WebSocket message format\nprint(\"\\nExample Binance WebSocket Message:\\n\")\n\nimport json\n\nexample_binance_message = {\n    \"e\": \"trade\",              # Event type\n    \"E\": 1704067800000,        # Event time (milliseconds)\n    \"s\": \"BTCUSDT\",            # Symbol\n    \"t\": 123456789,            # Trade ID\n    \"p\": \"65000.00\",           # Price\n    \"q\": \"0.05\",               # Quantity\n    \"T\": 1704067800000,        # Trade time (milliseconds)\n    \"m\": False,                # Is buyer maker?\n    \"M\": True                  # Is best match?\n}\n\nprint(\"Raw Binance WebSocket JSON:\")\nprint(json.dumps(example_binance_message, indent=2))\n\nprint(\"\\nConverted to K2 V2 Schema:\\n\")\n\n# Show v2 conversion (simplified example)\nexample_v2_trade = {\n    \"message_id\": \"123e4567-e89b-12d3-a456-426614174000\",  # Generated UUID\n    \"trade_id\": \"BINANCE-123456789\",                       # Exchange trade ID\n    \"symbol\": \"BTCUSDT\",                                   # Trading pair\n    \"exchange\": \"BINANCE\",                                 # Exchange name\n    \"asset_class\": \"crypto\",                               # Asset class\n    \"timestamp\": \"2024-01-01T00:00:00.000000Z\",            # Microseconds\n    \"price\": \"65000.00000000\",                             # Decimal(18,8)\n    \"quantity\": \"0.05000000\",                              # Decimal(18,8)\n    \"currency\": \"USDT\",                                    # Quote currency\n    \"side\": \"BUY\",                                         # Aggressor side\n    \"source_sequence\": 123456789,                          # Exchange sequence\n    \"vendor_data\": {                                        # Binance-specific fields\n        \"event_type\": \"trade\",\n        \"event_time\": \"1704067800000\",\n        \"trade_time\": \"1704067800000\",\n        \"is_buyer_maker\": \"False\",\n        \"is_best_match\": \"True\",\n        \"base_asset\": \"BTC\",\n        \"quote_asset\": \"USDT\"\n    }\n}\n\nprint(\"K2 V2 Schema (with vendor_data):\")\nprint(json.dumps(example_v2_trade, indent=2))\n\nprint(\"\\nKey Points:\")\nprint(\"  \u2022 Industry-standard core fields (symbol, price, quantity, etc.)\")\nprint(\"  \u2022 Binance-specific fields preserved in vendor_data JSON\")\nprint(\"  \u2022 UUID message_id for deduplication\")\nprint(\"  \u2022 Microsecond precision timestamps\")\nprint(\"  \u2022 Decimal(18,8) precision for prices/quantities\")"
  },
  {
   "cell_type": "markdown",
   "id": "3lvsw5ih4e4",
   "metadata": {},
   "source": "## 6. Kafka Topic Inspection\n\nLet's verify that the trades we just streamed are actually in Kafka. We'll check:\n1. Topic metadata (partitions, leaders)\n2. Total message count per partition\n3. Sample a few messages to see the Avro-serialized data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w4uyfi6uwj",
   "metadata": {},
   "outputs": [],
   "source": "# Check Kafka topic metadata\nprint(\"\\nKafka Topic Metadata:\\n\")\n\nfrom confluent_kafka.admin import AdminClient\n\nadmin = AdminClient({\"bootstrap.servers\": \"localhost:9092\"})\nmetadata = admin.list_topics(topic=\"market.crypto.trades.binance\", timeout=10)\n\ntopic_meta = metadata.topics[\"market.crypto.trades.binance\"]\n\nprint(f\"Topic: {topic_meta.topic}\")\nprint(f\"Partitions: {len(topic_meta.partitions)}\")\n\n# Create partition table\ntable = Table(title=\"Partition Distribution\", show_header=True)\ntable.add_column(\"Partition\", style=\"cyan\")\ntable.add_column(\"Leader\", style=\"green\")\ntable.add_column(\"Replicas\", style=\"yellow\")\ntable.add_column(\"ISR\", style=\"magenta\")\n\nfor partition_id, partition in sorted(topic_meta.partitions.items()):\n    table.add_row(\n        str(partition_id),\n        str(partition.leader),\n        str(len(partition.replicas)),\n        str(len(partition.isrs))\n    )\n\nprint(table)\nprint(\"\\n Topic exists and has healthy partition distribution\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dy5zb3i7n2p",
   "metadata": {},
   "outputs": [],
   "source": "# Count messages in topic\nprint(\"\\nCounting messages in topic...\\n\")\n\nfrom confluent_kafka import Consumer\n\nconsumer = Consumer({\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"group.id\": \"notebook-inspector\",\n    \"auto.offset.reset\": \"earliest\",\n})\n\n# Get partition assignments\nconsumer.subscribe([\"market.crypto.trades.binance\"])\nconsumer.poll(timeout=1.0)  # Trigger partition assignment\nassignment = consumer.assignment()\n\nif not assignment:\n    print(\" No partitions assigned yet. Topic might be empty.\")\nelse:\n    # Get watermark offsets (low = earliest, high = latest)\n    table = Table(title=\"Message Count by Partition\", show_header=True)\n    table.add_column(\"Partition\", style=\"cyan\")\n    table.add_column(\"Low Offset\", style=\"yellow\")\n    table.add_column(\"High Offset\", style=\"green\")\n    table.add_column(\"Messages\", style=\"magenta\")\n\n    total_messages = 0\n    for tp in sorted(assignment, key=lambda x: x.partition):\n        low, high = consumer.get_watermark_offsets(tp, timeout=5.0)\n        messages = high - low\n        total_messages += messages\n\n        table.add_row(\n            str(tp.partition),\n            f\"{low:,}\",\n            f\"{high:,}\",\n            f\"{messages:,}\"\n        )\n\n    print(table)\n    print(f\"\\nTotal messages in topic: {total_messages:,}\")\n\n    if total_messages == 0:\n        print(\"\\n No messages found. The streaming demo might have failed or not run yet.\")\n    else:\n        print(f\"Average messages per partition: {total_messages / len(assignment):,.1f}\")\n\nconsumer.close()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cy1b5eds6k",
   "metadata": {},
   "outputs": [],
   "source": "# Sample 5 messages from the topic\nprint(\"\\nSampling messages from Kafka...\\n\")\n\nfrom confluent_kafka import Consumer\nfrom confluent_kafka.schema_registry import SchemaRegistryClient\nfrom confluent_kafka.schema_registry.avro import AvroDeserializer\n\n# Create Schema Registry client\nschema_registry_client = SchemaRegistryClient({\"url\": \"http://localhost:8081\"})\n\n# Create consumer\nconsumer = Consumer({\n    \"bootstrap.servers\": \"localhost:9092\",\n    \"group.id\": \"notebook-sampler\",\n    \"auto.offset.reset\": \"earliest\",\n    \"enable.auto.commit\": False,\n})\n\nconsumer.subscribe([\"market.crypto.trades.binance\"])\n\n# Consume 5 messages\nmessages = []\nattempts = 0\nmax_attempts = 20\n\nprint(\"Consuming messages...\")\n\nwhile len(messages) < 5 and attempts < max_attempts:\n    msg = consumer.poll(timeout=1.0)\n    attempts += 1\n\n    if msg is None:\n        continue\n    if msg.error():\n        print(f\"Consumer error: {msg.error()}\")\n        continue\n\n    # Deserialize Avro value\n    try:\n        # Get schema ID from message\n        schema_id = int.from_bytes(msg.value()[1:5], byteorder='big')\n        schema = schema_registry_client.get_schema(schema_id)\n\n        deserializer = AvroDeserializer(\n            schema_registry_client,\n            schema.schema_str\n        )\n\n        value = deserializer(msg.value(), None)\n        messages.append(value)\n\n    except Exception as e:\n        print(f\"Deserialization error: {e}\")\n        continue\n\nconsumer.close()\n\nif messages:\n    print(f\"\\n Successfully sampled {len(messages)} messages\\n\")\n\n    # Convert to DataFrame\n    df_kafka = pd.DataFrame(messages)\n\n    # Display sample\n    display_columns = [\"symbol\", \"timestamp\", \"price\", \"quantity\", \"side\", \"exchange\"]\n    if all(col in df_kafka.columns for col in display_columns):\n        print(\"Sample Messages:\")\n        display(df_kafka[display_columns].head())\n\n        print(f\"\\nMessage structure: {len(df_kafka.columns)} fields\")\n        print(f\"Fields: {', '.join(df_kafka.columns[:10])}...\")\n    else:\n        print(\"Sample Messages (all fields):\")\n        display(df_kafka.head())\nelse:\n    print(\"\\n No messages found. The topic might be empty.\")\n    print(\"Run the Binance streaming demo above to populate the topic.\")"
  },
  {
   "cell_type": "markdown",
   "id": "c9h9a55wuh",
   "metadata": {},
   "source": "## 7. Consumer Pipeline (Kafka \u2192 Iceberg)\n\nNow let's consume messages from Kafka and write them to Iceberg. The consumer will:\n1. Read messages in batches (500 records)\n2. Deserialize Avro to Python objects\n3. Write to Iceberg with ACID guarantees\n4. Track metrics (throughput, latency, errors)\n\nWe'll consume **1,000 messages** to demonstrate the pipeline."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "igq255o8hu",
   "metadata": {},
   "outputs": [],
   "source": "# Run consumer to write messages to Iceberg\nprint(\"\\nStarting consumer pipeline...\\n\")\n\nfrom k2.ingestion.consumer import MarketDataConsumer\n\n# Create consumer\nconsumer = MarketDataConsumer(\n    topics=[\"market.crypto.trades.binance\"],\n    group_id=\"notebook-consumer\",\n    table_version=\"v2\",\n)\n\nprint(\"Consumer Configuration:\")\nprint(f\"  Topics: {consumer.topics}\")\nprint(f\"  Group ID: {consumer.group_id}\")\nprint(f\"  Batch size: {consumer.batch_size}\")\nprint(\"  Table version: v2\")\nprint(\"\\n Consumer initialized\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8hhxl2g3t46",
   "metadata": {},
   "outputs": [],
   "source": "# Consume and write messages\nprint(\"\\nConsuming messages and writing to Iceberg...\\n\")\n\nmax_messages = 1000\nmessages_consumed = 0\nmessages_written = 0\nstart_time = time.time()\n\nprint(f\"Target: {max_messages} messages\")\nprint(\"This may take 30-60 seconds...\\n\")\n\ntry:\n    with Progress(\n        SpinnerColumn(),\n        TextColumn(\"[progress.description]{task.description}\"),\n        console=console,\n    ) as progress:\n        task = progress.add_task(\"Consuming messages...\", total=max_messages)\n\n        # Start consuming\n        for message in consumer.consume(max_messages=max_messages):\n            messages_consumed += 1\n            progress.update(task, advance=1)\n\n            # Update every 100 messages\n            if messages_consumed % 100 == 0:\n                elapsed = time.time() - start_time\n                throughput = messages_consumed / elapsed if elapsed > 0 else 0\n                progress.update(\n                    task,\n                    description=f\"Consuming messages... ({messages_consumed}/{max_messages}, {throughput:.1f} msg/s)\"\n                )\n\n    elapsed_time = time.time() - start_time\n    throughput = messages_consumed / elapsed_time if elapsed_time > 0 else 0\n\n    print(\"\\n Consumption complete!\")\n    print(\"\\nResults:\")\n    print(f\"  Messages consumed: {messages_consumed:,}\")\n    print(f\"  Time elapsed: {elapsed_time:.2f}s\")\n    print(f\"  Throughput: {throughput:.2f} msg/s\")\n\n    # Note: Messages are written in batches, so written count may differ\n    print(\"\\nMessages have been written to Iceberg table: market_data.trades_v2\")\n\nexcept KeyboardInterrupt:\n    print(\"\\n Consumption interrupted by user\")\nexcept Exception as e:\n    print(f\"\\n Consumer error: {e}\")\n    import traceback\n    traceback.print_exc()\nfinally:\n    # Close consumer\n    consumer.close()\n    print(\"\\n Consumer closed\")"
  },
  {
   "cell_type": "markdown",
   "id": "8dwsnm13zfi",
   "metadata": {},
   "source": "## 8. Iceberg Table Inspection\n\nLet's verify that data was successfully written to the Iceberg lakehouse. We'll:\n1. Load the table and inspect its schema\n2. Count total rows\n3. Sample recent data\n4. Examine vendor_data fields"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4i9wu3c8xb",
   "metadata": {},
   "outputs": [],
   "source": "# Load Iceberg table and inspect schema\nprint(\"\\nLoading Iceberg table...\\n\")\n\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\n    \"k2\",\n    **{\n        \"uri\": \"http://localhost:8181\",\n        \"s3.endpoint\": \"http://localhost:9000\",\n        \"s3.access-key-id\": \"admin\",\n        \"s3.secret-access-key\": \"password\",\n        \"s3.path-style-access\": \"true\",\n    },\n)\n\ntry:\n    table = catalog.load_table(\"market_data.trades_v2\")\n\n    print(f\" Table loaded: {table.name()}\")\n    print(\"\\nTable Schema (V2):\\n\")\n\n    # Create schema table\n    schema_table = Table(title=\"trades_v2 Schema\", show_header=True)\n    schema_table.add_column(\"Field ID\", style=\"cyan\")\n    schema_table.add_column(\"Field Name\", style=\"green\")\n    schema_table.add_column(\"Type\", style=\"yellow\")\n    schema_table.add_column(\"Required\", style=\"magenta\")\n\n    for field in table.schema().fields:\n        required = \"\" if field.required else \"\"\n        schema_table.add_row(\n            str(field.field_id),\n            field.name,\n            str(field.field_type),\n            required\n        )\n\n    print(schema_table)\n    print(f\"\\nTotal fields: {len(table.schema().fields)}\")\n\nexcept Exception as e:\n    print(f\" Error loading table: {e}\")\n    print(\"\\nMake sure the consumer has written data to the table.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9h9d86c8dz",
   "metadata": {},
   "outputs": [],
   "source": "# Count rows using DuckDB\nprint(\"\\nCounting rows in Iceberg table...\\n\")\n\nimport duckdb\n\n# Create DuckDB connection\nconn = duckdb.connect(\":memory:\")\nconn.install_extension(\"iceberg\")\nconn.load_extension(\"iceberg\")\n\n# Configure S3 access for DuckDB\nconn.execute(\"\"\"\n    CREATE SECRET secret1 (\n        TYPE S3,\n        KEY_ID 'admin',\n        SECRET 'password',\n        ENDPOINT 'localhost:9000',\n        URL_STYLE 'path',\n        REGION 'us-east-1',\n        USE_SSL false\n    );\n\"\"\")\n\ntry:\n    # Count rows\n    result = conn.execute(\"\"\"\n        SELECT COUNT(*) as count\n        FROM iceberg_scan('http://localhost:8181/v1/market_data/trades_v2', allow_moved_paths=true)\n    \"\"\").fetchone()\n\n    row_count = result[0]\n\n    print(f\"Total rows in trades_v2: {row_count:,}\")\n\n    if row_count == 0:\n        print(\"\\n No rows found. The consumer might not have written data yet.\")\n        print(\"Run the consumer pipeline above to write data to Iceberg.\")\n    else:\n        # Get data distribution by symbol\n        result = conn.execute(\"\"\"\n            SELECT \n                symbol,\n                COUNT(*) as trade_count,\n                MIN(timestamp) as first_trade,\n                MAX(timestamp) as last_trade\n            FROM iceberg_scan('http://localhost:8181/v1/market_data/trades_v2', allow_moved_paths=true)\n            GROUP BY symbol\n            ORDER BY trade_count DESC\n        \"\"\").fetchdf()\n\n        print(\"\\nData distribution by symbol:\\n\")\n        display(result)\n\nexcept Exception as e:\n    print(f\" Error querying table: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "onxfoyk7i2q",
   "metadata": {},
   "outputs": [],
   "source": "# Sample recent data\nprint(\"\\nSampling recent trades...\\n\")\n\ntry:\n    # Query 10 most recent trades\n    df_recent = conn.execute(\"\"\"\n        SELECT *\n        FROM iceberg_scan('http://localhost:8181/v1/market_data/trades_v2', allow_moved_paths=true)\n        ORDER BY timestamp DESC\n        LIMIT 10\n    \"\"\").fetchdf()\n\n    if len(df_recent) > 0:\n        print(f\" Found {len(df_recent)} recent trades\\n\")\n\n        # Display key columns\n        display_columns = [\"symbol\", \"exchange\", \"timestamp\", \"price\", \"quantity\", \"side\", \"currency\"]\n        print(\"Recent Trades (Key Fields):\")\n        display(df_recent[display_columns])\n\n        # Show sample trade details\n        print(\"\\nSample Trade Details:\")\n        sample = df_recent.iloc[0]\n        print(f\"  Symbol: {sample['symbol']}\")\n        print(f\"  Exchange: {sample['exchange']}\")\n        print(f\"  Asset Class: {sample['asset_class']}\")\n        print(f\"  Timestamp: {sample['timestamp']}\")\n        print(f\"  Price: {sample['price']} {sample['currency']}\")\n        print(f\"  Quantity: {sample['quantity']}\")\n        print(f\"  Side: {sample['side']}\")\n        print(f\"  Trade ID: {sample['trade_id']}\")\n        print(f\"  Message ID: {sample['message_id']}\")\n\n    else:\n        print(\" No trades found in table\")\n\nexcept Exception as e:\n    print(f\" Error sampling data: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gmtzvxvrm9q",
   "metadata": {},
   "outputs": [],
   "source": "# Examine vendor_data (Binance-specific fields)\nprint(\"\\nExamining vendor_data...\\n\")\n\nimport json\n\nif len(df_recent) > 0:\n    sample = df_recent.iloc[0]\n\n    print(\"Vendor Data (Binance-specific fields):\\n\")\n\n    # Parse vendor_data JSON\n    if pd.notna(sample['vendor_data']):\n        vendor_data = json.loads(sample['vendor_data'])\n\n        # Create vendor data table\n        vendor_table = Table(title=\"Binance Vendor Data\", show_header=True)\n        vendor_table.add_column(\"Field\", style=\"cyan\")\n        vendor_table.add_column(\"Value\", style=\"green\")\n        vendor_table.add_column(\"Description\", style=\"yellow\")\n\n        field_descriptions = {\n            \"event_type\": \"Type of event (trade, aggTrade, etc.)\",\n            \"event_time\": \"Event timestamp from Binance (milliseconds)\",\n            \"trade_time\": \"Trade execution timestamp (milliseconds)\",\n            \"is_buyer_maker\": \"Was the buyer the maker? (true/false)\",\n            \"is_best_match\": \"Was this the best price match? (true/false)\",\n            \"base_asset\": \"Base asset symbol (e.g., BTC in BTCUSDT)\",\n            \"quote_asset\": \"Quote asset symbol (e.g., USDT in BTCUSDT)\",\n        }\n\n        for key, value in vendor_data.items():\n            description = field_descriptions.get(key, \"N/A\")\n            vendor_table.add_row(key, str(value), description)\n\n        print(vendor_table)\n\n        print(\"\\nKey Points:\")\n        print(\"  \u2022 vendor_data preserves all Binance-specific fields\")\n        print(\"  \u2022 Core fields (price, quantity, side) are normalized to v2 schema\")\n        print(\"  \u2022 Exchange-specific data enables advanced analysis\")\n        print(\"  \u2022 Same v2 schema works across ASX (equities) and Binance (crypto)\")\n\n    else:\n        print(\" No vendor_data found in sample trade\")\nelse:\n    print(\" No data available to examine vendor_data\")"
  },
  {
   "cell_type": "markdown",
   "id": "cttj14bkvon",
   "metadata": {},
   "source": "## 9. Query Engine Demo\n\nNow let's demonstrate the K2 Query Engine, which provides a high-level Python API for querying trades. The QueryEngine:\n- Abstracts DuckDB complexity\n- Supports time-range queries\n- Filters by symbol, exchange, asset class\n- Returns data as Python dictionaries or DataFrames\n- Provides sub-second query performance"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acmisi6tiw",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize QueryEngine\nprint(\"\\nInitializing Query Engine...\\n\")\n\nfrom k2.query.engine import QueryEngine\n\nengine = QueryEngine(table_version=\"v2\")\n\nprint(\" QueryEngine initialized\")\nprint(f\"  Iceberg Catalog: {engine.iceberg_catalog_uri}\")\nprint(\"  Table: market_data.trades_v2\")\nprint(\"  Query Engine: DuckDB with Iceberg extension\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "byfs6rk0pxi",
   "metadata": {},
   "outputs": [],
   "source": "# Basic queries\nprint(\"\\nRunning basic queries...\\n\")\n\n# Query 1: Get all available symbols\ntry:\n    symbols = engine.get_symbols(exchange=\"BINANCE\")\n    print(f\"Available symbols: {', '.join(symbols)}\")\nexcept Exception as e:\n    print(f\"Error getting symbols: {e}\")\n    symbols = []\n\n# Query 2: Query trades for each symbol\nif symbols:\n    for symbol in symbols:\n        try:\n            trades = engine.query_trades(\n                symbol=symbol,\n                exchange=\"BINANCE\",\n                limit=100,\n            )\n            print(f\" {symbol}: {len(trades)} trades (limit 100)\")\n        except Exception as e:\n            print(f\" {symbol}: Error - {e}\")\nelse:\n    print(\"\\n No symbols found. Query all trades instead:\")\n    try:\n        all_trades = engine.query_trades(limit=100)\n        print(f\" Found {len(all_trades)} trades (limit 100)\")\n\n        # Get unique symbols from results\n        if all_trades:\n            unique_symbols = set(t['symbol'] for t in all_trades)\n            print(f\"Symbols in data: {', '.join(unique_symbols)}\")\n    except Exception as e:\n        print(f\" Error querying trades: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ct58js5185",
   "metadata": {},
   "outputs": [],
   "source": "# Time-range query\nprint(\"\\nTime-range query (last 5 minutes)...\\n\")\n\nfrom datetime import UTC\n\n# Query last 5 minutes\nend_time = datetime.now(UTC)\nstart_time = end_time - timedelta(minutes=5)\n\ntry:\n    recent_trades = engine.query_trades(\n        start_time=start_time,\n        end_time=end_time,\n        limit=1000,\n    )\n\n    print(f\" Found {len(recent_trades)} trades in last 5 minutes\")\n\n    if recent_trades:\n        # Convert to DataFrame for analysis\n        df_trades = pd.DataFrame(recent_trades)\n\n        print(\"\\nTime range:\")\n        print(f\"  Start: {start_time}\")\n        print(f\"  End: {end_time}\")\n        print(\"  Duration: 5 minutes\")\n\n        print(\"\\nSample trades:\")\n        display(df_trades[[\"symbol\", \"timestamp\", \"price\", \"quantity\", \"side\"]].head(10))\n\n        # Show trade distribution\n        print(\"\\nTrade distribution:\")\n        symbol_counts = df_trades[\"symbol\"].value_counts()\n        for symbol, count in symbol_counts.items():\n            print(f\"  {symbol}: {count} trades\")\n    else:\n        print(\"\\n No trades found in the specified time range\")\n        print(\"This is normal if data is older than 5 minutes.\")\n\nexcept Exception as e:\n    print(f\" Error in time-range query: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7omqhrysp7u",
   "metadata": {},
   "outputs": [],
   "source": "# OHLCV aggregation query\nprint(\"\\nOHLCV (Open, High, Low, Close, Volume) Aggregation...\\n\")\n\ntry:\n    # Query all trades\n    all_trades = engine.query_trades(limit=5000)\n\n    if all_trades:\n        df_all = pd.DataFrame(all_trades)\n\n        # Convert price and quantity to numeric\n        df_all[\"price\"] = pd.to_numeric(df_all[\"price\"])\n        df_all[\"quantity\"] = pd.to_numeric(df_all[\"quantity\"])\n\n        # Calculate OHLCV by symbol\n        ohlcv = df_all.groupby(\"symbol\").agg({\n            \"price\": [\"first\", \"max\", \"min\", \"last\"],\n            \"quantity\": \"sum\",\n            \"trade_id\": \"count\",\n        })\n\n        # Flatten column names\n        ohlcv.columns = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"]\n\n        # Calculate price range and spread\n        ohlcv[\"range\"] = ohlcv[\"high\"] - ohlcv[\"low\"]\n        ohlcv[\"spread_pct\"] = (ohlcv[\"range\"] / ohlcv[\"open\"] * 100).round(2)\n\n        print(\"OHLCV Summary by Symbol:\\n\")\n        display(ohlcv)\n\n        print(\"\\nAnalysis:\")\n        for symbol in ohlcv.index:\n            row = ohlcv.loc[symbol]\n            print(f\"\\n{symbol}:\")\n            print(f\"  Open: {row['open']:.2f}, Close: {row['close']:.2f}\")\n            print(f\"  High: {row['high']:.2f}, Low: {row['low']:.2f}\")\n            print(f\"  Range: {row['range']:.2f} ({row['spread_pct']:.2f}%)\")\n            print(f\"  Volume: {row['volume']:.4f}\")\n            print(f\"  Trades: {row['trades']:.0f}\")\n    else:\n        print(\" No trades available for OHLCV calculation\")\n\nexcept Exception as e:\n    print(f\" Error in OHLCV aggregation: {e}\")\n    import traceback\n    traceback.print_exc()"
  },
  {
   "cell_type": "markdown",
   "id": "zbh09cc11ya",
   "metadata": {},
   "source": "## 10. Real-Time Price Visualization\n\nLet's visualize the live cryptocurrency trade data with interactive charts:\n1. **Price Scatter Plot** - BTC and ETH prices over time (colored by trade size)\n2. **Volume Bar Chart** - Trading volume aggregated in 30-second buckets\n3. **Price Distribution** - Histogram showing price ranges"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ya83x42f6ce",
   "metadata": {},
   "outputs": [],
   "source": "# Prepare data for visualization\nprint(\"\\nPreparing data for visualization...\\n\")\n\nimport matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\n# Enable matplotlib inline plotting\n%matplotlib inline\n\n# Set style\nplt.style.use('seaborn-v0_8-darkgrid')\n\ntry:\n    # Query recent trades for visualization\n    viz_trades = engine.query_trades(limit=2000)\n\n    if viz_trades:\n        df_viz = pd.DataFrame(viz_trades)\n\n        # Convert data types\n        df_viz[\"timestamp\"] = pd.to_datetime(df_viz[\"timestamp\"])\n        df_viz[\"price\"] = pd.to_numeric(df_viz[\"price\"])\n        df_viz[\"quantity\"] = pd.to_numeric(df_viz[\"quantity\"])\n\n        # Sort by timestamp\n        df_viz = df_viz.sort_values(\"timestamp\")\n\n        print(f\" Prepared {len(df_viz)} trades for visualization\")\n        print(f\"Time range: {df_viz['timestamp'].min()} to {df_viz['timestamp'].max()}\")\n        print(f\"Symbols: {', '.join(df_viz['symbol'].unique())}\")\n\n        # Separate by symbol for plotting\n        symbols = df_viz[\"symbol\"].unique()\n        df_by_symbol = {symbol: df_viz[df_viz[\"symbol\"] == symbol] for symbol in symbols}\n\n        print(\"\\n Data ready for plotting\")\n\n    else:\n        print(\" No trades available for visualization\")\n        df_viz = None\n        df_by_symbol = {}\n\nexcept Exception as e:\n    print(f\" Error preparing data: {e}\")\n    df_viz = None\n    df_by_symbol = {}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l30br5u4b4t",
   "metadata": {},
   "outputs": [],
   "source": "# Price scatter plot by symbol\nif df_by_symbol:\n    print(\"\\nPlotting price charts...\\n\")\n\n    num_symbols = len(df_by_symbol)\n    fig, axes = plt.subplots(num_symbols, 1, figsize=(14, 5 * num_symbols), sharex=True)\n\n    # Handle single symbol case\n    if num_symbols == 1:\n        axes = [axes]\n\n    # Color maps for different symbols\n    color_maps = [\"viridis\", \"plasma\", \"inferno\", \"magma\", \"cividis\"]\n\n    for idx, (symbol, df_symbol) in enumerate(df_by_symbol.items()):\n        ax = axes[idx]\n\n        # Scatter plot with quantity as color\n        scatter = ax.scatter(\n            df_symbol[\"timestamp\"],\n            df_symbol[\"price\"],\n            c=df_symbol[\"quantity\"],\n            cmap=color_maps[idx % len(color_maps)],\n            alpha=0.6,\n            s=50,\n            edgecolors='black',\n            linewidth=0.5,\n        )\n\n        # Labels and title\n        ax.set_ylabel(f\"Price ({df_symbol['currency'].iloc[0]})\", fontsize=12, fontweight='bold')\n        ax.set_title(f\"{symbol} Live Trades - Price Over Time\", fontsize=14, fontweight='bold')\n        ax.grid(True, alpha=0.3)\n\n        # Add colorbar\n        cbar = plt.colorbar(scatter, ax=ax)\n        cbar.set_label(\"Trade Quantity\", fontsize=10)\n\n        # Add price statistics\n        mean_price = df_symbol[\"price\"].mean()\n        std_price = df_symbol[\"price\"].std()\n        ax.axhline(y=mean_price, color='red', linestyle='--', linewidth=1, alpha=0.7, label=f'Mean: {mean_price:.2f}')\n        ax.axhline(y=mean_price + std_price, color='orange', linestyle=':', linewidth=1, alpha=0.5, label=f'+1\u03c3: {mean_price + std_price:.2f}')\n        ax.axhline(y=mean_price - std_price, color='orange', linestyle=':', linewidth=1, alpha=0.5, label=f'-1\u03c3: {mean_price - std_price:.2f}')\n        ax.legend(loc='upper right', fontsize=8)\n\n    # Format x-axis (time)\n    axes[-1].set_xlabel(\"Time (UTC)\", fontsize=12, fontweight='bold')\n    axes[-1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\" Price charts rendered\")\nelse:\n    print(\" No data available for plotting\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uwv03yjrdy",
   "metadata": {},
   "outputs": [],
   "source": "# Volume bar chart (30-second buckets)\nif df_viz is not None and len(df_viz) > 0:\n    print(\"\\nPlotting volume chart...\\n\")\n\n    # Create time buckets (30 seconds)\n    df_viz[\"time_bucket\"] = df_viz[\"timestamp\"].dt.floor(\"30S\")\n\n    # Aggregate volume by bucket and symbol\n    volume_agg = df_viz.groupby([\"time_bucket\", \"symbol\"])[\"quantity\"].sum().reset_index()\n\n    # Create volume chart\n    fig, ax = plt.subplots(figsize=(14, 6))\n\n    # Plot bars for each symbol\n    symbols = volume_agg[\"symbol\"].unique()\n    bar_width = pd.Timedelta(seconds=30) / (len(symbols) + 1)\n\n    for idx, symbol in enumerate(symbols):\n        symbol_data = volume_agg[volume_agg[\"symbol\"] == symbol]\n\n        # Offset bars for each symbol\n        offset = bar_width * (idx - len(symbols) / 2)\n\n        ax.bar(\n            symbol_data[\"time_bucket\"] + offset,\n            symbol_data[\"quantity\"],\n            width=bar_width,\n            label=symbol,\n            alpha=0.7,\n            edgecolor='black',\n            linewidth=0.5,\n        )\n\n    ax.set_xlabel(\"Time (UTC)\", fontsize=12, fontweight='bold')\n    ax.set_ylabel(\"Trading Volume\", fontsize=12, fontweight='bold')\n    ax.set_title(\"Trading Volume by Symbol (30-second buckets)\", fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3, axis='y')\n    ax.legend(loc='upper right', fontsize=10)\n\n    # Format x-axis\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    print(\" Volume chart rendered\")\n\n    # Print volume statistics\n    print(\"\\nVolume Statistics:\")\n    for symbol in symbols:\n        symbol_trades = df_viz[df_viz[\"symbol\"] == symbol]\n        total_volume = symbol_trades[\"quantity\"].sum()\n        avg_trade_size = symbol_trades[\"quantity\"].mean()\n        print(f\"\\n{symbol}:\")\n        print(f\"  Total volume: {total_volume:.4f}\")\n        print(f\"  Average trade size: {avg_trade_size:.6f}\")\n        print(f\"  Number of trades: {len(symbol_trades):,}\")\nelse:\n    print(\" No data available for volume chart\")"
  },
  {
   "cell_type": "markdown",
   "id": "ueafj2wpd6s",
   "metadata": {},
   "source": "## 11. Pipeline Metrics Visualization\n\nThe K2 platform exposes comprehensive metrics through Prometheus. Let's query and visualize:\n1. **Consumer throughput** - Messages consumed per second over time\n2. **Binance connection status** - WebSocket connection health\n3. **Message lag** - Consumer lag behind Kafka\n4. **Error rates** - Any errors in the pipeline"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfvgnw04c",
   "metadata": {},
   "outputs": [],
   "source": "# Query current metrics from Prometheus\nprint(\"\\nQuerying Prometheus metrics...\\n\")\n\nimport requests\n\nprom_url = \"http://localhost:9090/api/v1/query\"\n\ndef query_prometheus(metric_name):\n    \"\"\"Query Prometheus for a metric.\"\"\"\n    try:\n        response = requests.get(prom_url, params={\"query\": metric_name}, timeout=5)\n        data = response.json()\n\n        if data[\"status\"] == \"success\" and data[\"data\"][\"result\"]:\n            # Return all results\n            return data[\"data\"][\"result\"]\n        return []\n    except Exception as e:\n        print(f\"Error querying {metric_name}: {e}\")\n        return []\n\n# Query key metrics\nmetrics_to_check = [\n    (\"k2_kafka_messages_consumed_total\", \"Total messages consumed\"),\n    (\"k2_kafka_messages_produced_total\", \"Total messages produced\"),\n    (\"k2_binance_messages_received_total\", \"Total Binance messages received\"),\n    (\"k2_binance_connection_status\", \"Binance connection status\"),\n]\n\n# Create metrics summary table\nmetrics_table = Table(title=\"Current Pipeline Metrics\", show_header=True)\nmetrics_table.add_column(\"Metric\", style=\"cyan\")\nmetrics_table.add_column(\"Description\", style=\"yellow\")\nmetrics_table.add_column(\"Value\", style=\"green\")\n\nfor metric_name, description in metrics_to_check:\n    results = query_prometheus(metric_name)\n\n    if results:\n        # Sum values if multiple series\n        total = sum(float(r[\"value\"][1]) for r in results)\n        metrics_table.add_row(metric_name, description, f\"{total:,.0f}\")\n    else:\n        metrics_table.add_row(metric_name, description, \"N/A\")\n\nprint(metrics_table)\nprint(\"\\n Current metrics retrieved\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rcml4lx25vs",
   "metadata": {},
   "outputs": [],
   "source": "# Query time-series metrics for visualization\nprint(\"\\nQuerying time-series metrics (last 10 minutes)...\\n\")\n\nprom_range_url = \"http://localhost:9090/api/v1/query_range\"\n\ndef query_prometheus_range(query, duration_minutes=10):\n    \"\"\"Query Prometheus for a time-series metric.\"\"\"\n    try:\n        end_time = datetime.now(UTC)\n        start_time = end_time - timedelta(minutes=duration_minutes)\n\n        params = {\n            \"query\": query,\n            \"start\": int(start_time.timestamp()),\n            \"end\": int(end_time.timestamp()),\n            \"step\": \"15s\",\n        }\n\n        response = requests.get(prom_range_url, params=params, timeout=10)\n        data = response.json()\n\n        if data[\"status\"] == \"success\" and data[\"data\"][\"result\"]:\n            # Parse time-series data\n            all_series = []\n            for series in data[\"data\"][\"result\"]:\n                values = series[\"values\"]\n                timestamps = [datetime.fromtimestamp(v[0], tz=UTC) for v in values]\n                vals = [float(v[1]) for v in values]\n                labels = series[\"metric\"]\n\n                all_series.append({\n                    \"timestamps\": timestamps,\n                    \"values\": vals,\n                    \"labels\": labels,\n                })\n\n            return all_series\n        return []\n    except Exception as e:\n        print(f\"Error querying range: {e}\")\n        return []\n\n# Query consumer throughput (rate of messages consumed)\nthroughput_series = query_prometheus_range(\"rate(k2_kafka_messages_consumed_total[1m])\", duration_minutes=10)\n\nif throughput_series:\n    print(f\" Found {len(throughput_series)} throughput series\")\nelse:\n    print(\" No throughput data available (might need to wait for metrics to populate)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff560dgxxk",
   "metadata": {},
   "outputs": [],
   "source": "# Plot throughput metrics\nif throughput_series:\n    print(\"\\nPlotting consumer throughput...\\n\")\n\n    fig, ax = plt.subplots(figsize=(14, 6))\n\n    for series in throughput_series:\n        # Get label for legend\n        topic = series[\"labels\"].get(\"topic\", \"unknown\")\n        label = f\"Topic: {topic}\"\n\n        # Plot line\n        ax.plot(\n            series[\"timestamps\"],\n            series[\"values\"],\n            linewidth=2,\n            marker=\"o\",\n            markersize=4,\n            label=label,\n            alpha=0.8,\n        )\n\n    ax.set_xlabel(\"Time (UTC)\", fontsize=12, fontweight='bold')\n    ax.set_ylabel(\"Messages/second\", fontsize=12, fontweight='bold')\n    ax.set_title(\"Consumer Throughput (Last 10 Minutes)\", fontsize=14, fontweight='bold')\n    ax.grid(True, alpha=0.3)\n    ax.legend(loc='upper right', fontsize=10)\n\n    # Format x-axis\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M:%S\"))\n    plt.xticks(rotation=45)\n\n    plt.tight_layout()\n    plt.show()\n\n    # Calculate statistics\n    all_values = []\n    for series in throughput_series:\n        all_values.extend(series[\"values\"])\n\n    if all_values:\n        avg_throughput = np.mean(all_values)\n        max_throughput = np.max(all_values)\n        min_throughput = np.min(all_values)\n\n        print(\"\\nThroughput Statistics:\")\n        print(f\"  Average: {avg_throughput:.2f} msg/s\")\n        print(f\"  Peak: {max_throughput:.2f} msg/s\")\n        print(f\"  Minimum: {min_throughput:.2f} msg/s\")\n\n    print(\"\\n Throughput chart rendered\")\nelse:\n    print(\" No throughput data to plot\")\n    print(\"Metrics may not be available yet. Run the consumer pipeline and wait 1-2 minutes.\")"
  },
  {
   "cell_type": "markdown",
   "id": "6lugc9nlia",
   "metadata": {},
   "source": "## 12. Data Quality Checks\n\nLet's validate data integrity and quality across the pipeline:\n1. **Schema validation** - All required fields present and non-null\n2. **Sequence gap detection** - Check for missing sequence numbers\n3. **Duplicate detection** - Find duplicate message_ids or trade_ids\n4. **Data quality score** - Overall quality assessment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0euf6525mr",
   "metadata": {},
   "outputs": [],
   "source": "# Schema validation\nprint(\"\\nValidating schema compliance...\\n\")\n\ntry:\n    # Query trades for validation\n    validation_trades = engine.query_trades(limit=1000)\n\n    if validation_trades:\n        df_validation = pd.DataFrame(validation_trades)\n\n        # Required fields according to v2 schema\n        required_fields = [\n            \"message_id\", \"trade_id\", \"symbol\", \"exchange\", \"asset_class\",\n            \"timestamp\", \"price\", \"quantity\", \"currency\", \"side\",\n            \"source_sequence\", \"ingestion_timestamp\", \"platform_sequence\",\n        ]\n\n        # Create validation table\n        validation_table = Table(title=\"Schema Validation Results\", show_header=True)\n        validation_table.add_column(\"Field\", style=\"cyan\")\n        validation_table.add_column(\"Present\", style=\"green\")\n        validation_table.add_column(\"Null Count\", style=\"yellow\")\n        validation_table.add_column(\"Status\", style=\"magenta\")\n\n        all_valid = True\n        for field in required_fields:\n            present = field in df_validation.columns\n\n            if present:\n                null_count = df_validation[field].isnull().sum()\n                status = \" Valid\" if null_count == 0 else \" Has nulls\"\n\n                if null_count > 0:\n                    all_valid = False\n\n                validation_table.add_row(\n                    field,\n                    \" Yes\" if present else \" No\",\n                    str(null_count),\n                    status\n                )\n            else:\n                validation_table.add_row(field, \" No\", \"N/A\", \" Missing\")\n                all_valid = False\n\n        print(validation_table)\n\n        if all_valid:\n            print(\"\\n All required fields present with no nulls\")\n        else:\n            print(\"\\n Some fields have issues (see table above)\")\n    else:\n        print(\" No data available for schema validation\")\n        df_validation = None\n\nexcept Exception as e:\n    print(f\" Error in schema validation: {e}\")\n    df_validation = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3rqzd15uhm",
   "metadata": {},
   "outputs": [],
   "source": "# Sequence gap detection\nprint(\"\\nDetecting sequence gaps...\\n\")\n\nif df_validation is not None and len(df_validation) > 0:\n    gaps_detected = []\n\n    # Check for gaps per symbol\n    for symbol in df_validation[\"symbol\"].unique():\n        df_symbol = df_validation[df_validation[\"symbol\"] == symbol].copy()\n\n        # Sort by source_sequence\n        df_symbol = df_symbol.sort_values(\"source_sequence\")\n\n        # Check for gaps in source_sequence\n        if \"source_sequence\" in df_symbol.columns:\n            sequences = df_symbol[\"source_sequence\"].dropna().astype(int).values\n\n            if len(sequences) > 1:\n                # Find gaps\n                min_seq = int(sequences.min())\n                max_seq = int(sequences.max())\n                expected = set(range(min_seq, max_seq + 1))\n                actual = set(sequences)\n                gaps = expected - actual\n\n                if gaps:\n                    gaps_detected.append({\n                        \"symbol\": symbol,\n                        \"gap_count\": len(gaps),\n                        \"sequence_range\": f\"{min_seq} - {max_seq}\",\n                        \"total_trades\": len(sequences),\n                    })\n\n    if gaps_detected:\n        print(\" Sequence gaps detected:\\n\")\n\n        gaps_table = Table(title=\"Sequence Gaps by Symbol\", show_header=True)\n        gaps_table.add_column(\"Symbol\", style=\"cyan\")\n        gaps_table.add_column(\"Gap Count\", style=\"yellow\")\n        gaps_table.add_column(\"Sequence Range\", style=\"green\")\n        gaps_table.add_column(\"Total Trades\", style=\"magenta\")\n\n        for gap in gaps_detected:\n            gaps_table.add_row(\n                gap[\"symbol\"],\n                str(gap[\"gap_count\"]),\n                gap[\"sequence_range\"],\n                str(gap[\"total_trades\"])\n            )\n\n        print(gaps_table)\n        print(\"\\nNote: Sequence gaps are normal in live streaming (network delays, filtering, etc.)\")\n    else:\n        print(\" No sequence gaps detected\")\nelse:\n    print(\" No data available for sequence gap detection\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeqoq4hh6qf",
   "metadata": {},
   "outputs": [],
   "source": "# Duplicate detection\nprint(\"\\nDetecting duplicates...\\n\")\n\nif df_validation is not None and len(df_validation) > 0:\n    # Check for duplicate message_ids\n    duplicate_msgs = df_validation[df_validation.duplicated(subset=[\"message_id\"], keep=False)]\n\n    print(f\"Duplicate message_ids: {len(duplicate_msgs)}\")\n\n    if len(duplicate_msgs) > 0:\n        print(\" Found duplicate message IDs:\")\n        display(duplicate_msgs[[\"message_id\", \"symbol\", \"timestamp\", \"trade_id\"]].head(10))\n    else:\n        print(\" No duplicate message_ids\")\n\n    # Check for duplicate trade_ids\n    duplicate_trades = df_validation[df_validation.duplicated(subset=[\"trade_id\"], keep=False)]\n\n    print(f\"\\nDuplicate trade_ids: {len(duplicate_trades)}\")\n\n    if len(duplicate_trades) > 0:\n        print(\" Found duplicate trade IDs:\")\n        display(duplicate_trades[[\"trade_id\", \"symbol\", \"timestamp\", \"message_id\"]].head(10))\n    else:\n        print(\" No duplicate trade_ids\")\n\n    # Store for quality score\n    has_duplicates = len(duplicate_msgs) > 0 or len(duplicate_trades) > 0\n\nelse:\n    print(\" No data available for duplicate detection\")\n    has_duplicates = False"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3xaisggigzv",
   "metadata": {},
   "outputs": [],
   "source": "# Data quality summary\nprint(\"\\n\" + \"=\" * 60)\nprint(\"DATA QUALITY REPORT\")\nprint(\"=\" * 60 + \"\\n\")\n\nif df_validation is not None and len(df_validation) > 0:\n    print(\"Dataset:\")\n    print(f\"  Total trades analyzed: {len(df_validation):,}\")\n    print(f\"  Unique symbols: {df_validation['symbol'].nunique()}\")\n    print(f\"  Time range: {df_validation['timestamp'].min()} to {df_validation['timestamp'].max()}\")\n\n    print(\"\\nQuality Checks:\")\n\n    # Schema validation\n    schema_pass = all_valid if 'all_valid' in locals() else True\n    print(f\"  {'' if schema_pass else ''} Schema validation: {'All fields present and valid' if schema_pass else 'Some fields have issues'}\")\n\n    # Sequence gaps\n    gaps_pass = len(gaps_detected) == 0 if 'gaps_detected' in locals() else True\n    print(f\"  {'' if gaps_pass else ''} Sequence gaps: {len(gaps_detected) if 'gaps_detected' in locals() else 0} symbols with gaps\")\n\n    # Duplicates\n    dup_pass = not has_duplicates\n    print(f\"  {'' if dup_pass else ''} Duplicate detection: {'No duplicates' if dup_pass else 'Duplicates found'}\")\n\n    # Calculate quality score\n    score = 100\n    if not schema_pass:\n        score -= 30\n    if not gaps_pass:\n        score -= 10  # Gaps are normal in streaming\n    if not dup_pass:\n        score -= 40\n\n    # Display score with color\n    if score >= 90:\n        score_color = \"green\"\n        rating = \"EXCELLENT\"\n    elif score >= 70:\n        score_color = \"yellow\"\n        rating = \"GOOD\"\n    elif score >= 50:\n        score_color = \"yellow\"\n        rating = \"FAIR\"\n    else:\n        score_color = \"red\"\n        rating = \"POOR\"\n\n    print(f\"\\n[bold {score_color}]Data Quality Score: {score}/100 ({rating})[/bold {score_color}]\")\n\n    # Recommendations\n    if score < 100:\n        print(\"\\nRecommendations:\")\n        if not schema_pass:\n            print(\"  \u2022 Fix schema validation issues (missing or null fields)\")\n        if not dup_pass:\n            print(\"  \u2022 Investigate duplicate detection logic in consumer\")\n        if not gaps_pass:\n            print(\"  \u2022 Sequence gaps are normal but monitor gap frequency\")\n    else:\n        print(\"\\n Data quality is excellent! Pipeline is operating correctly.\")\nelse:\n    print(\" No data available for quality report\")\n\nprint(\"\\n\" + \"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "id": "w4otl2o6kt",
   "metadata": {},
   "source": "## 13. Vendor Data Analysis\n\nThe v2 schema's `vendor_data` field is key to multi-source compatibility. Let's analyze how Binance-specific fields are preserved:\n1. **Field coverage** - Which vendor fields are present\n2. **Field statistics** - Coverage percentage across trades\n3. **Example data** - Sample vendor_data entries"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltba4ic4ceq",
   "metadata": {},
   "outputs": [],
   "source": "# Parse vendor_data and analyze field coverage\nprint(\"\\nAnalyzing vendor_data fields...\\n\")\n\nimport json\n\nif df_validation is not None and len(df_validation) > 0:\n    # Parse vendor_data JSON for all trades\n    df_validation[\"vendor_parsed\"] = df_validation[\"vendor_data\"].apply(\n        lambda x: json.loads(x) if pd.notna(x) and x else {}\n    )\n\n    # Collect all unique vendor fields\n    all_vendor_fields = set()\n    for vendor_dict in df_validation[\"vendor_parsed\"]:\n        all_vendor_fields.update(vendor_dict.keys())\n\n    print(f\" Found {len(all_vendor_fields)} unique Binance-specific fields\\n\")\n    print(\"Binance vendor_data fields:\")\n    for field in sorted(all_vendor_fields):\n        print(f\"  \u2022 {field}\")\n\n    # Calculate field coverage\n    field_counts = {}\n    for field in all_vendor_fields:\n        count = sum(1 for vd in df_validation[\"vendor_parsed\"] if field in vd)\n        field_counts[field] = count\n\n    # Create coverage table\n    coverage_table = Table(title=\"Vendor Field Coverage Statistics\", show_header=True)\n    coverage_table.add_column(\"Field\", style=\"cyan\")\n    coverage_table.add_column(\"Count\", style=\"green\")\n    coverage_table.add_column(\"Coverage %\", style=\"yellow\")\n\n    total_trades = len(df_validation)\n    for field in sorted(field_counts.keys(), key=lambda x: field_counts[x], reverse=True):\n        count = field_counts[field]\n        percentage = (count / total_trades) * 100\n        coverage_table.add_row(\n            field,\n            f\"{count:,}\",\n            f\"{percentage:.1f}%\"\n        )\n\n    print(\"\\n\")\n    print(coverage_table)\n\nelse:\n    print(\" No data available for vendor_data analysis\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aovu7yauu8k",
   "metadata": {},
   "outputs": [],
   "source": "# Show example vendor_data entries\nprint(\"\\nExample vendor_data entries:\\n\")\n\nif df_validation is not None and len(df_validation) > 0:\n    # Show 3 example vendor_data entries\n    num_examples = min(3, len(df_validation))\n\n    for i in range(num_examples):\n        trade = df_validation.iloc[i]\n        vendor_data = trade[\"vendor_parsed\"]\n\n        print(f\"Trade {i+1} ({trade['symbol']}):\")\n        print(\"  Core fields:\")\n        print(f\"    \u2022 Price: {trade['price']} {trade['currency']}\")\n        print(f\"    \u2022 Quantity: {trade['quantity']}\")\n        print(f\"    \u2022 Side: {trade['side']}\")\n        print(f\"    \u2022 Timestamp: {trade['timestamp']}\")\n\n        print(\"  Vendor data (Binance-specific):\")\n        for key, value in vendor_data.items():\n            print(f\"    \u2022 {key}: {value}\")\n        print()\n\n    print(\"Key Insight:\")\n    print(\"  \u2022 Core fields (price, quantity, side) are normalized across all exchanges\")\n    print(\"  \u2022 vendor_data preserves exchange-specific fields without modification\")\n    print(\"  \u2022 Same v2 schema works for ASX (equities), Binance (crypto), and future exchanges\")\n    print(\"  \u2022 Enables advanced analysis using exchange-specific metadata\")\n\nelse:\n    print(\" No data available for vendor_data examples\")"
  },
  {
   "cell_type": "markdown",
   "id": "z1a5i3emkv",
   "metadata": {},
   "source": "## 14. Architecture Summary & Links\n\n### What We Demonstrated\n\nThis notebook showcased the complete end-to-end Binance cryptocurrency streaming pipeline:\n\n| Component | Description | Status |\n|-----------|-------------|--------|\n| **Binance WebSocket** | Live crypto trades streaming (BTC, ETH) | \u2713 Validated |\n| **Kafka Broker** | Message queueing with 4 partitions | \u2713 Validated |\n| **Schema Registry** | V2 Avro schema management | \u2713 Validated |\n| **Consumer Pipeline** | Batch processing (Kafka \u2192 Iceberg) | \u2713 Validated |\n| **Iceberg Lakehouse** | ACID transactions, Parquet storage | \u2713 Validated |\n| **Query Engine** | DuckDB analytical queries | \u2713 Validated |\n| **V2 Hybrid Schema** | Standard fields + vendor_data map | \u2713 Validated |\n| **Multi-Asset-Class** | Crypto + Equities support | \u2713 Validated |\n| **Prometheus Metrics** | Real-time pipeline monitoring | \u2713 Validated |\n| **Data Quality** | Schema, sequence, duplicate checks | \u2713 Validated |\n\n---\n\n### Key Achievements (Phase 2 Prep)\n\n**V2 Schema Evolution:**\n- Industry-standard core fields (symbol, price, quantity, side, etc.)\n- `vendor_data` JSON map for exchange-specific fields\n- Works across ASX (equities) and Binance (crypto)\n- Future-proof for Coinbase, Kraken, and other exchanges\n\n**Multi-Source Capability:**\n- Batch ingestion: ASX CSV files\n- Streaming ingestion: Binance WebSocket\n- Same v2 schema for both sources\n- Unified query interface\n\n**Production-Grade Features:**\n- SSL/TLS support\n- Exponential backoff and circuit breakers\n- Prometheus metrics (7 Binance-specific metrics)\n- Sub-second query performance\n- 138+ msg/s consumer throughput\n\n**Data Guarantees:**\n- ACID transactions via Apache Iceberg\n- Exactly-once semantics with idempotent producers\n- Schema evolution support\n- Time-travel queries\n\n---\n\n### Performance Metrics\n\nFrom E2E validation session (2026-01-13):\n\n- **Messages Received**: 69,666+ trades from Binance\n- **Messages Written**: 5,000+ trades to Iceberg\n- **Consumer Throughput**: 138 msg/s\n- **Query Latency**: Sub-second for 5,000 records\n- **Uptime**: 0 connection errors during demo\n\n---\n\n### Links & Resources\n\n**Local Services:**\n- **Prometheus Metrics**: [http://localhost:9090](http://localhost:9090)\n- **Grafana Dashboards**: [http://localhost:3000](http://localhost:3000) (admin/admin)\n- **Kafka UI**: [http://localhost:8080](http://localhost:8080)\n- **MinIO Console**: [http://localhost:9001](http://localhost:9001) (minioadmin/minioadmin)\n- **Schema Registry**: [http://localhost:8081](http://localhost:8081)\n\n**Documentation:**\n- Phase 2 Prep README: `docs/phases/phase-2-prep/README.md`\n- E2E Demo Success Summary: `docs/operations/e2e-demo-success-summary.md`\n- V2 Schema Design: `docs/architecture/schema-design-v2.md`\n- Streaming Architecture: `docs/architecture/streaming-architecture.md`\n\n**Scripts:**\n- Binance Streaming: `scripts/binance_stream.py`\n- Infrastructure Init: `scripts/init_e2e_demo.py`\n- Consumer: `src/k2/ingestion/consumer.py`\n- Query Engine: `src/k2/query/engine.py`\n\n---\n\n### Next Steps\n\n**Explore the Platform:**\n1. View metrics in Prometheus: [http://localhost:9090](http://localhost:9090)\n2. Create Grafana dashboards for real-time monitoring\n3. Query trades via the REST API (coming in Phase 2 Demo Enhancements)\n4. Try the ASX historical data demo: `notebooks/demo.ipynb`\n\n**Extend the Pipeline:**\n1. Add more crypto symbols (BNB, SOL, ADA, etc.)\n2. Stream from multiple exchanges (see Section 15 TODO)\n3. Build custom analytics queries\n4. Set up alerting rules in Prometheus\n\n**Production Deployment:**\n1. Enable SSL certificate verification\n2. Add Kafka broker replication (3+ brokers)\n3. Deploy distributed Iceberg catalog\n4. Set up authentication and authorization\n5. Configure data retention policies\n\n---\n\n### Architecture Highlights\n\n**Why This Architecture?**\n\n1. **Apache Kafka**: Industry-standard streaming platform, horizontal scalability, fault tolerance\n2. **Apache Iceberg**: ACID guarantees, time-travel queries, schema evolution, S3 compatibility\n3. **DuckDB**: Embedded analytics, Parquet-native, sub-second queries, no separate server\n4. **Avro + Schema Registry**: Schema evolution, compact serialization, version management\n5. **Prometheus + Grafana**: Real-time metrics, alerting, visualization\n\n**Design Principles:**\n\n- **Separation of Concerns**: Ingestion \u2192 Storage \u2192 Query\n- **Idempotency**: Safe retries, exactly-once semantics\n- **Schema Evolution**: Forward and backward compatibility\n- **Multi-Tenancy**: Asset classes, exchanges, symbols\n- **Observability**: Metrics, logs, traces\n\n---\n\n**Congratulations!** You've completed the Binance E2E streaming pipeline demo.\n\nThis platform demonstrates **Principal/Staff-level data engineering** with production-grade architecture, multi-source compatibility, and sub-second query performance."
  },
  {
   "cell_type": "markdown",
   "id": "39xvyyrv9vl",
   "metadata": {},
   "source": "## 15. TODO - Cross-Exchange Comparison (Future Enhancement)\n\n### Goal\n\nCompare cryptocurrency prices across multiple exchanges (Binance, Coinbase, Kraken) to:\n- Demonstrate the platform's multi-source capability\n- Identify arbitrage opportunities\n- Validate v2 schema works across exchanges\n- Show value of vendor_data for exchange-specific analysis\n\n---\n\n### Implementation Plan\n\n#### 1. Add Coinbase WebSocket Client\n\n**File**: `src/k2/ingestion/coinbase_client.py`\n\nSimilar to `BinanceWebSocketClient`, implement:\n- WebSocket connection to `wss://ws-feed.exchange.coinbase.com`\n- Subscribe to `matches` channel for BTC-USD, ETH-USD\n- Convert Coinbase messages to v2 schema\n- Store Coinbase-specific fields in `vendor_data`:\n  - `maker_order_id`, `taker_order_id`\n  - `sequence`, `product_id`\n  - `maker_fee`, `taker_fee`\n\n**Example Coinbase Message:**\n```json\n{\n  \"type\": \"match\",\n  \"trade_id\": 12345678,\n  \"maker_order_id\": \"abc123\",\n  \"taker_order_id\": \"def456\",\n  \"side\": \"buy\",\n  \"size\": \"0.05\",\n  \"price\": \"65000.00\",\n  \"product_id\": \"BTC-USD\",\n  \"sequence\": 987654321,\n  \"time\": \"2024-01-01T00:00:00.000000Z\"\n}\n```\n\n**V2 Conversion:**\n- `symbol`: \"BTCUSD\" (normalized)\n- `exchange`: \"COINBASE\"\n- `asset_class`: \"crypto\"\n- `vendor_data`: Coinbase-specific fields\n\n---\n\n#### 2. Add Kraken WebSocket Client\n\n**File**: `src/k2/ingestion/kraken_client.py`\n\nSimilar implementation for Kraken:\n- WebSocket connection to `wss://ws.kraken.com`\n- Subscribe to `trade` channel for XBT/USD, ETH/USD\n- Convert Kraken messages to v2 schema\n- Store Kraken-specific fields in `vendor_data`:\n  - `order_type` (market, limit)\n  - `misc` (additional flags)\n\n---\n\n#### 3. Create Multi-Exchange Streaming Service\n\n**File**: `scripts/multi_exchange_stream.py`\n\nConnect to multiple exchanges simultaneously:\n\n```python\nimport asyncio\nfrom k2.ingestion.binance_client import BinanceWebSocketClient\nfrom k2.ingestion.coinbase_client import CoinbaseWebSocketClient\nfrom k2.ingestion.kraken_client import KrakenWebSocketClient\nfrom k2.ingestion.producer import MarketDataProducer\n\nasync def stream_all_exchanges():\n    producer = MarketDataProducer(schema_version=\"v2\")\n    \n    # Create clients\n    binance = BinanceWebSocketClient(symbols=[\"BTCUSDT\", \"ETHUSDT\"], producer=producer)\n    coinbase = CoinbaseWebSocketClient(symbols=[\"BTC-USD\", \"ETH-USD\"], producer=producer)\n    kraken = KrakenWebSocketClient(symbols=[\"XBT/USD\", \"ETH/USD\"], producer=producer)\n    \n    # Connect all\n    await asyncio.gather(\n        binance.connect(),\n        coinbase.connect(),\n        kraken.connect(),\n    )\n    \n    # Stream indefinitely\n    await asyncio.Event().wait()\n\nif __name__ == \"__main__\":\n    asyncio.run(stream_all_exchanges())\n```\n\n**Kafka Topics:**\n- Option 1: Single topic `market.crypto.trades` (all exchanges)\n- Option 2: Per-exchange topics:\n  - `market.crypto.trades.binance`\n  - `market.crypto.trades.coinbase`\n  - `market.crypto.trades.kraken`\n\n---\n\n#### 4. Notebook Enhancements\n\nAdd new cells to this notebook:\n\n**Cell: Query trades from all exchanges**\n```python\n# Query BTCUSDT from all exchanges\ntrades_binance = engine.query_trades(symbol=\"BTCUSDT\", exchange=\"BINANCE\", limit=1000)\ntrades_coinbase = engine.query_trades(symbol=\"BTCUSD\", exchange=\"COINBASE\", limit=1000)\ntrades_kraken = engine.query_trades(symbol=\"XBTUSD\", exchange=\"KRAKEN\", limit=1000)\n\n# Normalize symbol names for comparison\ndf_binance = pd.DataFrame(trades_binance)\ndf_coinbase = pd.DataFrame(trades_coinbase)\ndf_kraken = pd.DataFrame(trades_kraken)\n```\n\n**Cell: Calculate price spreads**\n```python\n# Calculate average prices\navg_binance = df_binance[\"price\"].mean()\navg_coinbase = df_coinbase[\"price\"].mean()\navg_kraken = df_kraken[\"price\"].mean()\n\n# Calculate spreads (arbitrage opportunities)\nspread_binance_coinbase = abs(avg_binance - avg_coinbase)\nspread_binance_kraken = abs(avg_binance - avg_kraken)\nspread_coinbase_kraken = abs(avg_coinbase - avg_kraken)\n\nprint(f\"Price Spreads:\")\nprint(f\"  Binance-Coinbase: ${spread_binance_coinbase:.2f}\")\nprint(f\"  Binance-Kraken: ${spread_binance_kraken:.2f}\")\nprint(f\"  Coinbase-Kraken: ${spread_coinbase_kraken:.2f}\")\n```\n\n**Cell: Visualize price comparison**\n```python\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Plot prices from all exchanges\nax.plot(df_binance[\"timestamp\"], df_binance[\"price\"], label=\"Binance\", alpha=0.7)\nax.plot(df_coinbase[\"timestamp\"], df_coinbase[\"price\"], label=\"Coinbase\", alpha=0.7)\nax.plot(df_kraken[\"timestamp\"], df_kraken[\"price\"], label=\"Kraken\", alpha=0.7)\n\nax.set_xlabel(\"Time (UTC)\")\nax.set_ylabel(\"Price (USD)\")\nax.set_title(\"BTC Price Comparison Across Exchanges\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.show()\n```\n\n**Cell: Heatmap of spreads over time**\n```python\n# Aggregate prices by 1-minute buckets\ndf_binance[\"bucket\"] = df_binance[\"timestamp\"].dt.floor(\"1min\")\ndf_coinbase[\"bucket\"] = df_coinbase[\"timestamp\"].dt.floor(\"1min\")\ndf_kraken[\"bucket\"] = df_kraken[\"timestamp\"].dt.floor(\"1min\")\n\n# Calculate mean price per bucket\nprice_binance = df_binance.groupby(\"bucket\")[\"price\"].mean()\nprice_coinbase = df_coinbase.groupby(\"bucket\")[\"price\"].mean()\nprice_kraken = df_kraken.groupby(\"bucket\")[\"price\"].mean()\n\n# Calculate spread matrix\nspread_matrix = pd.DataFrame({\n    \"Binance-Coinbase\": abs(price_binance - price_coinbase),\n    \"Binance-Kraken\": abs(price_binance - price_kraken),\n    \"Coinbase-Kraken\": abs(price_coinbase - price_kraken),\n})\n\n# Plot heatmap\nimport seaborn as sns\nsns.heatmap(spread_matrix.T, cmap=\"YlOrRd\", cbar_kws={\"label\": \"Spread (USD)\"})\nplt.title(\"Price Spreads Over Time (1-minute buckets)\")\nplt.xlabel(\"Time\")\nplt.ylabel(\"Exchange Pair\")\nplt.show()\n```\n\n---\n\n### Benefits\n\n**Demonstrates Platform Capabilities:**\n- Multi-source data ingestion (3+ exchanges)\n- V2 schema flexibility (works across all exchanges)\n- vendor_data preserves exchange-specific fields\n- Unified query interface for cross-exchange analysis\n\n**Real-World Use Cases:**\n- **Arbitrage Detection**: Identify profitable price differences\n- **Market Microstructure**: Study price formation across venues\n- **Liquidity Analysis**: Compare trade volumes and spreads\n- **Best Execution**: Route orders to best-priced exchange\n\n**Technical Validation:**\n- Schema evolution: Same v2 schema for Binance, Coinbase, Kraken\n- Query performance: Sub-second queries across millions of trades\n- Data quality: Validate consistency across exchanges\n- Vendor data: Preserve exchange-specific fields for advanced analysis\n\n---\n\n### Example Output\n\n**Price Comparison Table:**\n\n| Exchange | Avg Price | Min Price | Max Price | Spread % | Volume |\n|----------|-----------|-----------|-----------|----------|--------|\n| Binance  | 65,123.45 | 65,000.00 | 65,250.00 | 0.38% | 150.25 BTC |\n| Coinbase | 65,145.20 | 65,020.00 | 65,270.00 | 0.38% | 98.50 BTC |\n| Kraken   | 65,110.80 | 64,990.00 | 65,230.00 | 0.37% | 75.30 BTC |\n\n**Arbitrage Opportunities:**\n\n- Binance \u2192 Coinbase: $21.75 spread (0.03%)\n- Kraken \u2192 Coinbase: $34.40 spread (0.05%) \u2190 **Best opportunity**\n- Binance \u2192 Kraken: $12.65 spread (0.02%)\n\n**Transaction costs**: ~0.1% (typical exchange fees)\n**Profitable if**: Spread > 0.1%\n**Result**: Kraken \u2192 Coinbase arbitrage is profitable (0.05% > 0.1%)\n\n---\n\n### Next Steps to Implement\n\n1. **Research APIs**: Study Coinbase and Kraken WebSocket APIs\n2. **Implement clients**: Build CoinbaseWebSocketClient and KrakenWebSocketClient\n3. **Test conversion**: Validate v2 schema conversion for each exchange\n4. **Deploy streaming**: Run multi_exchange_stream.py in production\n5. **Create notebook**: Add cross-exchange comparison cells\n6. **Document findings**: Write report on arbitrage opportunities\n\n**Estimated Time**: 8-12 hours\n**Priority**: Medium (nice-to-have, demonstrates platform flexibility)\n**Dependencies**: Phase 2 Prep complete (v2 schema + Binance streaming)\n\n---\n\n**Note**: This is documented as future work. The platform's v2 schema and architecture are already designed to support multiple exchanges - implementation is straightforward once exchange clients are built."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}