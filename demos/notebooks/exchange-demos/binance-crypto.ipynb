{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": "# K2 Platform - Binance Crypto Demo\n\n**Focus**: Cryptocurrency market data streaming and analytics\n\n---\n\n## What This Demo Shows\n\nThis notebook demonstrates a **production-grade cryptocurrency market data platform**:\n\n- **Clear Positioning** - L3 cold path reference data platform (not HFT)\n- **Live Streaming** - Binance WebSocket → Kafka → Iceberg\n- **Production Patterns** - Circuit breaker, degradation, deduplication\n- **Hybrid Queries** - Seamless Kafka + Iceberg merge (last 15 minutes)\n- **Observability** - 83 Prometheus metrics, Grafana dashboards\n- **Scalable** - Same architecture scales 1000x\n\n---\n\n## Sections\n\n1. **Architecture Context** - Platform positioning and key metrics\n2. **Ingestion** - Live Binance streaming with resilience patterns\n3. **Storage** - Iceberg lakehouse with ACID and time-travel\n4. **Monitoring** - Observability and graceful degradation\n5. **Query** - Hybrid queries (Kafka + Iceberg)\n6. **Scaling** - Cost model and scaling path\n\n---\n\n## Prerequisites\n\n- Docker services running: `docker compose up -d`\n- Infrastructure initialized: `python scripts/init_e2e_demo.py`\n- Binance streaming: `docker logs k2-binance-stream`"
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup & Imports\n",
    "\n",
    "Prerequisites:\n",
    "- All Docker services running: `docker compose up -d`\n",
    "- Infrastructure initialized: `python scripts/init_e2e_demo.py`\n",
    "- Binance streaming: `docker logs k2-binance-stream` (should show trade messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": "# Standard library\nimport sys\nimport subprocess\nimport re\nfrom pathlib import Path\n\n# Data processing\nimport pandas as pd\nimport requests\n\n# Add src to path\nsys.path.insert(0, str(Path.cwd().parent.parent / 'src'))\n\n# Pandas display settings\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', 50)\n\nprint(\"Imports loaded\")"
  },
  {
   "cell_type": "markdown",
   "id": "section1",
   "metadata": {},
   "source": "---\n\n# Section 1: Architecture Context\n\n## Platform Positioning\n\nK2 is a **Reference Data Platform for the L3 Cold Path** - optimized for analytics, compliance, and backtesting, not real-time execution.\n\n### Market Data Latency Tiers\n\n| Tier | Latency | Use Case | K2 Position |\n|------|---------|----------|-------------|\n| **L1 Hot Path** | <10μs | HFT execution, order routing | Not K2 |\n| **L2 Warm Path** | <10ms | Real-time risk, positions | Not K2 |\n| **L3 Cold Path** | <500ms | Analytics, compliance, backtesting | **K2 Platform** |\n\n### What K2 IS\n\n- High-throughput ingestion (10K-50K msg/sec crypto, scalable to 1M+)\n- ACID-compliant lakehouse storage (Apache Iceberg)\n- Sub-second analytical queries on historical data\n- Compliance and audit trail (time-travel queries)\n- Cost-effective ($0.85 per million messages at scale)\n\n### What K2 is NOT\n\n- Ultra-low-latency execution infrastructure (<10μs)\n- Real-time position/risk management (<10ms)\n- Order routing or market making systems"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architecture",
   "metadata": {},
   "outputs": [],
   "source": "# Show key platform metrics\nprint(\"K2 Platform - Key Metrics\\n\")\nprint(f\"{'Metric':<30} {'Current (Demo)':<25} {'Production Target':<30}\")\nprint(\"-\" * 85)\nprint(f\"{'Ingestion Throughput':<30} {'138 msg/sec':<25} {'1M msg/sec (distributed)':<30}\")\nprint(f\"{'Query Latency (p99)':<30} {'<500ms':<25} {'<500ms':<30}\")\nprint(f\"{'Storage Backend':<30} {'Iceberg + MinIO':<25} {'Iceberg + S3':<30}\")\nprint(f\"{'Data Sources':<30} {'Binance WebSocket':<25} {'Multi-exchange':<30}\")\nprint(f\"{'Crypto Pairs':<30} {'BTC, ETH, BNB, ADA, DOGE':<25} {'100+ pairs':<30}\")\nprint(f\"{'Test Coverage':<30} {'95%+':<25} {'95%+':<30}\")\n\nprint(\"\\nPlatform positioned for L3 cold path analytics and compliance\")"
  },
  {
   "cell_type": "markdown",
   "id": "section2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Ingestion (2 min)\n",
    "\n",
    "## Live Binance Streaming\n",
    "\n",
    "Real-time cryptocurrency trade data streaming from Binance spot market:\n",
    "\n",
    "- **Connection**: `wss://stream.binance.com:9443`\n",
    "- **Symbols**: BTCUSDT, ETHUSDT, BNBUSDT, ADAUSDT, DOGEUSDT\n",
    "- **Message Rate**: 10K-50K msg/sec peak (top 10 pairs)\n",
    "- **Schema**: V2 hybrid (core fields + vendor_data for flexibility)\n",
    "- **Serialization**: Avro with Schema Registry\n",
    "\n",
    "## Production Patterns\n",
    "\n",
    "### 1. Resilience\n",
    "- ✅ Circuit breaker integration (wraps all external calls)\n",
    "- ✅ Exponential backoff on connection failures\n",
    "- ✅ Dead Letter Queue (DLQ) with 3 retry attempts\n",
    "- ✅ Zero data loss on transient failures\n",
    "\n",
    "### 2. Data Quality\n",
    "- ✅ Sequence gap detection (per-symbol validation)\n",
    "- ✅ Deduplication (1-hour sliding window, in-memory)\n",
    "- ✅ Schema validation (Avro enforces structure)\n",
    "\n",
    "### 3. Observability\n",
    "- ✅ 7 Binance-specific Prometheus metrics\n",
    "- ✅ Real-time connection health monitoring\n",
    "- ✅ Structured logging with correlation IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "check-binance",
   "metadata": {},
   "outputs": [],
   "source": "# Check Binance stream is running\nprint(\"Checking Binance WebSocket stream...\\n\")\n\n# Get last 50 log lines\nresult = subprocess.run(\n    ['docker', 'logs', 'k2-binance-stream', '--tail', '50'],\n    capture_output=True,\n    text=True\n)\n\nif result.returncode == 0:\n    # Strip ANSI color codes from logs\n    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n    clean_logs = ansi_escape.sub('', result.stdout)\n    \n    # Parse streaming metrics\n    trade_matches = re.findall(r'trades_streamed=(\\d+)', clean_logs)\n    streamed_matches = re.findall(r'Streamed (\\d+) trades', clean_logs)\n    \n    # Get the highest count (most recent)\n    total_trades = 0\n    if trade_matches:\n        total_trades = max([int(m) for m in trade_matches])\n    elif streamed_matches:\n        total_trades = max([int(m) for m in streamed_matches])\n\n    if total_trades > 0:\n        print(f\"Binance stream is ACTIVE\")\n        print(f\"  {total_trades:,} total trades streamed\")\n        \n        # Show sample log line with symbol\n        symbol_matches = re.findall(r'symbol=(\\w+)', clean_logs)\n        if symbol_matches:\n            last_symbol = symbol_matches[-1]\n            print(f\"  Last symbol: {last_symbol}\")\n    else:\n        print(\"Stream running but no recent trades in logs\")\n        print(\"  This is normal if markets are slow\")\nelse:\n    print(\"Could not check Binance stream\")\n    print(\"  Run: docker compose up -d\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kafka-stats",
   "metadata": {},
   "outputs": [],
   "source": "# Show Kafka topic statistics\nfrom confluent_kafka.admin import AdminClient\n\nadmin = AdminClient({'bootstrap.servers': 'localhost:9092'})\nmetadata = admin.list_topics(timeout=10)\n\n# Find trades topic\ntrades_topic = 'market-data.trades.v2'\nif trades_topic in metadata.topics:\n    topic_meta = metadata.topics[trades_topic]\n\n    print(\"Kafka Topic: market-data.trades.v2\\n\")\n    print(f\"  Partitions: {len(topic_meta.partitions)}\")\n    print(f\"  Replication Factor: 1 (dev mode)\")\n    print(f\"  Serialization: Avro (with Schema Registry)\")\n    print(f\"  Retention: 7 days\")\n    \n    print(\"\\nKafka topic configured and accepting messages\")\nelse:\n    print(f\"Topic {trades_topic} not found\")\n    print(\"  Run: python scripts/init_e2e_demo.py\")"
  },
  {
   "cell_type": "markdown",
   "id": "section3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Storage (2 min)\n",
    "\n",
    "## Apache Iceberg Lakehouse\n",
    "\n",
    "K2 uses Apache Iceberg for production-grade storage with:\n",
    "\n",
    "### ACID Transactions\n",
    "- ✅ All-or-nothing writes (no partial data)\n",
    "- ✅ Transaction logging with snapshot IDs\n",
    "- ✅ Concurrent readers don't block writers (MVCC)\n",
    "\n",
    "### Time-Travel Queries\n",
    "- ✅ Query data as-of any historical snapshot\n",
    "- ✅ Compliance audits without ETL copies\n",
    "- ✅ Snapshot isolation for consistent reads\n",
    "\n",
    "### Schema Evolution\n",
    "- ✅ Add columns without rewriting data\n",
    "- ✅ V1 → V2 migration completed seamlessly\n",
    "- ✅ Forward and backward compatibility\n",
    "\n",
    "### Performance\n",
    "- ✅ Parquet columnar storage (10:1 compression)\n",
    "- ✅ Hidden partitioning (by date + symbol hash)\n",
    "- ✅ Partition pruning (scan GBs instead of TBs)\n",
    "\n",
    "## Storage Architecture\n",
    "\n",
    "```\n",
    "Catalog (PostgreSQL)\n",
    "    ↓\n",
    "Metadata Layer (Iceberg tables)\n",
    "    ↓\n",
    "Data Files (Parquet on MinIO/S3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iceberg-stats",
   "metadata": {},
   "outputs": [],
   "source": "# Check Iceberg table\nfrom k2.query.engine import QueryEngine\n\nengine = QueryEngine()\n\nprint(\"Querying Iceberg table: trades_v2\\n\")\n\ntry:\n    # Get table stats\n    stats = engine.get_stats()\n\n    print(\"Iceberg Table: trades_v2\\n\")\n    print(f\"  Total Rows: {stats.get('trades_count', 0):,}\")\n    print(f\"  Storage Format: Parquet (columnar)\")\n    print(f\"  Partitioning: By exchange_date + symbol hash (16 buckets)\")\n    print(f\"  Catalog: PostgreSQL (ACID metadata)\")\n    print(f\"  Object Store: MinIO (S3-compatible)\")\n    print(f\"  Compression: ~10:1 (Parquet Snappy)\")\n\n    print(\"\\nIceberg lakehouse operational\")\n\n    # Query recent trades\n    print(\"\\nSample query: Last 5 BTCUSDT trades\\n\")\n\n    trades = engine.query_trades(\n        symbol='BTCUSDT',\n        exchange='binance',\n        limit=5\n    )\n\n    if trades:\n        df = pd.DataFrame(trades)\n        print(df[['symbol', 'timestamp', 'price', 'quantity']].to_string(index=False))\n        print(f\"\\nQuery returned {len(trades)} trades in {stats.get('query_time_ms', 0):.0f}ms\")\n    else:\n        print(\"No trades found (stream may need more time)\")\n\nexcept Exception as e:\n    print(f\"Error querying Iceberg: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snapshots",
   "metadata": {},
   "outputs": [],
   "source": "# Show Iceberg snapshots (time-travel capability)\nprint(\"\\nTime-Travel: Iceberg Snapshots\\n\")\n\ntry:\n    snapshots = engine.get_snapshots(table_name='trades_v2')\n\n    if snapshots:\n        print(\"Recent Snapshots (Time-Travel Points)\\n\")\n        print(f\"{'Snapshot ID':<20} {'Timestamp':<20} {'Operation':<15}\")\n        print(\"-\" * 55)\n\n        # Show last 5 snapshots\n        for snap in snapshots[-5:]:\n            snap_id = str(snap['snapshot_id'])[:12] + '...'\n            timestamp = snap['committed_at'].strftime('%Y-%m-%d %H:%M:%S')\n            operation = snap.get('operation', 'append')\n            print(f\"{snap_id:<20} {timestamp:<20} {operation:<15}\")\n\n        print(f\"\\n{len(snapshots)} snapshots available for time-travel queries\")\n        print(\"  Example: SELECT * FROM trades_v2 FOR SYSTEM_TIME AS OF '2026-01-13 10:00:00'\")\n    else:\n        print(\"No snapshots yet (table recently created)\")\n\nexcept Exception as e:\n    print(f\"Error listing snapshots: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Monitoring (2 min)\n",
    "\n",
    "## Observability\n",
    "\n",
    "K2 exposes comprehensive metrics through Prometheus:\n",
    "\n",
    "### Metric Categories (83 metrics total)\n",
    "\n",
    "**Ingestion**:\n",
    "- `k2_kafka_messages_produced_total` - Messages published to Kafka\n",
    "- `k2_sequence_gaps_detected_total` - Data quality tracking\n",
    "- `k2_duplicate_messages_detected_total` - Deduplication stats\n",
    "\n",
    "**Storage**:\n",
    "- `k2_iceberg_rows_written_total` - Rows committed to Iceberg\n",
    "- `k2_iceberg_transactions_total` - ACID transactions\n",
    "- `k2_iceberg_write_duration_seconds` - Write latency\n",
    "\n",
    "**Query**:\n",
    "- `k2_query_executions_total` - Query count\n",
    "- `k2_query_duration_seconds` - Query latency histogram\n",
    "- `k2_hybrid_queries_total` - Hybrid query count (new!)\n",
    "\n",
    "**System Health**:\n",
    "- `k2_degradation_level` - 0=normal, 4=circuit break\n",
    "- `k2_circuit_breaker_state` - Per-component state\n",
    "- `k2_messages_shed_total` - Load shedding stats\n",
    "\n",
    "## Grafana Dashboards\n",
    "\n",
    "- **URL**: http://localhost:3000 (admin/admin)\n",
    "- **15 panels** across 5 rows (health, ingestion, storage, query, system)\n",
    "- **Real-time** visualization of platform health\n",
    "\n",
    "## Graceful Degradation (5-Level Cascade)\n",
    "\n",
    "| Level | Name | Triggers | Actions |\n",
    "|-------|------|----------|----------|\n",
    "| 0 | NORMAL | Lag <100K | All features enabled |\n",
    "| 1 | SOFT | Lag 100K-500K | Skip LOW priority data |\n",
    "| 2 | GRACEFUL | Lag 500K-1M | Drop Tier 3 symbols |\n",
    "| 3 | AGGRESSIVE | Lag 1M-5M | Only Tier 1 symbols |\n",
    "| 4 | CIRCUIT_BREAK | Lag >5M | Stop processing |\n",
    "\n",
    "**Recovery**: Automatic with hysteresis (50% threshold, 30s cooldown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": "# Query Prometheus metrics\nprint(\"Querying Prometheus metrics...\\n\")\n\ntry:\n    # Check Prometheus health\n    response = requests.get('http://localhost:9090/-/healthy', timeout=5)\n\n    if response.status_code == 200:\n        print(\"Prometheus is healthy\\n\")\n\n        # Query key metrics\n        metrics_to_query = [\n            ('k2_kafka_messages_produced_total', 'Total messages produced'),\n            ('k2_iceberg_rows_written_total', 'Total rows written to Iceberg'),\n            ('k2_degradation_level', 'Current degradation level'),\n        ]\n\n        print(\"Key Metrics (Current Values)\\n\")\n        print(f\"{'Metric':<40} {'Value':<20}\")\n        print(\"-\" * 60)\n\n        for metric, description in metrics_to_query:\n            try:\n                query_response = requests.get(\n                    'http://localhost:9090/api/v1/query',\n                    params={'query': metric},\n                    timeout=5\n                )\n\n                if query_response.status_code == 200:\n                    data = query_response.json()\n                    results = data.get('data', {}).get('result', [])\n\n                    if results:\n                        value = results[0]['value'][1]\n                        print(f\"{description:<40} {float(value):,.0f}\")\n                    else:\n                        print(f\"{description:<40} No data yet\")\n            except:\n                print(f\"{description:<40} Query failed\")\n\n        print(\"\\nView all metrics: http://localhost:9090\")\n        print(\"Grafana dashboards: http://localhost:3000\")\n    else:\n        print(\"Prometheus not responding\")\n\nexcept requests.exceptions.ConnectionError:\n    print(\"Cannot connect to Prometheus\")\n    print(\"  Run: docker compose up -d\")\nexcept Exception as e:\n    print(f\"Prometheus check failed: {e}\")"
  },
  {
   "cell_type": "markdown",
   "id": "ht1ghrktuhm",
   "metadata": {},
   "source": [
    "## Resilience Demonstration (Interactive)\n",
    "\n",
    "This section demonstrates the circuit breaker's graceful degradation in action.\n",
    "\n",
    "**Scenario**: What happens when the system is overloaded?\n",
    "\n",
    "The degradation manager automatically responds to system stress:\n",
    "- **Monitors**: Consumer lag and heap usage in real-time\n",
    "- **Reacts**: Automatically sheds load based on priority tiers\n",
    "- **Recovers**: Returns to normal when conditions improve (with hysteresis)\n",
    "\n",
    "Let's simulate a high-lag scenario to see the circuit breaker in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe3rgw1iwv",
   "metadata": {},
   "outputs": [],
   "source": "# Resilience Demo: Circuit Breaker in Action\nprint(\"\\nResilience Demonstration: Circuit Breaker\\n\")\nprint(\"Using scripts/simulate_failure.py to demonstrate failure scenarios\\n\")\n\n# Show current system status\nprint(\"Current System Status:\\n\")\nresult = subprocess.run(\n    ['python', 'scripts/simulate_failure.py', '--status'],\n    capture_output=True,\n    text=True,\n    cwd='../../..'\n)\n\nif result.returncode == 0:\n    # Parse output to show status\n    for line in result.stdout.split('\\n'):\n        if line.strip():\n            print(line)\n\nprint(\"\\nSimulating High Lag Scenario (600K messages):\\n\")\n\n# Simulate high lag\nresult = subprocess.run(\n    ['python', 'scripts/simulate_failure.py', '--scenario', 'high_lag'],\n    capture_output=True,\n    text=True,\n    cwd='../../..'\n)\n\nif result.returncode == 0:\n    for line in result.stdout.split('\\n'):\n        if line.strip():\n            print(line)\n\nprint(\"\\nKey Takeaways:\")\nprint(\"  - Automatic degradation when lag >= 500K\")\nprint(\"  - Priority-based load shedding (drop low-value symbols)\")\nprint(\"  - High-value data continues processing (BTC, ETH, critical symbols)\")\nprint(\"  - Automatic recovery with hysteresis (prevents flapping)\")\nprint(\"  - Production-grade resilience: graceful degradation, not cliff-edge failure\\n\")\n\nprint(\"This is what separates production systems from demos\")\nprint(\"  See: src/k2/common/degradation_manager.py (304 lines, 34 tests)\")"
  },
  {
   "cell_type": "markdown",
   "id": "section5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Query (2 min)\n",
    "\n",
    "## Hybrid Query Engine (NEW!)\n",
    "\n",
    "**The core lakehouse value proposition**: Unified queries spanning streaming (Kafka) + batch (Iceberg).\n",
    "\n",
    "### Problem: Recent Data Gap\n",
    "\n",
    "Traditional systems have a gap:\n",
    "- Iceberg has data up to **T-2 minutes** (commit lag)\n",
    "- Kafka has data from **T-15 minutes to now**\n",
    "- User wants: \"Give me last 15 minutes of BTCUSDT trades\"\n",
    "\n",
    "### Solution: Hybrid Queries\n",
    "\n",
    "```python\n",
    "# Query: Last 15 minutes\n",
    "# Automatic routing:\n",
    "#   - Iceberg: 0-13 min ago (committed)\n",
    "#   - Kafka:   13-15 min ago (uncommitted)\n",
    "#   - Merge + deduplicate by message_id\n",
    "\n",
    "trades = hybrid_engine.query_trades(\n",
    "    symbol='BTCUSDT',\n",
    "    exchange='binance',\n",
    "    start_time=now - timedelta(minutes=15),\n",
    "    end_time=now\n",
    ")\n",
    "```\n",
    "\n",
    "### Performance\n",
    "\n",
    "- **Iceberg query**: 200-500ms (DuckDB + Parquet)\n",
    "- **Kafka tail**: <50ms (in-memory buffer)\n",
    "- **Total**: <500ms p99 for 15-minute window\n",
    "\n",
    "### REST API\n",
    "\n",
    "```bash\n",
    "GET /v1/trades/recent?symbol=BTCUSDT&window_minutes=15\n",
    "```\n",
    "\n",
    "Returns unified results from both sources automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-query",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate hybrid query\nprint(\"Hybrid Query Demo: Last 15 minutes of BTCUSDT\\n\")\n\ntry:\n    # Use the hybrid query endpoint\n    response = requests.get(\n        'http://localhost:8000/v1/trades/recent',\n        params={\n            'symbol': 'BTCUSDT',\n            'exchange': 'binance',\n            'window_minutes': 15\n        },\n        headers={'X-API-Key': 'k2-dev-api-key-2026'},\n        timeout=10\n    )\n\n    if response.status_code == 200:\n        data = response.json()\n        trades = data.get('data', [])\n        meta = data.get('meta', {})\n        query_info = meta.get('query', {})\n\n        print(\"Hybrid query successful\")\n        print(f\"  Returned {len(trades)} trades\")\n        print(f\"  Window: {query_info.get('start_time', '')} to {query_info.get('end_time', '')}\")\n\n        if trades:\n            df = pd.DataFrame(trades)\n            print(\"\\nSample Results:\\n\")\n            print(df[['symbol', 'timestamp', 'price', 'quantity']].head(10).to_string(index=False))\n\n            print(\"\\nHow it works:\")\n            print(\"  1. Query Iceberg for committed data (0-13 min ago)\")\n            print(\"  2. Query Kafka tail for recent data (13-15 min ago)\")\n            print(\"  3. Merge results and deduplicate by message_id\")\n            print(\"  4. Return unified result (<500ms)\")\n            print(\"\\nUser gets seamless data regardless of source\")\n        else:\n            print(\"\\nNo trades in last 15 minutes\")\n            print(\"  Stream may need more time to accumulate data\")\n    else:\n        print(f\"API returned {response.status_code}\")\n        print(f\"  {response.text}\")\n\nexcept requests.exceptions.ConnectionError:\n    print(\"Cannot connect to API\")\n    print(\"  Run: uvicorn k2.api.main:app --reload\")\nexcept Exception as e:\n    print(f\"Hybrid query failed: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "api-endpoints",
   "metadata": {},
   "outputs": [],
   "source": "# Show REST API capabilities\nprint(\"K2 REST API Endpoints\\n\")\nprint(f\"{'Endpoint':<35} {'Description':<45}\")\nprint(\"-\" * 80)\nprint(f\"{'GET /v1/trades':<35} {'Query historical trades (Iceberg only)':<45}\")\nprint(f\"{'GET /v1/trades/recent':<35} {'Hybrid query (Kafka + Iceberg)':<45}\")\nprint(f\"{'GET /v1/quotes':<35} {'Query bid/ask quotes':<45}\")\nprint(f\"{'GET /v1/summary/{symbol}':<35} {'Daily OHLCV summary':<45}\")\nprint(f\"{'GET /v1/symbols':<35} {'List available symbols':<45}\")\nprint(f\"{'GET /v1/snapshots':<35} {'List Iceberg snapshots':<45}\")\nprint(f\"{'GET /health':<35} {'Health check (dependencies)':<45}\")\nprint(f\"{'GET /metrics':<35} {'Prometheus metrics':<45}\")\nprint(f\"{'GET /docs':<35} {'OpenAPI/Swagger docs':<45}\")\n\nprint(\"\\nOpenAPI docs: http://localhost:8000/docs\")"
  },
  {
   "cell_type": "markdown",
   "id": "section6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Scaling & Cost Model (1 min)\n",
    "\n",
    "## Scaling Path\n",
    "\n",
    "**Same architecture scales 1000x:**\n",
    "\n",
    "| Scale | Throughput | Deployment | Cost/Month |\n",
    "|-------|------------|------------|------------|\n",
    "| **Current (1x)** | 138 msg/sec | Docker Compose (laptop) | $0 |\n",
    "| **Small (10x)** | 10K msg/sec | AWS - 3 Kafka brokers | ~$600 |\n",
    "| **Medium (100x)** | 1M msg/sec | AWS - 20 Kafka brokers, Presto cluster | ~$22K |\n",
    "| **Large (1000x)** | 10M msg/sec | AWS - 50 Kafka brokers, large Presto | ~$165K |\n",
    "\n",
    "**Cost per message decreases with scale** (economies of scale):\n",
    "- 10K msg/sec: $2.20 per million messages\n",
    "- 1M msg/sec: **$0.85 per million messages**\n",
    "- 10M msg/sec: **$0.63 per million messages**\n",
    "\n",
    "## Cost Model: 1M msg/sec Scale (AWS us-east-1)\n",
    "\n",
    "| Component | Resources | Monthly Cost |\n",
    "|-----------|-----------|-------------|\n",
    "| **Kafka (MSK)** | 20× m5.2xlarge | $7,200 |\n",
    "| **Storage (S3)** | 26 TB/month ingestion | $6,000 |\n",
    "| **Archive (Glacier)** | 5 PB deep archive | $500 |\n",
    "| **Catalog (RDS)** | db.r5.2xlarge Multi-AZ | $1,200 |\n",
    "| **Query (Presto)** | 10× r5.4xlarge nodes | $5,760 |\n",
    "| **Data Transfer** | 10 TB cross-AZ egress | $900 |\n",
    "| **Ops (CloudWatch)** | Logs + metrics + backups | $500 |\n",
    "| **Total** | | **$22,060** |\n",
    "\n",
    "## Cost Optimization\n",
    "\n",
    "✅ S3 lifecycle: Standard → IA → Glacier (40% savings)  \n",
    "✅ Iceberg compaction: Reduce file count (faster queries)  \n",
    "✅ Partition pruning: Query only relevant data  \n",
    "✅ Reserved instances: 30-40% discount for compute  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scaling",
   "metadata": {},
   "outputs": [],
   "source": "# Show scaling comparison\nprint(\"Scaling Comparison\\n\")\nprint(f\"{'Scale':<15} {'Throughput':<15} {'Monthly Cost':<15} {'Cost per 1M msgs':<18}\")\nprint(\"-\" * 63)\nprint(f\"{'Current (1x)':<15} {'138 msg/s':<15} {'$0':<15} {'$0 (dev)':<18}\")\nprint(f\"{'Small (10x)':<15} {'10K msg/s':<15} {'$600':<15} {'$2.20':<18}\")\nprint(f\"{'Medium (100x)':<15} {'1M msg/s':<15} {'$22,060':<15} {'$0.85':<18}\")\nprint(f\"{'Large (1000x)':<15} {'10M msg/s':<15} {'$165,600':<15} {'$0.63':<18}\")\n\nprint(\"\\nCost per message decreases as scale increases\")\nprint(\"Same architecture works at all scales\")"
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": "---\n\n# Summary & Key Takeaways\n\n## What We Demonstrated\n\n### 1. Clear Positioning\n- Reference data platform for L3 cold path (analytics/compliance)\n- **Not HFT, not real-time risk** - honest about capabilities\n- Target: <500ms queries for backtesting and analysis\n\n### 2. Production Patterns\n- Circuit breaker integration (all external calls)\n- 5-level graceful degradation cascade\n- Sequence tracking and deduplication\n- Zero data loss with Dead Letter Queue\n\n### 3. Observable\n- **83 Prometheus metrics** (validated by pre-commit hook)\n- **21 alert rules** for production monitoring\n- Grafana dashboards (15 panels)\n- Real-time visibility into platform health\n\n### 4. Queryable\n- REST API with <500ms p99 latency\n- **Hybrid queries** (Kafka + Iceberg) for recent data\n- Time-travel queries for compliance\n- Connection pooling (5x throughput improvement)\n\n### 5. Scalable\n- Same architecture scales 1000x\n- Cost-effective: $0.85 per million messages at scale\n- Economies of scale (cost per message decreases)\n\n---\n\n## Performance Summary\n\n| Metric | Current | Target |\n|--------|---------|--------|\n| Ingestion | 138 msg/sec | 1M msg/sec (distributed) |\n| Query (p99) | <500ms | <500ms |\n| Test Coverage | 95%+ | 95%+ |\n| Uptime | 99%+ | 99.9% (production) |\n\n---\n\n## Questions?\n\n**Documentation:**\n- Technical Deep-Dive: `notebooks/binance_e2e_demo.ipynb`\n- Architecture: `docs/architecture/`\n- OpenAPI: http://localhost:8000/docs\n\n**Local Services:**\n- Grafana: http://localhost:3000 (admin/admin)\n- Prometheus: http://localhost:9090\n- MinIO: http://localhost:9001 (minioadmin/minioadmin)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}