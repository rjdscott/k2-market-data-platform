{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + Iceberg Queries - K2 Medallion Architecture\n",
    "\n",
    "Query Bronze/Silver/Gold Iceberg tables using PySpark locally from Jupyter notebook.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### 1. Environment Setup\n",
    "\n",
    "**Install Java (Required for PySpark):**\n",
    "\n",
    "PySpark requires Java 11 or Java 17. Check if Java is installed:\n",
    "\n",
    "```bash\n",
    "java -version\n",
    "```\n",
    "\n",
    "If not installed:\n",
    "\n",
    "**macOS:**\n",
    "```bash\n",
    "# Using Homebrew\n",
    "brew install openjdk@17\n",
    "\n",
    "# Set JAVA_HOME (add to ~/.zshrc or ~/.bash_profile)\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "**Ubuntu/Debian:**\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install openjdk-17-jdk\n",
    "\n",
    "# Set JAVA_HOME (add to ~/.bashrc)\n",
    "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "**Verify Java installation:**\n",
    "```bash\n",
    "echo $JAVA_HOME\n",
    "java -version  # Should show version 17.x.x\n",
    "```\n",
    "\n",
    "**Install Python dependencies:**\n",
    "```bash\n",
    "# Install Jupyter and PySpark (if not already installed)\n",
    "cd /path/to/k2-market-data-platform\n",
    "uv add jupyter pyspark==3.5.0\n",
    "\n",
    "# Verify installation\n",
    "uv run python -c \"import pyspark; print(f'PySpark {pyspark.__version__}')\"\n",
    "```\n",
    "\n",
    "### 2. Start Required Services\n",
    "\n",
    "**Start all infrastructure:**\n",
    "```bash\n",
    "# From project root\n",
    "docker-compose up -d\n",
    "\n",
    "# Verify services are running\n",
    "docker-compose ps\n",
    "\n",
    "# Should see:\n",
    "# - k2-spark-master (port 8090 - Spark Web UI)\n",
    "# - k2-spark-worker-1\n",
    "# - k2-spark-worker-2  \n",
    "# - k2-iceberg-rest (port 8181)\n",
    "# - k2-minio (port 9000)\n",
    "# - k2-kafka (port 9092)\n",
    "# - k2-kafka-ui (port 8080)\n",
    "```\n",
    "\n",
    "**Check Spark Web UI:**\n",
    "- Open: http://localhost:8090\n",
    "- Verify: 2 workers registered (each with 2 cores, 3GB memory)\n",
    "\n",
    "### 3. Create Bronze/Silver/Gold Tables\n",
    "\n",
    "**Create Bronze tables (Binance + Kraken):**\n",
    "```bash\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/create_bronze_table.py all\n",
    "```\n",
    "\n",
    "**Create Silver tables:**\n",
    "```bash\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/create_silver_tables.py\n",
    "```\n",
    "\n",
    "**Create Gold table:**\n",
    "```bash\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/create_gold_table.py\n",
    "```\n",
    "\n",
    "### 4. Start Data Producers (Optional)\n",
    "\n",
    "**Start crypto streaming producers to populate Bronze tables:**\n",
    "```bash\n",
    "# Terminal 1 - Binance WebSocket\n",
    "uv run python scripts/binance_stream.py\n",
    "\n",
    "# Terminal 2 - Kraken WebSocket  \n",
    "uv run python scripts/kraken_stream.py\n",
    "```\n",
    "\n",
    "Wait 30-60 seconds for data to accumulate in Kafka, then Bronze tables will be populated by the Spark streaming jobs.\n",
    "\n",
    "### 5. Launch This Notebook\n",
    "\n",
    "```bash\n",
    "# Set JAVA_HOME first (if not in your shell profile)\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)  # macOS\n",
    "# or\n",
    "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64  # Linux\n",
    "\n",
    "# Launch notebook from project root\n",
    "uv run jupyter notebook demos/notebooks/spark-iceberg-queries.ipynb\n",
    "```\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "**Local (Notebook) → Docker Services:**\n",
    "- **Notebook runs on host** with local PySpark installation\n",
    "- **Connects to Docker services:**\n",
    "  - Iceberg REST Catalog: `http://localhost:8181`\n",
    "  - MinIO (S3): `http://localhost:9000`\n",
    "  - Spark Cluster: Workers run in Docker (for streaming jobs)\n",
    "\n",
    "**Data Flow:**\n",
    "```\n",
    "WebSocket → Kafka → Bronze (Spark Streaming) → Silver → Gold\n",
    "                        ↓                         ↓       ↓\n",
    "                    (This notebook queries these tables)\n",
    "```\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "1. **Java Required:** PySpark needs Java 11 or 17 installed locally\n",
    "   - Check: `java -version`\n",
    "   - Set `JAVA_HOME` environment variable\n",
    "\n",
    "2. **Network Access:** All services must be accessible from host:\n",
    "   - Spark Web UI: `localhost:8090`\n",
    "   - Kafka UI: `localhost:8080`\n",
    "   - Iceberg REST: `localhost:8181`\n",
    "   - MinIO: `localhost:9000`\n",
    "   - Kafka: `localhost:9092`\n",
    "\n",
    "3. **Memory:** Ensure Docker has at least 8GB RAM allocated for Spark workers\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Issue: \"JAVA_HOME is not set\" or \"Java gateway process exited\"**\n",
    "\n",
    "```bash\n",
    "# Check if Java is installed\n",
    "java -version\n",
    "\n",
    "# If not installed, install Java 17 (see step 1 above)\n",
    "\n",
    "# Set JAVA_HOME\n",
    "# macOS:\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)\n",
    "\n",
    "# Linux:\n",
    "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
    "\n",
    "# Verify\n",
    "echo $JAVA_HOME\n",
    "\n",
    "# Add to shell profile for persistence\n",
    "# macOS: ~/.zshrc or ~/.bash_profile\n",
    "# Linux: ~/.bashrc\n",
    "echo 'export JAVA_HOME=$(/usr/libexec/java_home -v 17)' >> ~/.zshrc  # macOS\n",
    "```\n",
    "\n",
    "**Issue: Cannot connect to Iceberg catalog**\n",
    "```bash\n",
    "# Check iceberg-rest is running\n",
    "docker ps | grep iceberg-rest\n",
    "curl http://localhost:8181/v1/config\n",
    "```\n",
    "\n",
    "**Issue: Cannot read from S3/MinIO**\n",
    "```bash\n",
    "# Check MinIO is running\n",
    "docker ps | grep minio\n",
    "# Access MinIO console: http://localhost:9001 (admin/password)\n",
    "```\n",
    "\n",
    "**Issue: Tables don't exist**\n",
    "```bash\n",
    "# Run table creation scripts (see step 3 above)\n",
    "# Verify in MinIO: http://localhost:9001 → warehouse bucket\n",
    "```\n",
    "\n",
    "**Issue: Tables are empty**\n",
    "```bash\n",
    "# Check if producers are running and sending data to Kafka\n",
    "docker exec k2-kafka kafka-console-consumer --bootstrap-server localhost:9092 \\\n",
    "  --topic market.crypto.trades.binance --max-messages 5\n",
    "\n",
    "# Check if Bronze streaming jobs are running\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/spark-sql-kafka-0-10_2.12-3.5.3.jar,/opt/spark/jars-extra/kafka-clients-3.5.1.jar,/opt/spark/jars-extra/commons-pool2-2.11.1.jar,/opt/spark/jars-extra/spark-token-provider-kafka-0-10_2.12-3.5.3.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/streaming/bronze_binance_ingestion.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to query? Execute the cells below sequentially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Iceberg Catalog\n",
    "\n",
    "**Note:** First run will download required JARs (~500MB) from Maven Central. This takes 2-3 minutes but is cached for future runs.\n",
    "\n",
    "**Required JARs:**\n",
    "- `iceberg-spark-runtime-3.5_2.12:1.4.0` - Iceberg integration\n",
    "- `hadoop-aws:3.3.4` - S3A filesystem support  \n",
    "- `software.amazon.awssdk:bundle:2.20.18` - AWS SDK v2 for S3/MinIO\n",
    "- `software.amazon.awssdk:url-connection-client:2.20.18` - AWS HTTP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/18 22:14:40 WARN Utils: Your hostname, RS-Dev-01 resolves to a loopback address: 127.0.1.1; using 192.168.4.25 instead (on interface wlo1)\n",
      "26/01/18 22:14:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/rjdscott/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rjdscott/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f40092b1-169d-41dd-8501-ee162c5b06f1;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.18 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.18 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.18 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/rjdscott/Documents/projects/k2-market-data-platform/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound software.amazon.awssdk#annotations;2.20.18 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.18 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.18 in central\n",
      ":: resolution report :: resolve 135ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f40092b1-169d-41dd-8501-ee162c5b06f1\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/3ms)\n",
      "26/01/18 22:14:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/18 22:14:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.5.0 connected to Iceberg catalog\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Iceberg catalog\n",
    "# Note: Using .packages to automatically download required JARs\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"K2-Iceberg-Query-Demo\")\n",
    "    # Iceberg catalog configuration\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://localhost:8181\")  # iceberg-rest\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3://warehouse/\")\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://localhost:9000\")  # minio\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"password\")\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\")\n",
    "    # S3/MinIO configuration\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"us-east-1\")\n",
    "    # AWS SDK v2 region configuration (required for MinIO)\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Daws.region=us-east-1\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Daws.region=us-east-1\")\n",
    "    # Download required JARs automatically (Maven coordinates)\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"software.amazon.awssdk:bundle:2.20.18,\"\n",
    "            \"software.amazon.awssdk:url-connection-client:2.20.18\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark {spark.version} connected to Iceberg catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Iceberg Catalog URI: http://localhost:8181\n",
      "Warehouse: s3://warehouse/\n",
      "S3 Endpoint: http://localhost:9000\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|market_data|\n",
      "+-----------+\n",
      "\n",
      "\n",
      "✓ Successfully connected to Iceberg catalog\n"
     ]
    }
   ],
   "source": [
    "## Verify Connection\n",
    "\n",
    "# Check Spark version and configuration\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Iceberg Catalog URI: {spark.conf.get('spark.sql.catalog.iceberg.uri')}\")\n",
    "print(f\"Warehouse: {spark.conf.get('spark.sql.catalog.iceberg.warehouse')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "\n",
    "# Test catalog connection\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n",
    "    print(\"\\n✓ Successfully connected to Iceberg catalog\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Failed to connect to Iceberg catalog: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+-----------+\n",
      "|namespace  |tableName            |isTemporary|\n",
      "+-----------+---------------------+-----------+\n",
      "|market_data|silver_binance_trades|false      |\n",
      "|market_data|silver_kraken_trades |false      |\n",
      "|market_data|gold_crypto_trades   |false      |\n",
      "|market_data|bronze_kraken_trades |false      |\n",
      "|market_data|bronze_binance_trades|false      |\n",
      "+-----------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN iceberg.market_data\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer - Raw Kafka Data\n",
    "\n",
    "Raw Avro bytes from Kafka (per-exchange tables).\n",
    "\n",
    "**What to expect:**\n",
    "- `bronze_binance_trades`: Raw Kafka messages from Binance WebSocket\n",
    "- `bronze_kraken_trades`: Raw Kafka messages from Kraken WebSocket\n",
    "- Fields: `message_key`, `avro_payload` (binary), Kafka metadata, ingestion timestamp\n",
    "- Partitioned by: `ingestion_date`\n",
    "\n",
    "**Note:** If tables are empty, start the Bronze streaming jobs and data producers (see setup instructions above).\n",
    "\n",
    "**Quick check - see if any data exists:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Table Row Counts:\n",
      "==================================================\n",
      "bronze_binance_trades: 401,262 records\n",
      "bronze_kraken_trades: 259 records\n",
      "\n",
      "✓ Found 401,521 total records\n"
     ]
    }
   ],
   "source": [
    "# Quick check - row counts for Bronze tables\n",
    "print(\"Bronze Table Row Counts:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "binance_count = spark.sql(\"SELECT COUNT(*) as count FROM iceberg.market_data.bronze_binance_trades\").collect()[0]['count']\n",
    "print(f\"bronze_binance_trades: {binance_count:,} records\")\n",
    "\n",
    "kraken_count = spark.sql(\"SELECT COUNT(*) as count FROM iceberg.market_data.bronze_kraken_trades\").collect()[0]['count']\n",
    "print(f\"bronze_kraken_trades: {kraken_count:,} records\")\n",
    "\n",
    "if binance_count == 0 and kraken_count == 0:\n",
    "    print(\"\\n⚠️  Bronze tables are empty. Start the Bronze streaming jobs and producers.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found {binance_count + kraken_count:,} total records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer - Raw Kafka Data\n",
    "\n",
    "Raw Avro bytes from Kafka (per-exchange tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|ingestion_date|records|kafka_partitions|         first_trade|          last_trade|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|    2026-01-18| 399672|               3|2026-01-18 19:22:...|2026-01-18 22:26:...|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binance Bronze - row count by partition\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ingestion_date,\n",
    "        COUNT(*) as records,\n",
    "        COUNT(DISTINCT partition) as kafka_partitions,\n",
    "        MIN(kafka_timestamp) as first_trade,\n",
    "        MAX(kafka_timestamp) as last_trade\n",
    "    FROM iceberg.market_data.bronze_binance_trades\n",
    "    GROUP BY ingestion_date\n",
    "    ORDER BY ingestion_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|ingestion_date|records|kafka_partitions|         first_trade|          last_trade|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|    2026-01-18|    259|               2|2026-01-18 22:03:...|2026-01-18 22:27:...|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kraken Bronze - row count\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ingestion_date,\n",
    "        COUNT(*) as records,\n",
    "        COUNT(DISTINCT partition) as kafka_partitions,\n",
    "        MIN(kafka_timestamp) as first_trade,\n",
    "        MAX(kafka_timestamp) as last_trade\n",
    "    FROM iceberg.market_data.bronze_kraken_trades\n",
    "    GROUP BY ingestion_date\n",
    "    ORDER BY ingestion_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer - Validated Per-Exchange Trades\n",
    "\n",
    "Deserialized and validated V2 trades (per-exchange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binance Silver - recent trades\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        message_id,\n",
    "        trade_id,\n",
    "        symbol,\n",
    "        exchange,\n",
    "        CAST(timestamp / 1000000 AS TIMESTAMP) as trade_time,\n",
    "        price,\n",
    "        quantity,\n",
    "        side\n",
    "    FROM iceberg.market_data.silver_binance_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binance Silver - price aggregation (1-minute bars)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        DATE_TRUNC('minute', CAST(timestamp / 1000000 AS TIMESTAMP)) as minute,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(AVG(price), 2) as avg_price,\n",
    "        ROUND(MIN(price), 2) as low,\n",
    "        ROUND(MAX(price), 2) as high,\n",
    "        ROUND(SUM(quantity), 8) as volume\n",
    "    FROM iceberg.market_data.silver_binance_trades\n",
    "    WHERE symbol = 'BTCUSDT'\n",
    "      AND exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY symbol, minute\n",
    "    ORDER BY minute DESC\n",
    "    LIMIT 20\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kraken Silver - recent trades\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        CAST(timestamp / 1000000 AS TIMESTAMP) as trade_time,\n",
    "        price,\n",
    "        quantity,\n",
    "        side,\n",
    "        vendor_data\n",
    "    FROM iceberg.market_data.silver_kraken_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer - Unified Multi-Exchange Analytics\n",
    "\n",
    "Combined trades from all exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold - trades by exchange (last 24h)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exchange,\n",
    "        COUNT(*) as trades,\n",
    "        COUNT(DISTINCT symbol) as symbols,\n",
    "        ROUND(SUM(price * quantity), 2) as total_value_usd\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY exchange\n",
    "    ORDER BY trades DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold - cross-exchange price comparison (BTC/USDT)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exchange,\n",
    "        symbol,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(AVG(price), 2) as avg_price,\n",
    "        ROUND(MIN(price), 2) as min_price,\n",
    "        ROUND(MAX(price), 2) as max_price,\n",
    "        ROUND(STDDEV(price), 2) as price_stddev\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    WHERE symbol IN ('BTCUSDT', 'BTCUSD')\n",
    "      AND exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY exchange, symbol\n",
    "    ORDER BY exchange\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold - hourly trade volume (all exchanges)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exchange_date,\n",
    "        exchange_hour,\n",
    "        exchange,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(SUM(quantity), 8) as total_quantity\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY exchange_date, exchange_hour, exchange\n",
    "    ORDER BY exchange_date DESC, exchange_hour DESC\n",
    "    LIMIT 50\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Schema Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Bronze table schema\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.market_data.bronze_binance_trades\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Silver table schema\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.market_data.silver_binance_trades\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Gold table schema\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.market_data.gold_crypto_trades\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table History & Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table history (Iceberg time-travel)\n",
    "spark.sql(\"SELECT * FROM iceberg.market_data.bronze_binance_trades.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show snapshots\n",
    "spark.sql(\"SELECT * FROM iceberg.market_data.bronze_binance_trades.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"✓ Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
