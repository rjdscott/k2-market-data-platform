{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + Iceberg Queries - K2 Medallion Architecture\n",
    "\n",
    "Query Bronze/Silver/Gold Iceberg tables using PySpark locally from Jupyter notebook.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### 1. Environment Setup\n",
    "\n",
    "**Install Java (Required for PySpark):**\n",
    "\n",
    "PySpark requires Java 11 or Java 17. Check if Java is installed:\n",
    "\n",
    "```bash\n",
    "java -version\n",
    "```\n",
    "\n",
    "If not installed:\n",
    "\n",
    "**macOS:**\n",
    "```bash\n",
    "# Using Homebrew\n",
    "brew install openjdk@17\n",
    "\n",
    "# Set JAVA_HOME (add to ~/.zshrc or ~/.bash_profile)\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "**Ubuntu/Debian:**\n",
    "```bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install openjdk-17-jdk\n",
    "\n",
    "# Set JAVA_HOME (add to ~/.bashrc)\n",
    "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
    "export PATH=$JAVA_HOME/bin:$PATH\n",
    "```\n",
    "\n",
    "**Verify Java installation:**\n",
    "```bash\n",
    "echo $JAVA_HOME\n",
    "java -version  # Should show version 17.x.x\n",
    "```\n",
    "\n",
    "**Install Python dependencies:**\n",
    "```bash\n",
    "# Install Jupyter and PySpark (if not already installed)\n",
    "cd /path/to/k2-market-data-platform\n",
    "uv add jupyter pyspark==3.5.0\n",
    "\n",
    "# Verify installation\n",
    "uv run python -c \"import pyspark; print(f'PySpark {pyspark.__version__}')\"\n",
    "```\n",
    "\n",
    "### 2. Start Required Services\n",
    "\n",
    "**Start all infrastructure:**\n",
    "```bash\n",
    "# From project root\n",
    "docker-compose up -d\n",
    "\n",
    "# Verify services are running\n",
    "docker-compose ps\n",
    "\n",
    "# Should see:\n",
    "# - k2-spark-master (port 8090 - Spark Web UI)\n",
    "# - k2-spark-worker-1\n",
    "# - k2-spark-worker-2  \n",
    "# - k2-iceberg-rest (port 8181)\n",
    "# - k2-minio (port 9000)\n",
    "# - k2-kafka (port 9092)\n",
    "# - k2-kafka-ui (port 8080)\n",
    "```\n",
    "\n",
    "**Check Spark Web UI:**\n",
    "- Open: http://localhost:8090\n",
    "- Verify: 2 workers registered (each with 2 cores, 3GB memory)\n",
    "\n",
    "### 3. Create Bronze/Silver/Gold Tables\n",
    "\n",
    "**Create Bronze tables (Binance + Kraken):**\n",
    "```bash\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/create_bronze_table.py all\n",
    "```\n",
    "\n",
    "**Create Silver tables:**\n",
    "```bash\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/create_silver_tables.py\n",
    "```\n",
    "\n",
    "**Create Gold table:**\n",
    "```bash\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/create_gold_table.py\n",
    "```\n",
    "\n",
    "### 4. Start Data Producers (Optional)\n",
    "\n",
    "**Start crypto streaming producers to populate Bronze tables:**\n",
    "```bash\n",
    "# Terminal 1 - Binance WebSocket\n",
    "uv run python scripts/binance_stream.py\n",
    "\n",
    "# Terminal 2 - Kraken WebSocket  \n",
    "uv run python scripts/kraken_stream.py\n",
    "```\n",
    "\n",
    "Wait 30-60 seconds for data to accumulate in Kafka, then Bronze tables will be populated by the Spark streaming jobs.\n",
    "\n",
    "### 5. Launch This Notebook\n",
    "\n",
    "```bash\n",
    "# Set JAVA_HOME first (if not in your shell profile)\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)  # macOS\n",
    "# or\n",
    "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64  # Linux\n",
    "\n",
    "# Launch notebook from project root\n",
    "uv run jupyter notebook demos/notebooks/spark-iceberg-queries.ipynb\n",
    "```\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "**Local (Notebook) → Docker Services:**\n",
    "- **Notebook runs on host** with local PySpark installation\n",
    "- **Connects to Docker services:**\n",
    "  - Iceberg REST Catalog: `http://localhost:8181`\n",
    "  - MinIO (S3): `http://localhost:9000`\n",
    "  - Spark Cluster: Workers run in Docker (for streaming jobs)\n",
    "\n",
    "**Data Flow:**\n",
    "```\n",
    "WebSocket → Kafka → Bronze (Spark Streaming) → Silver → Gold\n",
    "                        ↓                         ↓       ↓\n",
    "                    (This notebook queries these tables)\n",
    "```\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "1. **Java Required:** PySpark needs Java 11 or 17 installed locally\n",
    "   - Check: `java -version`\n",
    "   - Set `JAVA_HOME` environment variable\n",
    "\n",
    "2. **Network Access:** All services must be accessible from host:\n",
    "   - Spark Web UI: `localhost:8090`\n",
    "   - Kafka UI: `localhost:8080`\n",
    "   - Iceberg REST: `localhost:8181`\n",
    "   - MinIO: `localhost:9000`\n",
    "   - Kafka: `localhost:9092`\n",
    "\n",
    "3. **Memory:** Ensure Docker has at least 8GB RAM allocated for Spark workers\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Issue: \"JAVA_HOME is not set\" or \"Java gateway process exited\"**\n",
    "\n",
    "```bash\n",
    "# Check if Java is installed\n",
    "java -version\n",
    "\n",
    "# If not installed, install Java 17 (see step 1 above)\n",
    "\n",
    "# Set JAVA_HOME\n",
    "# macOS:\n",
    "export JAVA_HOME=$(/usr/libexec/java_home -v 17)\n",
    "\n",
    "# Linux:\n",
    "export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\n",
    "\n",
    "# Verify\n",
    "echo $JAVA_HOME\n",
    "\n",
    "# Add to shell profile for persistence\n",
    "# macOS: ~/.zshrc or ~/.bash_profile\n",
    "# Linux: ~/.bashrc\n",
    "echo 'export JAVA_HOME=$(/usr/libexec/java_home -v 17)' >> ~/.zshrc  # macOS\n",
    "```\n",
    "\n",
    "**Issue: Cannot connect to Iceberg catalog**\n",
    "```bash\n",
    "# Check iceberg-rest is running\n",
    "docker ps | grep iceberg-rest\n",
    "curl http://localhost:8181/v1/config\n",
    "```\n",
    "\n",
    "**Issue: Cannot read from S3/MinIO**\n",
    "```bash\n",
    "# Check MinIO is running\n",
    "docker ps | grep minio\n",
    "# Access MinIO console: http://localhost:9001 (admin/password)\n",
    "```\n",
    "\n",
    "**Issue: Tables don't exist**\n",
    "```bash\n",
    "# Run table creation scripts (see step 3 above)\n",
    "# Verify in MinIO: http://localhost:9001 → warehouse bucket\n",
    "```\n",
    "\n",
    "**Issue: Tables are empty**\n",
    "```bash\n",
    "# Check if producers are running and sending data to Kafka\n",
    "docker exec k2-kafka kafka-console-consumer --bootstrap-server localhost:9092 \\\n",
    "  --topic market.crypto.trades.binance --max-messages 5\n",
    "\n",
    "# Check if Bronze streaming jobs are running\n",
    "docker exec k2-spark-master /opt/spark/bin/spark-submit \\\n",
    "  --master spark://spark-master:7077 \\\n",
    "  --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/spark-sql-kafka-0-10_2.12-3.5.3.jar,/opt/spark/jars-extra/kafka-clients-3.5.1.jar,/opt/spark/jars-extra/commons-pool2-2.11.1.jar,/opt/spark/jars-extra/spark-token-provider-kafka-0-10_2.12-3.5.3.jar \\\n",
    "  /opt/k2/src/k2/spark/jobs/streaming/bronze_binance_ingestion.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Ready to query? Execute the cells below sequentially.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Iceberg Catalog\n",
    "\n",
    "**Note:** First run will download required JARs (~500MB) from Maven Central. This takes 2-3 minutes but is cached for future runs.\n",
    "\n",
    "**Required JARs:**\n",
    "- `iceberg-spark-runtime-3.5_2.12:1.4.0` - Iceberg integration\n",
    "- `hadoop-aws:3.3.4` - S3A filesystem support  \n",
    "- `software.amazon.awssdk:bundle:2.20.18` - AWS SDK v2 for S3/MinIO\n",
    "- `software.amazon.awssdk:url-connection-client:2.20.18` - AWS HTTP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/20 12:57:50 WARN Utils: Your hostname, RS-Dev-01 resolves to a loopback address: 127.0.1.1; using 192.168.4.25 instead (on interface wlo1)\n",
      "26/01/20 12:57:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/rjdscott/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rjdscott/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "software.amazon.awssdk#bundle added as a dependency\n",
      "software.amazon.awssdk#url-connection-client added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-17847712-6f36-4b66-9ce7-d5f6dcb5e114;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/rjdscott/Documents/projects/k2-market-data-platform/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound software.amazon.awssdk#bundle;2.20.18 in central\n",
      "\tfound software.amazon.eventstream#eventstream;1.0.1 in central\n",
      "\tfound software.amazon.awssdk#url-connection-client;2.20.18 in central\n",
      "\tfound software.amazon.awssdk#utils;2.20.18 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound software.amazon.awssdk#annotations;2.20.18 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound software.amazon.awssdk#http-client-spi;2.20.18 in central\n",
      "\tfound software.amazon.awssdk#metrics-spi;2.20.18 in central\n",
      ":: resolution report :: resolve 148ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.4.0 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tsoftware.amazon.awssdk#annotations;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#http-client-spi;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#metrics-spi;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#url-connection-client;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.awssdk#utils;2.20.18 from central in [default]\n",
      "\tsoftware.amazon.eventstream#eventstream;1.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-17847712-6f36-4b66-9ce7-d5f6dcb5e114\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/4ms)\n",
      "26/01/20 12:57:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/20 12:57:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Spark 3.5.0 connected to Iceberg catalog\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with Iceberg catalog\n",
    "# Note: Using .packages to automatically download required JARs\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"K2-Iceberg-Query-Demo\")\n",
    "    # Iceberg catalog configuration\n",
    "    .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.iceberg.type\", \"rest\")\n",
    "    .config(\"spark.sql.catalog.iceberg.uri\", \"http://localhost:8181\")  # iceberg-rest\n",
    "    .config(\"spark.sql.catalog.iceberg.warehouse\", \"s3://warehouse/\")\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.endpoint\", \"http://localhost:9000\")  # minio\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.access-key-id\", \"admin\")\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.secret-access-key\", \"password\")\n",
    "    .config(\"spark.sql.catalog.iceberg.s3.path-style-access\", \"true\")\n",
    "    # S3/MinIO configuration\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"password\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint.region\", \"us-east-1\")\n",
    "    # AWS SDK v2 region configuration (required for MinIO)\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Daws.region=us-east-1\")\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Daws.region=us-east-1\")\n",
    "    # Download required JARs automatically (Maven coordinates)\n",
    "    .config(\"spark.jars.packages\", \n",
    "            \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0,\"\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"software.amazon.awssdk:bundle:2.20.18,\"\n",
    "            \"software.amazon.awssdk:url-connection-client:2.20.18\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"✓ Spark {spark.version} connected to Iceberg catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List All Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n",
      "Iceberg Catalog URI: http://localhost:8181\n",
      "Warehouse: s3://warehouse/\n",
      "S3 Endpoint: http://localhost:9000\n",
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|market_data|\n",
      "+-----------+\n",
      "\n",
      "\n",
      "✓ Successfully connected to Iceberg catalog\n"
     ]
    }
   ],
   "source": [
    "## Verify Connection\n",
    "\n",
    "# Check Spark version and configuration\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Iceberg Catalog URI: {spark.conf.get('spark.sql.catalog.iceberg.uri')}\")\n",
    "print(f\"Warehouse: {spark.conf.get('spark.sql.catalog.iceberg.warehouse')}\")\n",
    "print(f\"S3 Endpoint: {spark.conf.get('spark.hadoop.fs.s3a.endpoint')}\")\n",
    "\n",
    "# Test catalog connection\n",
    "try:\n",
    "    spark.sql(\"SHOW NAMESPACES IN iceberg\").show()\n",
    "    print(\"\\n✓ Successfully connected to Iceberg catalog\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Failed to connect to Iceberg catalog: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------+-----------+\n",
      "|namespace  |tableName            |isTemporary|\n",
      "+-----------+---------------------+-----------+\n",
      "|market_data|silver_kraken_trades |false      |\n",
      "|market_data|silver_binance_trades|false      |\n",
      "|market_data|gold_crypto_trades   |false      |\n",
      "|market_data|silver_dlq_trades    |false      |\n",
      "|market_data|bronze_kraken_trades |false      |\n",
      "|market_data|bronze_binance_trades|false      |\n",
      "+-----------+---------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN iceberg.market_data\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer - Raw Kafka Data\n",
    "\n",
    "Raw Avro bytes from Kafka (per-exchange tables).\n",
    "\n",
    "**What to expect:**\n",
    "- `bronze_binance_trades`: Raw Kafka messages from Binance WebSocket\n",
    "- `bronze_kraken_trades`: Raw Kafka messages from Kraken WebSocket\n",
    "- Fields: `message_key`, `avro_payload` (binary), Kafka metadata, ingestion timestamp\n",
    "- Partitioned by: `ingestion_date`\n",
    "\n",
    "**Note:** If tables are empty, start the Bronze streaming jobs and data producers (see setup instructions above).\n",
    "\n",
    "**Quick check - see if any data exists:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze Table Row Counts:\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bronze_binance_trades: 999,120 records\n",
      "bronze_kraken_trades: 3,977 records\n",
      "\n",
      "✓ Found 1,003,097 total records\n"
     ]
    }
   ],
   "source": [
    "# Quick check - row counts for Bronze tables\n",
    "print(\"Bronze Table Row Counts:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "binance_count = spark.sql(\"SELECT COUNT(*) as count FROM iceberg.market_data.bronze_binance_trades\").collect()[0]['count']\n",
    "print(f\"bronze_binance_trades: {binance_count:,} records\")\n",
    "\n",
    "kraken_count = spark.sql(\"SELECT COUNT(*) as count FROM iceberg.market_data.bronze_kraken_trades\").collect()[0]['count']\n",
    "print(f\"bronze_kraken_trades: {kraken_count:,} records\")\n",
    "\n",
    "if binance_count == 0 and kraken_count == 0:\n",
    "    print(\"\\n⚠️  Bronze tables are empty. Start the Bronze streaming jobs and producers.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Found {binance_count + kraken_count:,} total records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer - Raw Kafka Data\n",
    "\n",
    "Raw Avro bytes from Kafka (per-exchange tables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=================================================>    (204 + 20) / 224]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|ingestion_date|records|kafka_partitions|         first_trade|          last_trade|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|    2026-01-20| 146511|               2|2026-01-20 12:19:...|2026-01-20 12:59:...|\n",
      "|    2026-01-19| 809266|               2|2026-01-19 19:18:...|2026-01-20 10:42:...|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Binance Bronze - row count by partition\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ingestion_date,\n",
    "        COUNT(*) as records,\n",
    "        COUNT(DISTINCT partition) as kafka_partitions,\n",
    "        MIN(kafka_timestamp) as first_trade,\n",
    "        MAX(kafka_timestamp) as last_trade\n",
    "    FROM iceberg.market_data.bronze_binance_trades\n",
    "    GROUP BY ingestion_date\n",
    "    ORDER BY ingestion_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+--------------------+--------------+\n",
      "|           raw_bytes|               topic|partition|offset|     kafka_timestamp| ingestion_timestamp|ingestion_date|\n",
      "+--------------------+--------------------+---------+------+--------------------+--------------------+--------------+\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670149|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670150|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670151|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670152|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670153|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670154|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670155|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670156|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670157|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "|[00 00 00 00 0A 0...|market.crypto.tra...|        0|670158|2026-01-19 23:14:...|2026-01-19 23:14:...|    2026-01-19|\n",
      "+--------------------+--------------------+---------+------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binance Bronze - row count by partition\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM iceberg.market_data.bronze_binance_trades\n",
    "    limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+--------------------+--------------+\n",
      "|           raw_bytes|               topic|partition|offset|     kafka_timestamp| ingestion_timestamp|ingestion_date|\n",
      "+--------------------+--------------------+---------+------+--------------------+--------------------+--------------+\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        0|   722|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        0|   723|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   974|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   975|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   976|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   977|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   978|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   979|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   980|2026-01-19 23:09:...|2026-01-19 23:10:...|    2026-01-19|\n",
      "|[00 00 00 00 0B 8...|market.crypto.tra...|        5|   887|2026-01-19 22:54:...|2026-01-19 22:54:...|    2026-01-19|\n",
      "+--------------------+--------------------+---------+------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binance Bronze - row count by partition\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM iceberg.market_data.bronze_kraken_trades\n",
    "    limit 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|ingestion_date|records|kafka_partitions|         first_trade|          last_trade|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "|    2026-01-19|   1493|               2|2026-01-19 19:18:...|2026-01-19 23:15:...|\n",
      "+--------------+-------+----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kraken Bronze - row count\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        ingestion_date,\n",
    "        COUNT(*) as records,\n",
    "        COUNT(DISTINCT partition) as kafka_partitions,\n",
    "        MIN(kafka_timestamp) as first_trade,\n",
    "        MAX(kafka_timestamp) as last_trade\n",
    "    FROM iceberg.market_data.bronze_kraken_trades\n",
    "    GROUP BY ingestion_date\n",
    "    ORDER BY ingestion_date DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer - Validated Per-Exchange Trades\n",
    "\n",
    "Deserialized and validated V2 trades (per-exchange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------+-------+--------+-----------+----------------+-------------+----------+--------+----+----------------+---------------+-------------------+-----------------+------------------------------------------------------------------------------------------------+-----------------------+--------------------------+---------+\n",
      "|message_id                          |trade_id          |symbol |exchange|asset_class|timestamp       |price        |quantity  |currency|side|trade_conditions|source_sequence|ingestion_timestamp|platform_sequence|vendor_data                                                                                     |validation_timestamp   |bronze_ingestion_timestamp|schema_id|\n",
      "+------------------------------------+------------------+-------+--------+-----------+----------------+-------------+----------+--------+----+----------------+---------------+-------------------+-----------------+------------------------------------------------------------------------------------------------+-----------------------+--------------------------+---------+\n",
      "|71037b73-1a1a-4409-bce0-9c602e5eafe4|BINANCE-3470711564|ETHUSDT|BINANCE |crypto     |1768865300010000|3181.92000000|0.00260000|USDT    |BUY |[]              |NULL           |1768865300136887   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300010,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|8188920c-f1f4-4120-9fac-2b423e938e92|BINANCE-3470711565|ETHUSDT|BINANCE |crypto     |1768865300017000|3181.92000000|0.00050000|USDT    |BUY |[]              |NULL           |1768865300137024   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300017,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|559889ca-e4dd-49ec-88ee-27945fe07b9f|BINANCE-3470711566|ETHUSDT|BINANCE |crypto     |1768865300017000|3181.92000000|0.00310000|USDT    |BUY |[]              |NULL           |1768865300137075   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300017,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|097253ca-b6f7-4d48-8b46-7cf7116e1b66|BINANCE-3470711567|ETHUSDT|BINANCE |crypto     |1768865300017000|3181.92000000|0.00080000|USDT    |BUY |[]              |NULL           |1768865300137119   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300017,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|a536c3de-4787-425b-98fd-c55c6b8ca9f4|BINANCE-3470711568|ETHUSDT|BINANCE |crypto     |1768865300082000|3181.92000000|0.00230000|USDT    |BUY |[]              |NULL           |1768865300140446   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300083,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|a56367ca-b1cd-4e6b-8305-755764bfa41e|BINANCE-3470711569|ETHUSDT|BINANCE |crypto     |1768865300082000|3181.92000000|0.00080000|USDT    |BUY |[]              |NULL           |1768865300140493   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300083,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|023bd9c5-1446-4fac-a2c4-bb8f261c139a|BINANCE-3470711570|ETHUSDT|BINANCE |crypto     |1768865300085000|3181.92000000|0.02500000|USDT    |BUY |[]              |NULL           |1768865300146887   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300086,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|af28e775-0f64-4dbf-908b-6137dad99f48|BINANCE-3470711571|ETHUSDT|BINANCE |crypto     |1768865300089000|3181.92000000|0.02500000|USDT    |BUY |[]              |NULL           |1768865300147069   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300089,\"is_buyer_maker\":false,\"is_best_match\":true}|2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|ad8ffd51-1ef7-4114-966d-d00b3cda72a8|BINANCE-3470711572|ETHUSDT|BINANCE |crypto     |1768865300176000|3181.91000000|0.00310000|USDT    |SELL|[]              |NULL           |1768865300241773   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300177,\"is_buyer_maker\":true,\"is_best_match\":true} |2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "|45125ed9-eb2b-4798-baa6-c9a588ed04ab|BINANCE-3470711573|ETHUSDT|BINANCE |crypto     |1768865300176000|3181.91000000|0.00310000|USDT    |SELL|[]              |NULL           |1768865300242058   |NULL             |{\"event_type\":\"trade\",\"event_time_ms\":1768865300177,\"is_buyer_maker\":true,\"is_best_match\":true} |2026-01-20 10:29:00.114|2026-01-20 10:28:30.009   |10       |\n",
      "+------------------------------------+------------------+-------+--------+-----------+----------------+-------------+----------+--------+----+----------------+---------------+-------------------+-----------------+------------------------------------------------------------------------------------------------+-----------------------+--------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Binance Silver - recent trades\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * --timestamp, symbol, price, quantity, currency\n",
    "    FROM iceberg.market_data.silver_binance_trades\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|symbol |\n",
      "+-------+\n",
      "|BTCUSDT|\n",
      "|ETHUSDT|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT distinct symbol\n",
    "    FROM iceberg.market_data.silver_binance_trades\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+---------------------------------+------+--------+-----------+----------------+--------------+----------+--------+----+----------------+---------------+-------------------+-----------------+--------------------------------------------------------------------+-----------------------+--------------------------+---------+\n",
      "|message_id                          |trade_id                         |symbol|exchange|asset_class|timestamp       |price         |quantity  |currency|side|trade_conditions|source_sequence|ingestion_timestamp|platform_sequence|vendor_data                                                         |validation_timestamp   |bronze_ingestion_timestamp|schema_id|\n",
      "+------------------------------------+---------------------------------+------+--------+-----------+----------------+--------------+----------+--------+----+----------------+---------------+-------------------+-----------------+--------------------------------------------------------------------+-----------------------+--------------------------+---------+\n",
      "|413f9c83-1c63-4371-8731-463d637ea58c|KRAKEN-1768829016.074058-96b7b2eb|ETHUSD|KRAKEN  |crypto     |1768829016074058|3212.50000000 |0.03059417|USD     |BUY |[limit]         |NULL           |1768829016245167   |NULL             |{\"channel_id\":13959169,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"ETH/USD\"} |2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|2b39bf94-e0d5-464c-8075-8273e231cce4|KRAKEN-1768829019.009978-ba62d476|ETHUSD|KRAKEN  |crypto     |1768829019009978|3212.19000000 |0.06012801|USD     |SELL|[limit]         |NULL           |1768829019145477   |NULL             |{\"channel_id\":13959169,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"ETH/USD\"} |2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|1d2097bf-73bc-4b72-aaf9-850699c8b80c|KRAKEN-1768829020.217600-7b2ed2db|ETHUSD|KRAKEN  |crypto     |1768829020217600|3211.97000000 |0.06012603|USD     |SELL|[limit]         |NULL           |1768829020352547   |NULL             |{\"channel_id\":13959169,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"ETH/USD\"} |2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|d520d2c2-073a-4dd2-a742-7ee308573d4f|KRAKEN-1768829034.511952-03d81ec1|ETHUSD|KRAKEN  |crypto     |1768829034511952|3212.24000000 |0.00901775|USD     |BUY |[market]        |NULL           |1768829034644210   |NULL             |{\"channel_id\":13959169,\"order_type\":\"m\",\"misc\":\"\",\"pair\":\"ETH/USD\"} |2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|71b93a9e-a7a7-4204-a2b8-661057f89c3a|KRAKEN-1768829012.073099-bee95778|BTCUSD|KRAKEN  |crypto     |1768829012073099|92855.60000000|0.00200000|USD     |SELL|[limit]         |NULL           |1768829012292159   |NULL             |{\"channel_id\":119930881,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"XBT/USD\"}|2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|667c86f0-1dc8-4d7e-a835-c3fb83cadb2c|KRAKEN-1768829015.689398-53c695ae|BTCUSD|KRAKEN  |crypto     |1768829015689398|92839.60000000|0.00138965|USD     |SELL|[limit]         |NULL           |1768829015824251   |NULL             |{\"channel_id\":119930881,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"XBT/USD\"}|2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|1b7b145e-333f-42d4-ac66-c523753ab6a5|KRAKEN-1768829016.839557-9537995f|BTCUSD|KRAKEN  |crypto     |1768829016839557|92839.70000000|0.00053856|USD     |BUY |[limit]         |NULL           |1768829017003682   |NULL             |{\"channel_id\":119930881,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"XBT/USD\"}|2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|67e81d47-ca44-4d14-b59b-399f6e188748|KRAKEN-1768829019.404643-4c0faa43|BTCUSD|KRAKEN  |crypto     |1768829019404643|92839.60000000|0.00161035|USD     |SELL|[market]        |NULL           |1768829019562974   |NULL             |{\"channel_id\":119930881,\"order_type\":\"m\",\"misc\":\"\",\"pair\":\"XBT/USD\"}|2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|9b9c7402-38b1-4394-8dfe-f55dd74819c5|KRAKEN-1768829021.862879-247f171a|BTCUSD|KRAKEN  |crypto     |1768829021862879|92835.90000000|0.00053858|USD     |SELL|[market]        |NULL           |1768829022017919   |NULL             |{\"channel_id\":119930881,\"order_type\":\"m\",\"misc\":\"\",\"pair\":\"XBT/USD\"}|2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "|ed42a17b-1574-4105-9c60-cf5fd799731d|KRAKEN-1768829021.893147-5d8d7021|BTCUSD|KRAKEN  |crypto     |1768829021893147|92832.40000000|0.00015831|USD     |SELL|[limit]         |NULL           |1768829022025462   |NULL             |{\"channel_id\":119930881,\"order_type\":\"l\",\"misc\":\"\",\"pair\":\"XBT/USD\"}|2026-01-20 00:24:30.062|2026-01-20 00:24:00.005   |11       |\n",
      "+------------------------------------+---------------------------------+------+--------+-----------+----------------+--------------+----------+--------+----+----------------+---------------+-------------------+-----------------+--------------------------------------------------------------------+-----------------------+--------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kraken Silver - recent trades\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * --timestamp, symbol, price, quantity, currency\n",
    "    FROM iceberg.market_data.silver_kraken_trades\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|exchange|count(exchange)|\n",
      "+--------+---------------+\n",
      "|KRAKEN  |8187           |\n",
      "|BINANCE |1860308        |\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Gold Trades\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    distinct exchange, count(exchange) --* --timestamp, symbol, price, quantity, currency\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    GROUP BY EXCHANGE\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binance Silver - price aggregation (1-minute bars)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        DATE_TRUNC('minute', CAST(timestamp / 1000000 AS TIMESTAMP)) as minute,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(AVG(price), 2) as avg_price,\n",
    "        ROUND(MIN(price), 2) as low,\n",
    "        ROUND(MAX(price), 2) as high,\n",
    "        ROUND(SUM(quantity), 8) as volume\n",
    "    FROM iceberg.market_data.silver_binance_trades\n",
    "    WHERE symbol = 'BTCUSDT'\n",
    "      AND exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY symbol, minute\n",
    "    ORDER BY minute DESC\n",
    "    LIMIT 20\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kraken Silver - recent trades\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        CAST(timestamp / 1000000 AS TIMESTAMP) as trade_time,\n",
    "        price,\n",
    "        quantity,\n",
    "        side,\n",
    "        vendor_data\n",
    "    FROM iceberg.market_data.silver_kraken_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gold Layer - Unified Multi-Exchange Analytics\n",
    "\n",
    "Combined trades from all exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold - trades by exchange (last 24h)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exchange,\n",
    "        COUNT(*) as trades,\n",
    "        COUNT(DISTINCT symbol) as symbols,\n",
    "        ROUND(SUM(price * quantity), 2) as total_value_usd\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY exchange\n",
    "    ORDER BY trades DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold - cross-exchange price comparison (BTC/USDT)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exchange,\n",
    "        symbol,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(AVG(price), 2) as avg_price,\n",
    "        ROUND(MIN(price), 2) as min_price,\n",
    "        ROUND(MAX(price), 2) as max_price,\n",
    "        ROUND(STDDEV(price), 2) as price_stddev\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    WHERE symbol IN ('BTCUSDT', 'BTCUSD')\n",
    "      AND exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY exchange, symbol\n",
    "    ORDER BY exchange\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gold - hourly trade volume (all exchanges)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        exchange_date,\n",
    "        exchange_hour,\n",
    "        exchange,\n",
    "        COUNT(*) as trades,\n",
    "        ROUND(SUM(quantity), 8) as total_quantity\n",
    "    FROM iceberg.market_data.gold_crypto_trades\n",
    "    WHERE exchange_date >= CURRENT_DATE() - INTERVAL 1 DAY\n",
    "    GROUP BY exchange_date, exchange_hour, exchange\n",
    "    ORDER BY exchange_date DESC, exchange_hour DESC\n",
    "    LIMIT 50\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Schema Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Bronze table schema\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.market_data.bronze_binance_trades\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Silver table schema\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.market_data.silver_binance_trades\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe Gold table schema\n",
    "spark.sql(\"DESCRIBE EXTENDED iceberg.market_data.gold_crypto_trades\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table History & Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table history (Iceberg time-travel)\n",
    "spark.sql(\"SELECT * FROM iceberg.market_data.bronze_binance_trades.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show snapshots\n",
    "spark.sql(\"SELECT * FROM iceberg.market_data.bronze_binance_trades.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"✓ Spark session closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
