# Real-time Market Data Platform - Infrastructure Stack
#
# Architecture: Event-driven streaming platform with lakehouse storage
# - Kafka: Message broker for real-time market tick ingestion
# - Schema Registry: Centralized schema management for data governance
# - MinIO: S3-compatible object storage for Iceberg data lake
# - PostgreSQL: Metadata catalog for Iceberg tables
# - Iceberg REST Catalog: Table management and ACID guarantees
# - Prometheus + Grafana: Observability stack
#
# Scaling Considerations:
# - Kafka partitions can scale horizontally (add brokers to cluster)
# - MinIO supports distributed mode for petabyte-scale storage
# - Iceberg enables efficient partition pruning for large datasets
# - Resource limits prevent runaway consumption in local dev

services:
  # ============================================================================
  # KAFKA - Event Streaming Platform (KRaft mode, no Zookeeper)
  # ============================================================================
  # Why KRaft? Modern Kafka architecture removes Zookeeper dependency,
  # reducing operational complexity and improving metadata scalability.
  # In production: Would run 3+ brokers for fault tolerance
  kafka:
    image: confluentinc/cp-kafka:8.1.1
    container_name: k2-kafka
    hostname: kafka
    ports:
      - "9092:9092"       # External clients
      - "9093:9093"       # Internal broker communication
      - "9997:9997"       # JMX metrics for monitoring
    environment:
      # KRaft Configuration (Kafka Raft consensus)
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9093'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'

      # Listener Configuration
      # PLAINTEXT_HOST: External access for producers/consumers
      # PLAINTEXT: Internal Docker network communication
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092'
      KAFKA_LISTENERS: 'PLAINTEXT://kafka:29092,CONTROLLER://kafka:9093,PLAINTEXT_HOST://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'PLAINTEXT'

      # Cluster Configuration
      CLUSTER_ID: 'k2-market-data-cluster'

      # Performance Tuning for Market Data
      # Market ticks are small, high-frequency messages - optimize for throughput
      KAFKA_NUM_PARTITIONS: 6                    # Default partitions for parallelism
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1        # Single broker in dev, 3+ in prod
      KAFKA_MIN_INSYNC_REPLICAS: 1               # Write durability guarantee
      KAFKA_LOG_RETENTION_HOURS: 168             # 7 days retention
      KAFKA_LOG_SEGMENT_BYTES: 1073741824        # 1GB segments for efficient compaction
      KAFKA_COMPRESSION_TYPE: 'lz4'              # Fast compression for market data

      # Resource Management
      KAFKA_HEAP_OPTS: "-Xmx2G -Xms2G"           # Fixed heap prevents GC pauses

      # Monitoring
      KAFKA_JMX_PORT: 9997
      KAFKA_JMX_HOSTNAME: localhost

      # Consumer Group Coordinator (CRITICAL - Required for Schema Registry)
      KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 30000  # Wait 30s for members to join
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080          # 7 days

      # Transaction Coordinator
      KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

      # Broker Session Management (KRaft-specific)
      KAFKA_BROKER_SESSION_TIMEOUT_MS: 10000
      KAFKA_BROKER_HEARTBEAT_INTERVAL_MS: 3000

      # Protocol Version (ensures consumer group compatibility)
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: 3.6

      # Socket Settings (helps with connection stability)
      KAFKA_SOCKET_KEEPALIVE_ENABLED: "true"
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400

      # Log directories
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
    volumes:
      - kafka-data:/var/lib/kafka/data
      - ./config/kafka:/etc/kafka/conf
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
        reservations:
          cpus: '1.0'
          memory: 2G

  # ============================================================================
  # SCHEMA REGISTRY - Schema Evolution & Data Governance (Multi-Node)
  # ============================================================================
  # Central schema management ensures:
  # - Producer/consumer compatibility
  # - Schema evolution without breaking changes
  # - Data governance and documentation
  # Multi-node deployment: 2 instances for leader election reliability
  schema-registry-1:
    image: confluentinc/cp-schema-registry:8.1.1
    container_name: k2-schema-registry-1
    hostname: schema-registry-1
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'

      # Schema Compatibility
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: 'BACKWARD'

      # Performance
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xmx512M -Xms512M"

      # Multi-node leader election configuration
      SCHEMA_REGISTRY_KAFKASTORE_GROUP_ID: "schema-registry-cluster"
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: "true"

      # Consumer group coordination timeouts
      SCHEMA_REGISTRY_KAFKASTORE_INIT_TIMEOUT_MS: 90000
      SCHEMA_REGISTRY_REQUEST_TIMEOUT_MS: 40000
      SCHEMA_REGISTRY_SESSION_TIMEOUT_MS: 30000
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8081' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 768M

  schema-registry-2:
    image: confluentinc/cp-schema-registry:8.1.1
    container_name: k2-schema-registry-2
    hostname: schema-registry-2
    ports:
      - "8082:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-2
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'

      # Schema Compatibility
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: 'BACKWARD'

      # Performance
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xmx512M -Xms512M"

      # Multi-node leader election configuration (same group.id)
      SCHEMA_REGISTRY_KAFKASTORE_GROUP_ID: "schema-registry-cluster"
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: "true"

      # Consumer group coordination timeouts
      SCHEMA_REGISTRY_KAFKASTORE_INIT_TIMEOUT_MS: 90000
      SCHEMA_REGISTRY_REQUEST_TIMEOUT_MS: 40000
      SCHEMA_REGISTRY_SESSION_TIMEOUT_MS: 30000
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8081' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 768M

  # ============================================================================
  # MINIO - S3-Compatible Object Storage for Iceberg
  # ============================================================================
  # Why MinIO?
  # - S3 API compatibility (easy cloud migration)
  # - High performance for analytical workloads
  # - Distributed mode supports petabyte-scale
  # - Perfect for Iceberg parquet files
  minio:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: k2-minio
    hostname: minio
    ports:
      - "9000:9000"       # API
      - "9001:9001"       # Console UI
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-password}
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_UPDATE: off
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  # MinIO Client - Initialize buckets
  # Creates necessary buckets on startup for Iceberg warehouse
  minio-init:
    image: minio/mc:RELEASE.2024-01-16T16-06-34Z
    container_name: k2-minio-init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-password}
    entrypoint: >
      /bin/sh -c "
      mc alias set k2minio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD};
      mc mb --ignore-existing k2minio/warehouse;
      mc mb --ignore-existing k2minio/data;
      mc mb --ignore-existing k2minio/backups;
      mc policy set download k2minio/warehouse;
      echo 'MinIO initialized successfully';
      "
    networks:
      - k2-network

  # ============================================================================
  # POSTGRESQL - Iceberg Catalog Metadata Store
  # ============================================================================
  # Stores Iceberg table metadata:
  # - Table schemas and partitioning schemes
  # - Snapshot history for time-travel queries
  # - File manifests and statistics
  # Why Postgres? ACID guarantees critical for catalog consistency
  postgres:
    image: postgres:16-alpine
    container_name: k2-postgres
    hostname: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-iceberg}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-iceberg}
      POSTGRES_DB: ${POSTGRES_DB:-iceberg_catalog}
      # Performance tuning for metadata workload
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_MAX_CONNECTIONS: 200
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/iceberg/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-iceberg}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # ============================================================================
  # ICEBERG REST CATALOG - Table Management & ACID Operations
  # ============================================================================
  # Provides:
  # - ACID transactions across multiple files
  # - Schema evolution
  # - Time-travel queries
  # - Partition evolution without data rewrites
  # Critical for financial data: Ensures consistency and auditability
  iceberg-rest:
    image: tabulario/iceberg-rest:0.8.0  # apache/iceberg-rest-fixture:1.10.1 missing PostgreSQL JDBC driver - needs investigation
    container_name: k2-iceberg-rest
    hostname: iceberg-rest
    ports:
      - "8181:8181"
    environment:
      CATALOG_WAREHOUSE: s3://warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_ACCESS__KEY__ID: ${MINIO_ROOT_USER:-admin}
      CATALOG_S3_SECRET__ACCESS__KEY: ${MINIO_ROOT_PASSWORD:-password}
      CATALOG_S3_PATH__STYLE__ACCESS: true
      AWS_REGION: us-east-1
      CATALOG_URI: jdbc:postgresql://postgres:5432/${POSTGRES_DB:-iceberg_catalog}
      CATALOG_JDBC_USER: ${POSTGRES_USER:-iceberg}
      CATALOG_JDBC_PASSWORD: ${POSTGRES_PASSWORD:-iceberg}
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8181' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # ============================================================================
  # PROMETHEUS - Metrics Collection
  # ============================================================================
  # Scrapes metrics from:
  # - Kafka (JMX exporter)
  # - MinIO (built-in metrics)
  # - Application services (custom metrics)
  # Essential for production: Latency, throughput, error rates
  prometheus:
    image: prom/prometheus:v3.9.1  # Upgraded from v2.49.1 (2026-01-10)
    container_name: k2-prometheus
    hostname: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/prometheus/rules:/etc/prometheus/rules
      - prometheus-data:/prometheus
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # GRAFANA - Observability Dashboard
  # ============================================================================
  # Pre-configured dashboards for:
  # - Kafka lag and throughput
  # - Storage I/O patterns
  # - Query latency distributions
  # - System health metrics
  grafana:
    image: grafana/grafana:12.3.1  # Upgraded from 10.2.3 (skipped v11.x) - 2026-01-10
    container_name: k2-grafana
    hostname: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: false
      # GF_INSTALL_PLUGINS removed - grafana-piechart-panel deprecated in v11+, use native pie chart
    volumes:
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # K2 QUERY API - REST API Service for Market Data Queries
  # ============================================================================
  # Provides RESTful interface for:
  # - Market data queries (trades, quotes, summaries)
  # - Hybrid queries (Kafka + Iceberg integration)
  # - Health monitoring and metrics
  # - OpenAPI documentation (/docs, /redoc)
  # Production-ready: authentication, rate limiting, correlation IDs
  k2-query-api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # Install API server dependencies
        INSTALL_API: "true"
    container_name: k2-query-api
    hostname: k2-query-api
    ports:
      - "8000:8000"       # REST API
      - "9094:9094"       # Prometheus metrics (alternative to 9091 for API)
    environment:
      # FastAPI Configuration
      K2_API_KEY: ${K2_API_KEY:-k2-dev-api-key-2026}
      K2_API_RATE_LIMIT: ${K2_API_RATE_LIMIT:-100}
      K2_LOG_LEVEL: ${K2_API_LOG_LEVEL:-INFO}
      K2_LOG_FORMAT: json
      
      # Kafka Configuration
      K2_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      K2_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      K2_KAFKA_CONSUMER_GROUP_PREFIX: k2-api
      
      # Iceberg Configuration
      K2_ICEBERG_CATALOG_URI: jdbc:postgresql://postgres:5432/${POSTGRES_DB:-iceberg_catalog}
      K2_ICEBERG_CATALOG_WAREHOUSE: s3://warehouse/
      K2_ICEBERG_S3_ENDPOINT: http://minio:9000
      K2_ICEBERG_S3_ACCESS_KEY: ${MINIO_ROOT_USER:-admin}
      K2_ICEBERG_S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-password}
      K2_ICEBERG_S3_PATH_STYLE_ACCESS: "true"
      
      # Performance Configuration
      K2_QUERY_MEMORY_LIMIT: 1GB
      K2_QUERY_TIMEOUT_S: 30
      K2_HYBRID_QUERY_MAX_MESSAGES: 1000
      
      # Metrics Configuration
      K2_METRICS_ENABLED: "true"
      K2_METRICS_PORT: 9094
    command: >
      python -m uvicorn k2.api.main:app 
      --host 0.0.0.0 
      --port 8000 
      --workers 1 
      --access-log 
      --log-level info
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    restart: unless-stopped

  # ============================================================================
  # KAFKA UI - Development & Debugging Tool
  # ============================================================================
  # Provides web interface for:
  # - Topic inspection
  # - Consumer group monitoring
  # - Message browsing
  # - Schema Registry integration
  kafka-ui:
    image: kafbat/kafka-ui:v1.4.2  # Migrated from provectus (abandoned) to kafbat (active fork) - 2026-01-10
    container_name: k2-kafka-ui
    hostname: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: k2-market-data
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry-1:8081
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
      DYNAMIC_CONFIG_ENABLED: 'true'
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # BINANCE STREAM - Real-time Cryptocurrency Market Data Ingestion
  # ============================================================================
  # Streams live trades from Binance WebSocket API
  # - Connects to Binance public trade streams
  # - Converts Binance format to v2 schema
  # - Publishes to Kafka market.crypto.trades topic
  # - Production-grade resilience: circuit breakers, health checks, failover
  binance-stream:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: k2-binance-stream
    hostname: binance-stream
    environment:
      # Kafka Configuration
      K2_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      K2_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      K2_KAFKA_BATCH_SIZE: 16384
      K2_KAFKA_LINGER_MS: 10
      K2_KAFKA_COMPRESSION_TYPE: lz4
      K2_KAFKA_RETRIES: 3
      K2_KAFKA_REQUEST_TIMEOUT_MS: 30000
      K2_KAFKA_MAX_IN_FLIGHT_REQUESTS: 5
      K2_KAFKA_ACKS: 1
      K2_KAFKA_IDEMPOTENCE_ENABLED: "true"

      # Binance Configuration
      K2_BINANCE_ENABLED: "true"
      K2_BINANCE_SYMBOLS: '["BTCUSDT", "ETHUSDT", "BNBUSDT"]'
      K2_BINANCE_WEBSOCKET_URL: wss://stream.binance.com:9443/stream
      K2_BINANCE_FAILOVER_URLS: '["wss://stream.binance.us:9443/stream"]'

      # Resilience Configuration
      K2_BINANCE_RECONNECT_DELAY: 5
      K2_BINANCE_MAX_RECONNECT_ATTEMPTS: 10
      K2_BINANCE_HEALTH_CHECK_INTERVAL: 30
      K2_BINANCE_HEALTH_CHECK_TIMEOUT: 30

      # Metrics Configuration
      K2_METRICS_ENABLED: "true"
      K2_METRICS_PORT: 9091

      # Logging Configuration
      K2_LOG_LEVEL: INFO
      K2_LOG_FORMAT: json
    command: python scripts/binance_stream.py
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9091' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

# ==============================================================================
# NETWORKS
# ==============================================================================
# Isolated network for service communication
# In production: Would use overlay networks for multi-host deployment
networks:
  k2-network:
    driver: bridge
    name: k2-network

# ==============================================================================
# VOLUMES - Persistent Data Storage
# ==============================================================================
# Named volumes for data persistence across container restarts
# In production: Would use remote storage (EBS, NFS, etc.)
volumes:
  kafka-data:
    driver: local
  minio-data:
    driver: local
  postgres-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
