# Real-time Market Data Platform - Infrastructure Stack
#
# Architecture: Event-driven streaming platform with lakehouse storage
# - Kafka: Message broker for real-time market tick ingestion
# - Schema Registry: Centralized schema management for data governance
# - MinIO: S3-compatible object storage for Iceberg data lake
# - PostgreSQL: Metadata catalog for Iceberg tables
# - Iceberg REST Catalog: Table management and ACID guarantees
# - Prometheus + Grafana: Observability stack
#
# Scaling Considerations:
# - Kafka partitions can scale horizontally (add brokers to cluster)
# - MinIO supports distributed mode for petabyte-scale storage
# - Iceberg enables efficient partition pruning for large datasets
# - Resource limits prevent runaway consumption in local dev

services:
  # ============================================================================
  # KAFKA - Event Streaming Platform (KRaft mode, no Zookeeper)
  # ============================================================================
  # Why KRaft? Modern Kafka architecture removes Zookeeper dependency,
  # reducing operational complexity and improving metadata scalability.
  # In production: Would run 3+ brokers for fault tolerance
  kafka:
    image: confluentinc/cp-kafka:8.1.1
    container_name: k2-kafka
    hostname: kafka
    ports:
      - "9092:9092"       # External clients
      - "9093:9093"       # Internal broker communication
      - "9997:9997"       # JMX metrics for monitoring
    environment:
      # KRaft Configuration (Kafka Raft consensus)
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: 'broker,controller'
      KAFKA_CONTROLLER_QUORUM_VOTERS: '1@kafka:9093'
      KAFKA_CONTROLLER_LISTENER_NAMES: 'CONTROLLER'

      # Listener Configuration - Professional Multi-Listener Setup
      # INTERNAL: Container-to-container communication (Spark, Schema Registry, producers)
      # EXTERNAL: Host-based access for debugging and monitoring tools
      # CONTROLLER: KRaft consensus protocol (controller quorum)
      #
      # Pattern follows Netflix/Uber/LinkedIn production standards:
      # - Separate listeners for internal vs external clients
      # - Client connects to one listener, receives that listener in metadata
      # - No metadata redirection issues between container and host networks
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 'CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT'
      KAFKA_ADVERTISED_LISTENERS: 'INTERNAL://kafka:29092,EXTERNAL://localhost:9092'
      KAFKA_LISTENERS: 'INTERNAL://0.0.0.0:29092,CONTROLLER://kafka:9093,EXTERNAL://0.0.0.0:9092'
      KAFKA_INTER_BROKER_LISTENER_NAME: 'INTERNAL'

      # Cluster Configuration
      CLUSTER_ID: 'k2-market-data-cluster'

      # Performance Tuning for Market Data
      # Market ticks are small, high-frequency messages - optimize for throughput
      KAFKA_NUM_PARTITIONS: 6                    # Default partitions for parallelism
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1        # Single broker in dev, 3+ in prod
      KAFKA_MIN_INSYNC_REPLICAS: 1               # Write durability guarantee
      KAFKA_LOG_RETENTION_HOURS: 168             # 7 days retention
      KAFKA_LOG_SEGMENT_BYTES: 1073741824        # 1GB segments for efficient compaction
      KAFKA_COMPRESSION_TYPE: 'lz4'              # Fast compression for market data

      # Resource Management
      KAFKA_HEAP_OPTS: "-Xmx1G -Xms1G"           # Fixed heap prevents GC pauses

      # Monitoring
      KAFKA_JMX_PORT: 9997
      KAFKA_JMX_HOSTNAME: localhost

      # Consumer Group Coordinator (CRITICAL - Required for Schema Registry)
      KAFKA_OFFSETS_TOPIC_NUM_PARTITIONS: 3
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 30000  # Wait 30s for members to join
      KAFKA_OFFSETS_RETENTION_MINUTES: 10080          # 7 days

      # Transaction Coordinator
      KAFKA_TRANSACTION_STATE_LOG_NUM_PARTITIONS: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

      # Broker Session Management (KRaft-specific)
      KAFKA_BROKER_SESSION_TIMEOUT_MS: 10000
      KAFKA_BROKER_HEARTBEAT_INTERVAL_MS: 3000

      # Protocol Version (ensures consumer group compatibility)
      KAFKA_INTER_BROKER_PROTOCOL_VERSION: 3.6

      # Socket Settings (helps with connection stability)
      KAFKA_SOCKET_KEEPALIVE_ENABLED: "true"
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400

      # Log directories
      KAFKA_LOG_DIRS: '/var/lib/kafka/data'
    volumes:
      - kafka-data:/var/lib/kafka/data
      - ./config/kafka:/etc/kafka/conf
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "kafka-broker-api-versions --bootstrap-server localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2G
        reservations:
          cpus: '0.75'
          memory: 1G

  # ============================================================================
  # SCHEMA REGISTRY - Schema Evolution & Data Governance (Multi-Node)
  # ============================================================================
  # Central schema management ensures:
  # - Producer/consumer compatibility
  # - Schema evolution without breaking changes
  # - Data governance and documentation
  # Multi-node deployment: 2 instances for leader election reliability
  schema-registry-1:
    image: confluentinc/cp-schema-registry:8.1.1
    container_name: k2-schema-registry-1
    hostname: schema-registry-1
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry-1
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'

      # Schema Compatibility
      SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: 'BACKWARD'

      # Performance
      SCHEMA_REGISTRY_HEAP_OPTS: "-Xmx512M -Xms512M"

      # Multi-node leader election configuration
      SCHEMA_REGISTRY_KAFKASTORE_GROUP_ID: "schema-registry-cluster"
      SCHEMA_REGISTRY_LEADER_ELIGIBILITY: "true"

      # Consumer group coordination timeouts
      SCHEMA_REGISTRY_KAFKASTORE_INIT_TIMEOUT_MS: 90000
      SCHEMA_REGISTRY_REQUEST_TIMEOUT_MS: 40000
      SCHEMA_REGISTRY_SESSION_TIMEOUT_MS: 30000

      # Topic creation settings (ensures _schemas is compacted)
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: "_schemas"
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_CONFIG: "cleanup.policy=compact,segment.ms=3600000,min.cleanable.dirty.ratio=0.01"
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8081' || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 768M

  # ============================================================================
  # SCHEMA REGISTRY 2 - DISABLED FOR RESOURCE OPTIMIZATION
  # ============================================================================
  # Commented out to reduce resource usage (0.5 CPU, 768MB RAM saved)
  # Single Schema Registry instance is sufficient for demo/dev environments
  # Multi-node setup was designed for HA in production
  # To re-enable: uncomment this entire block
  # ============================================================================
  # schema-registry-2:
  #   image: confluentinc/cp-schema-registry:8.1.1
  #   container_name: k2-schema-registry-2
  #   hostname: schema-registry-2
  #   ports:
  #     - "8082:8081"
  #   environment:
  #     SCHEMA_REGISTRY_HOST_NAME: schema-registry-2
  #     SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
  #     SCHEMA_REGISTRY_LISTENERS: 'http://0.0.0.0:8081'
  #
  #     # Schema Compatibility
  #     SCHEMA_REGISTRY_SCHEMA_COMPATIBILITY_LEVEL: 'BACKWARD'
  #
  #     # Performance
  #     SCHEMA_REGISTRY_HEAP_OPTS: "-Xmx512M -Xms512M"
  #
  #     # Multi-node leader election configuration (same group.id)
  #     SCHEMA_REGISTRY_KAFKASTORE_GROUP_ID: "schema-registry-cluster"
  #     SCHEMA_REGISTRY_LEADER_ELIGIBILITY: "true"
  #
  #     # Consumer group coordination timeouts
  #     SCHEMA_REGISTRY_KAFKASTORE_INIT_TIMEOUT_MS: 90000
  #     SCHEMA_REGISTRY_REQUEST_TIMEOUT_MS: 40000
  #     SCHEMA_REGISTRY_SESSION_TIMEOUT_MS: 30000
  #
  #     # Topic creation settings (ensures _schemas is compacted)
  #     SCHEMA_REGISTRY_KAFKASTORE_TOPIC: "_schemas"
  #     SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 1
  #     SCHEMA_REGISTRY_KAFKASTORE_TOPIC_CONFIG: "cleanup.policy=compact,segment.ms=3600000,min.cleanable.dirty.ratio=0.01"
  #   depends_on:
  #     kafka:
  #       condition: service_healthy
  #   networks:
  #     - k2-network
  #   healthcheck:
  #     test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8081' || exit 1"]
  #     interval: 5s
  #     timeout: 5s
  #     retries: 10
  #     start_period: 30s
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: '0.5'
  #         memory: 768M

  # ============================================================================
  # MINIO - S3-Compatible Object Storage for Iceberg
  # ============================================================================
  # Why MinIO?
  # - S3 API compatibility (easy cloud migration)
  # - High performance for analytical workloads
  # - Distributed mode supports petabyte-scale
  # - Perfect for Iceberg parquet files
  minio:
    image: minio/minio:RELEASE.2024-01-16T16-07-38Z
    container_name: k2-minio
    hostname: minio
    ports:
      - "9000:9000"       # API
      - "9001:9001"       # Console UI
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-password}
      MINIO_PROMETHEUS_AUTH_TYPE: public
      MINIO_UPDATE: off
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  # MinIO Client - Initialize buckets
  # Creates necessary buckets on startup for Iceberg warehouse
  minio-init:
    image: minio/mc:RELEASE.2024-01-16T16-06-34Z
    container_name: k2-minio-init
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-password}
    entrypoint: >
      /bin/sh -c "
      mc alias set k2minio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD};
      mc mb --ignore-existing k2minio/warehouse;
      mc mb --ignore-existing k2minio/data;
      mc mb --ignore-existing k2minio/backups;
      mc mb --ignore-existing k2minio/flink;
      mc policy set download k2minio/warehouse;
      echo 'MinIO initialized successfully';
      "
    networks:
      - k2-network

  # ============================================================================
  # POSTGRESQL - Iceberg Catalog Metadata Store
  # ============================================================================
  # Stores Iceberg table metadata:
  # - Table schemas and partitioning schemes
  # - Snapshot history for time-travel queries
  # - File manifests and statistics
  # Why Postgres? ACID guarantees critical for catalog consistency
  postgres:
    image: postgres:16-alpine
    container_name: k2-postgres
    hostname: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-iceberg}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-iceberg}
      POSTGRES_DB: ${POSTGRES_DB:-iceberg_catalog}
      # Performance tuning for metadata workload
      POSTGRES_SHARED_BUFFERS: 256MB
      POSTGRES_MAX_CONNECTIONS: 200
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./config/iceberg/init.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-iceberg}"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # ============================================================================
  # ICEBERG REST CATALOG - Table Management & ACID Operations
  # ============================================================================
  # Provides:
  # - ACID transactions across multiple files
  # - Schema evolution
  # - Time-travel queries
  # - Partition evolution without data rewrites
  # Critical for financial data: Ensures consistency and auditability
  iceberg-rest:
    image: tabulario/iceberg-rest:0.8.0  # apache/iceberg-rest-fixture:1.10.1 missing PostgreSQL JDBC driver - needs investigation
    container_name: k2-iceberg-rest
    hostname: iceberg-rest
    ports:
      - "8181:8181"
    environment:
      CATALOG_WAREHOUSE: s3a://warehouse/
      CATALOG_IO__IMPL: org.apache.iceberg.aws.s3.S3FileIO
      CATALOG_S3_ENDPOINT: http://minio:9000
      CATALOG_S3_ACCESS__KEY__ID: ${MINIO_ROOT_USER:-admin}
      CATALOG_S3_SECRET__ACCESS__KEY: ${MINIO_ROOT_PASSWORD:-password}
      CATALOG_S3_PATH__STYLE__ACCESS: true
      AWS_REGION: us-east-1
      CATALOG_URI: jdbc:postgresql://postgres:5432/${POSTGRES_DB:-iceberg_catalog}
      CATALOG_JDBC_USER: ${POSTGRES_USER:-iceberg}
      CATALOG_JDBC_PASSWORD: ${POSTGRES_PASSWORD:-iceberg}
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/8181' || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # ============================================================================
  # PROMETHEUS - Metrics Collection
  # ============================================================================
  # Scrapes metrics from:
  # - Kafka (JMX exporter)
  # - MinIO (built-in metrics)
  # - Application services (custom metrics)
  # Essential for production: Latency, throughput, error rates
  prometheus:
    image: prom/prometheus:v3.9.1  # Upgraded from v2.49.1 (2026-01-10)
    container_name: k2-prometheus
    hostname: prometheus
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./config/prometheus/rules:/etc/prometheus/rules
      - prometheus-data:/prometheus
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # GRAFANA - Observability Dashboard
  # ============================================================================
  # Pre-configured dashboards for:
  # - Kafka lag and throughput
  # - Storage I/O patterns
  # - Query latency distributions
  # - System health metrics
  grafana:
    image: grafana/grafana:12.3.1  # Upgraded from 10.2.3 (skipped v11.x) - 2026-01-10
    container_name: k2-grafana
    hostname: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GRAFANA_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_USERS_ALLOW_SIGN_UP: false
      # GF_INSTALL_PLUGINS removed - grafana-piechart-panel deprecated in v11+, use native pie chart
    volumes:
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # K2 QUERY API - REST API Service for Market Data Queries
  # ============================================================================
  # Provides RESTful interface for:
  # - Market data queries (trades, quotes, summaries)
  # - Hybrid queries (Kafka + Iceberg integration)
  # - Health monitoring and metrics
  # - OpenAPI documentation (/docs, /redoc)
  # Production-ready: authentication, rate limiting, correlation IDs
  k2-query-api:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        # Install API server dependencies
        INSTALL_API: "true"
        # Force API dependencies installation
        FORCE_INSTALL_API: "true"
    container_name: k2-query-api
    hostname: k2-query-api
    ports:
      - "8000:8000"       # REST API
      - "9094:9094"       # Prometheus metrics (alternative to 9091 for API)
    environment:
      # FastAPI Configuration
      K2_API_KEY: ${K2_API_KEY:-k2-dev-api-key-2026}
      K2_API_RATE_LIMIT: ${K2_API_RATE_LIMIT:-100}
      K2_LOG_LEVEL: ${K2_API_LOG_LEVEL:-INFO}
      K2_LOG_FORMAT: json
      
      # Kafka Configuration
      K2_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      K2_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      K2_KAFKA_CONSUMER_GROUP_PREFIX: k2-api
      
      # Iceberg Configuration
      K2_ICEBERG_CATALOG_URI: http://iceberg-rest:8181
      K2_ICEBERG_CATALOG_WAREHOUSE: s3a://warehouse/
      K2_ICEBERG_S3_ENDPOINT: http://minio:9000
      K2_ICEBERG_S3_ACCESS_KEY: ${MINIO_ROOT_USER:-admin}
      K2_ICEBERG_S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-password}
      K2_ICEBERG_S3_PATH_STYLE_ACCESS: "true"
      
      # Performance Configuration
      K2_QUERY_MEMORY_LIMIT: 1GB
      K2_QUERY_TIMEOUT_S: 30
      K2_HYBRID_QUERY_MAX_MESSAGES: 1000
      
      # Metrics Configuration
      K2_METRICS_ENABLED: "true"
      K2_METRICS_PORT: 9094
    command: >
      python -m uvicorn k2.api.main:app 
      --host 0.0.0.0 
      --port 8000 
      --workers 1 
      --access-log 
      --log-level info
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    restart: unless-stopped

  # ============================================================================
  # KAFKA UI - Development & Debugging Tool
  # ============================================================================
  # Provides web interface for:
  # - Topic inspection
  # - Consumer group monitoring
  # - Message browsing
  # - Schema Registry integration
  kafka-ui:
    image: kafbat/kafka-ui:v1.4.2  # Migrated from provectus (abandoned) to kafbat (active fork) - 2026-01-10
    container_name: k2-kafka-ui
    hostname: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: k2-market-data
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry-1:8081
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
      DYNAMIC_CONFIG_ENABLED: 'true'
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/actuator/health"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ============================================================================
  # BINANCE STREAM - Real-time Cryptocurrency Market Data Ingestion (RAW)
  # ============================================================================
  # Streams live trades from Binance WebSocket API to Bronze layer (RAW)
  # - Connects to Binance public trade streams
  # - Stores exchange-native format (NO transformation) to Bronze layer
  # - Publishes to Kafka market.crypto.trades.binance.raw topic
  # - V2 transformation happens in Silver Spark jobs (industry best practice)
  # - Production-grade resilience: circuit breakers, health checks, failover
  binance-stream:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: k2-binance-stream
    hostname: binance-stream
    ports:
      - "9101:9101"       # Prometheus metrics (changed from 9091 to avoid conflict with Flink)
    environment:
      # Kafka Configuration
      K2_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      K2_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      K2_KAFKA_BATCH_SIZE: 16384
      K2_KAFKA_LINGER_MS: 10
      K2_KAFKA_COMPRESSION_TYPE: lz4
      K2_KAFKA_RETRIES: 3
      K2_KAFKA_REQUEST_TIMEOUT_MS: 30000
      K2_KAFKA_MAX_IN_FLIGHT_REQUESTS: 5
      K2_KAFKA_ACKS: 1
      K2_KAFKA_IDEMPOTENCE_ENABLED: "true"

      # Binance Configuration
      K2_BINANCE_ENABLED: "true"
      K2_BINANCE_SYMBOLS: '["BTCUSDT", "ETHUSDT"]'
      K2_BINANCE_WEBSOCKET_URL: wss://stream.binance.com:9443/stream
      K2_BINANCE_FAILOVER_URLS: '["wss://stream.binance.us:9443/stream"]'

      # Resilience Configuration
      K2_BINANCE_RECONNECT_DELAY: 5
      K2_BINANCE_MAX_RECONNECT_ATTEMPTS: 10
      K2_BINANCE_HEALTH_CHECK_INTERVAL: 30
      K2_BINANCE_HEALTH_CHECK_TIMEOUT: 30

      # Metrics Configuration
      K2_METRICS_ENABLED: "true"
      K2_METRICS_PORT: 9101

      # Logging Configuration
      K2_LOG_LEVEL: INFO
      K2_LOG_FORMAT: json
    command: python scripts/binance_stream_raw.py
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9101' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================================================
  # KRAKEN STREAMING SERVICE - Real-time Kraken Trade Ingestion (RAW)
  # ============================================================================
  # Connects to Kraken WebSocket API and streams RAW trades to Kafka (Bronze layer)
  # - Subscribes to BTC/USD, ETH/USD trade streams
  # - Stores exchange-native format (no transformation) to Bronze layer
  # - Publishes to Kafka market.crypto.trades.kraken.raw topic
  # - V2 transformation happens in Silver Spark jobs (industry best practice)
  # - Production-grade resilience: circuit breakers, health checks, failover
  kraken-stream:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: k2-kraken-stream
    hostname: kraken-stream
    ports:
      - "9095:9095"       # Prometheus metrics
    environment:
      # Kafka Configuration
      K2_KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      K2_KAFKA_SCHEMA_REGISTRY_URL: http://schema-registry-1:8081
      K2_KAFKA_BATCH_SIZE: 16384
      K2_KAFKA_LINGER_MS: 10
      K2_KAFKA_COMPRESSION_TYPE: lz4
      K2_KAFKA_RETRIES: 3
      K2_KAFKA_REQUEST_TIMEOUT_MS: 30000
      K2_KAFKA_MAX_IN_FLIGHT_REQUESTS: 5
      K2_KAFKA_ACKS: 1
      K2_KAFKA_IDEMPOTENCE_ENABLED: "true"

      # Kraken Configuration
      K2_KRAKEN_ENABLED: "true"
      K2_KRAKEN_SYMBOLS: '["BTC/USD", "ETH/USD"]'
      K2_KRAKEN_WEBSOCKET_URL: wss://ws.kraken.com
      K2_KRAKEN_FAILOVER_URLS: '["wss://ws-auth.kraken.com"]'

      # Resilience Configuration
      K2_KRAKEN_RECONNECT_DELAY: 5
      K2_KRAKEN_MAX_RECONNECT_ATTEMPTS: 10
      K2_KRAKEN_HEALTH_CHECK_INTERVAL: 30
      K2_KRAKEN_HEALTH_CHECK_TIMEOUT: 30

      # Metrics Configuration
      K2_METRICS_ENABLED: "true"
      K2_METRICS_PORT: 9095

      # Logging Configuration
      K2_LOG_LEVEL: INFO
      K2_LOG_FORMAT: json
    command: python scripts/kraken_stream_raw.py
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9095' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ============================================================================
  # KAFKA CONSUMER - Production-Ready Consumer for Market Data Ingestion
  # ============================================================================
  # ============================================================================
  # SPARK CLUSTER - Distributed Processing for Medallion Architecture
  # ============================================================================
  # Spark cluster for implementing Bronze → Silver → Gold transformations
  # - Reads from Kafka (structured streaming)
  # - Writes to Iceberg tables (ACID transactions)
  # - Implements data quality validation
  # - Supports real-time aggregations

  # SPARK MASTER - Cluster Coordinator
  spark-master:
    image: apache/spark:3.5.3
    container_name: k2-spark-master
    hostname: spark-master
    ports:
      - "8090:8080"       # Spark Web UI (changed from 8080 to avoid Kafka UI conflict)
      - "7077:7077"       # Spark master port
      - "4040:4040"       # Spark driver UI
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    environment:
      - SPARK_NO_DAEMONIZE=true
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G

  # SPARK WORKER 1
  spark-worker-1:
    image: apache/spark:3.5.3
    container_name: k2-spark-worker-1
    hostname: spark-worker-1
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=3g
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'org.apache.spark.deploy.worker.Worker' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '3.5'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # SPARK WORKER 2
  spark-worker-2:
    image: apache/spark:3.5.3
    container_name: k2-spark-worker-2
    hostname: spark-worker-2
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_NO_DAEMONIZE=true
      - SPARK_WORKER_CORES=3
      - SPARK_WORKER_MEMORY=3g
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'org.apache.spark.deploy.worker.Worker' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '3.5'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # ============================================================================
  # APACHE FLINK - Real-Time Stream Processing Cluster
  # ============================================================================
  # Flink cluster for Bronze layer ingestion (Kafka → Iceberg)
  # Architecture: 1 JobManager + 2 TaskManagers (session cluster)
  # - JobManager: Coordinates jobs, Web UI, checkpoints
  # - TaskManager-1: Runs Binance Bronze job (2 task slots)
  # - TaskManager-2: Runs Kraken Bronze job (2 task slots)
  # Resources: 5 CPU, 5GB RAM total (vs Spark 13 CPU, 12GB RAM)

  # ============================================================================
  # FLINK SERVICES (DISABLED - Investigation Complete, See Phase 12 Docs)
  # Preserved for future investigation - Do NOT delete
  # To re-enable: uncomment lines 845-1054
  # ============================================================================
# 
#   flink-jobmanager:
#     image: flink-k2:1.18.1
#     container_name: k2-flink-jobmanager
#     hostname: flink-jobmanager
#     ports:
#       - "8082:8081"     # Flink Web UI (mapped to 8082 to avoid conflict with Schema Registry)
#       - "6123:6123"     # RPC
#       - "9091:9091"     # Prometheus metrics
#     command: jobmanager
#     environment:
#       - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
#       - HADOOP_CONF_DIR=/opt/flink/conf
#       - AWS_REGION=us-east-1
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin
#     volumes:
#       - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
#       - ./config/flink-sql:/opt/flink/sql:ro
#     networks:
#       - k2-network
#     depends_on:
#       kafka:
#         condition: service_healthy
#       schema-registry-1:
#         condition: service_healthy
#       iceberg-rest:
#         condition: service_healthy
#       minio:
#         condition: service_healthy
#     healthcheck:
#       test: ["CMD-SHELL", "curl -f http://localhost:8081 || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 3
#       start_period: 40s
#     deploy:
#       resources:
#         limits:
#           cpus: '1.0'
#           memory: 1G
#         reservations:
#           cpus: '0.5'
#           memory: 512M
# 
#   flink-taskmanager-1:
#     image: flink-k2:1.18.1
#     container_name: k2-flink-taskmanager-1
#     hostname: flink-taskmanager-1
#     command: taskmanager
#     environment:
#       - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
#       - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
#       - AWS_REGION=us-east-1
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin
#     volumes:
#       - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
#     networks:
#       - k2-network
#     depends_on:
#       flink-jobmanager:
#         condition: service_healthy
#     healthcheck:
#       test: ["CMD-SHELL", "pgrep -f 'org.apache.flink.runtime.taskexecutor.TaskManagerRunner' || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 3
#       start_period: 40s
#     deploy:
#       resources:
#         limits:
#           cpus: '2.0'
#           memory: 2G
#         reservations:
#           cpus: '1.0'
#           memory: 1G
# 
#   flink-taskmanager-2:
#     image: flink-k2:1.18.1
#     container_name: k2-flink-taskmanager-2
#     hostname: flink-taskmanager-2
#     command: taskmanager
#     environment:
#       - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
#       - TASK_MANAGER_NUMBER_OF_TASK_SLOTS=2
#       - AWS_REGION=us-east-1
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin
#     volumes:
#       - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
#     networks:
#       - k2-network
#     depends_on:
#       flink-jobmanager:
#         condition: service_healthy
#     healthcheck:
#       test: ["CMD-SHELL", "pgrep -f 'org.apache.flink.runtime.taskexecutor.TaskManagerRunner' || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 3
#       start_period: 40s
#     deploy:
#       resources:
#         limits:
#           cpus: '2.0'
#           memory: 2G
#         reservations:
#           cpus: '1.0'
#           memory: 1G
# 
#   # SQL Gateway for submitting SQL streaming jobs (Flink best practice)
#   flink-sql-gateway:
#     image: flink-k2:1.18.1
#     container_name: k2-flink-sql-gateway
#     hostname: flink-sql-gateway
#     ports:
#       - "8083:8083"  # SQL Gateway REST API
#     command: bin/sql-gateway.sh start-foreground
#     environment:
#       - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
#       - HADOOP_CONF_DIR=/opt/flink/conf
#       - AWS_REGION=us-east-1
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin
#     volumes:
#       - ./config/flink/flink-conf.yaml:/opt/flink/conf/flink-conf.yaml
#     networks:
#       - k2-network
#     depends_on:
#       flink-jobmanager:
#         condition: service_healthy
#     healthcheck:
#       test: ["CMD-SHELL", "curl -f http://localhost:8083/v1/info || exit 1"]
#       interval: 30s
#       timeout: 10s
#       retries: 3
#       start_period: 40s
#     restart: unless-stopped
#     deploy:
#       resources:
#         limits:
#           cpus: '0.5'
#           memory: 512M
# 
#   flink-bronze-binance-job:
#     build:
#       context: ./flink-jobs
#       dockerfile: Dockerfile
#     image: k2-flink-bronze-jobs:1.0.0
#     container_name: k2-flink-bronze-binance-job
#     hostname: flink-bronze-binance-job
#     command: ["com.k2.flink.bronze.BinanceBronzeJob", "bronze_binance_ingestion"]
#     environment:
#       - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
#       - HADOOP_CONF_DIR=/opt/flink/conf
#       - AWS_REGION=us-east-1
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin
#     networks:
#       - k2-network
#     depends_on:
#       flink-jobmanager:
#         condition: service_healthy
#       flink-taskmanager-1:
#         condition: service_healthy
#       flink-taskmanager-2:
#         condition: service_healthy
#     restart: unless-stopped
#     deploy:
#       resources:
#         limits:
#           cpus: '0.5'
#           memory: 512M
# 
#   flink-bronze-kraken-job:
#     image: k2-flink-bronze-jobs:1.0.0
#     container_name: k2-flink-bronze-kraken-job
#     hostname: flink-bronze-kraken-job
#     command: ["com.k2.flink.bronze.KrakenBronzeJob", "bronze_kraken_ingestion"]
#     environment:
#       - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
#       - HADOOP_CONF_DIR=/opt/flink/conf
#       - AWS_REGION=us-east-1
#       - AWS_ACCESS_KEY_ID=minioadmin
#       - AWS_SECRET_ACCESS_KEY=minioadmin
#     networks:
#       - k2-network
#     depends_on:
#       flink-jobmanager:
#         condition: service_healthy
#       flink-taskmanager-1:
#         condition: service_healthy
#       flink-taskmanager-2:
#         condition: service_healthy
#     restart: unless-stopped
#     deploy:
#       resources:
#         limits:
#           cpus: '0.5'
#           memory: 512M
# 
#   # ============================================================================
#   # BRONZE STREAMING JOBS - Kafka to Iceberg Raw Ingestion
#   # ============================================================================
#   # These services run Spark Structured Streaming jobs that consume from Kafka
#   # and write raw Avro payloads to Bronze Iceberg tables (per-exchange).
#   # - Automatic restart on failure
#   # - Resource limits prevent cluster starvation
#   # - Checkpoints enable fault tolerance

  bronze-binance-stream:
    image: apache/spark:3.5.3
    container_name: k2-bronze-binance-stream
    hostname: bronze-binance-stream
    environment:
      - AWS_REGION=us-east-1
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'bronze_binance_ingestion.py' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 90s
    restart: unless-stopped
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --total-executor-cores 1
      --executor-cores 1
      --executor-memory 1024m
      --driver-memory 512m
      --conf spark.driver.extraJavaOptions='-Daws.region=us-east-1'
      --conf spark.executor.extraJavaOptions='-Daws.region=us-east-1'
      --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars-extra/spark-sql-kafka-0-10_2.12-3.5.3.jar,/opt/spark/jars-extra/kafka-clients-3.5.1.jar,/opt/spark/jars-extra/commons-pool2-2.11.1.jar,/opt/spark/jars-extra/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,/opt/spark/jars-extra/spark-avro_2.12-3.5.3.jar
      /opt/k2/src/k2/spark/jobs/streaming/bronze_binance_ingestion.py
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  bronze-kraken-stream:
    image: apache/spark:3.5.3
    container_name: k2-bronze-kraken-stream
    hostname: bronze-kraken-stream
    environment:
      - AWS_REGION=us-east-1
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
      kafka:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'bronze_kraken_ingestion.py' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 90s
    restart: unless-stopped
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --total-executor-cores 1
      --executor-cores 1
      --executor-memory 1024m
      --driver-memory 512m
      --conf spark.driver.extraJavaOptions='-Daws.region=us-east-1'
      --conf spark.executor.extraJavaOptions='-Daws.region=us-east-1'
      --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars-extra/spark-sql-kafka-0-10_2.12-3.5.3.jar,/opt/spark/jars-extra/kafka-clients-3.5.1.jar,/opt/spark/jars-extra/commons-pool2-2.11.1.jar,/opt/spark/jars-extra/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,/opt/spark/jars-extra/spark-avro_2.12-3.5.3.jar
      /opt/k2/src/k2/spark/jobs/streaming/bronze_kraken_ingestion.py
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ============================================================================
  # SILVER TRANSFORMATION JOBS - Validated Data (Medallion Architecture)
  # ============================================================================
  # Industry Best Practice: Silver layer validates and deserializes Bronze data
  # - Avro deserialization with Schema Registry
  # - Data quality validation (DLQ pattern for failures)
  # - Metadata enrichment (validation_timestamp, schema_id)

  silver-binance-transformation:
    image: apache/spark:3.5.3
    container_name: k2-silver-binance-transformation
    hostname: silver-binance-transformation
    environment:
      - AWS_REGION=us-east-1
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
      bronze-binance-stream:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'silver_binance_transformation_v3.py' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --total-executor-cores 1
      --executor-cores 1
      --executor-memory 1024m
      --driver-memory 512m
      --conf spark.driver.extraJavaOptions='-Daws.region=us-east-1'
      --conf spark.executor.extraJavaOptions='-Daws.region=us-east-1'
      --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars-extra/spark-avro_2.12-3.5.3.jar
      /opt/k2/src/k2/spark/jobs/streaming/silver_binance_transformation_v3.py
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  silver-kraken-transformation:
    image: apache/spark:3.5.3
    container_name: k2-silver-kraken-transformation
    hostname: silver-kraken-transformation
    environment:
      - AWS_REGION=us-east-1
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
      bronze-kraken-stream:
        condition: service_healthy
      schema-registry-1:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'silver_kraken_transformation_v3.py' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --total-executor-cores 1
      --executor-cores 1
      --executor-memory 1024m
      --driver-memory 512m
      --conf spark.driver.extraJavaOptions='-Daws.region=us-east-1'
      --conf spark.executor.extraJavaOptions='-Daws.region=us-east-1'
      --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.262.jar,/opt/spark/jars-extra/spark-avro_2.12-3.5.3.jar
      /opt/k2/src/k2/spark/jobs/streaming/silver_kraken_transformation_v3.py
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ============================================================================
  # GOLD AGGREGATION JOB - Unified Analytics (Medallion Architecture)
  # ============================================================================
  # Industry Best Practice: Gold layer provides unified cross-exchange analytics
  # - Union of Silver tables (multi-exchange view)
  # - Deduplication by message_id (exactly-once semantics)
  # - Watermarking for late data handling (5-minute grace period)
  # - Hourly partitioning for efficient time-series queries

  gold-aggregation:
    image: apache/spark:3.5.3
    container_name: k2-gold-aggregation
    hostname: gold-aggregation
    environment:
      - AWS_REGION=us-east-1
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - ./spark-jars:/opt/spark/jars-extra
      - spark-checkpoints:/checkpoints
    depends_on:
      spark-master:
        condition: service_healthy
      iceberg-rest:
        condition: service_healthy
      silver-binance-transformation:
        condition: service_healthy
      silver-kraken-transformation:
        condition: service_healthy
    networks:
      - k2-network
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f 'gold_aggregation.py' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 180s
    restart: unless-stopped
    command: >
      /opt/spark/bin/spark-submit
      --master spark://spark-master:7077
      --total-executor-cores 1
      --executor-cores 1
      --executor-memory 1024m
      --driver-memory 512m
      --conf spark.driver.extraJavaOptions='-Daws.region=us-east-1'
      --conf spark.executor.extraJavaOptions='-Daws.region=us-east-1'
      --jars /opt/spark/jars-extra/iceberg-spark-runtime-3.5_2.12-1.4.0.jar,/opt/spark/jars-extra/iceberg-aws-1.4.0.jar,/opt/spark/jars-extra/bundle-2.20.18.jar,/opt/spark/jars-extra/url-connection-client-2.20.18.jar,/opt/spark/jars-extra/hadoop-aws-3.3.4.jar,/opt/spark/jars-extra/aws-java-sdk-bundle-1.12.262.jar
      /opt/k2/src/k2/spark/jobs/streaming/gold_aggregation.py
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ============================================================================
  # CHECKPOINT CLEANUP SERVICE - Prevents Metadata Accumulation
  # ============================================================================
  # Lightweight service that runs daily to clean up old Spark checkpoint metadata
  # - Removes metadata files older than 7 days
  # - Cleans up empty directories
  # - Prevents disk space exhaustion from checkpoint accumulation
  # - Production recommendation: Adjust retention based on recovery requirements
  spark-checkpoint-cleaner:
    image: alpine:3.19
    container_name: k2-checkpoint-cleaner
    hostname: checkpoint-cleaner
    volumes:
      - spark-checkpoints:/checkpoints
    command: >
      sh -c '
      echo "Checkpoint Cleaner Service Started";
      echo "Retention policy: 7 days";
      echo "Check interval: 24 hours";
      while true; do
        echo "[$(date)] Running checkpoint cleanup...";
        find /checkpoints -type f -name "metadata" -mtime +7 -exec rm -v {} \; 2>/dev/null || true;
        find /checkpoints -type f -name "*.compact" -mtime +7 -exec rm -v {} \; 2>/dev/null || true;
        find /checkpoints -type d -empty -delete 2>/dev/null || true;
        echo "[$(date)] Cleanup complete. Sleeping 24h...";
        sleep 86400;
      done
      '
    restart: unless-stopped
    networks:
      - k2-network
    deploy:
      resources:
        limits:
          cpus: '0.1'
          memory: 128M
        reservations:
          cpus: '0.05'
          memory: 64M

# ==============================================================================
# NETWORKS
# ==============================================================================
# Isolated network for service communication
# In production: Would use overlay networks for multi-host deployment
networks:
  k2-network:
    driver: bridge
    name: k2-network

# ==============================================================================
# VOLUMES - Persistent Data Storage
# ==============================================================================
# Named volumes for data persistence across container restarts
# In production: Would use remote storage (EBS, NFS, etc.)
volumes:
  kafka-data:
    driver: local
  minio-data:
    driver: local
  postgres-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  spark-checkpoints:
    driver: local
