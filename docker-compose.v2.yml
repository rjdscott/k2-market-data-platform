# K2 Market Data Platform v2 - Consolidated Architecture
# Version: 2.0.0-consolidated
# Last Updated: 2026-02-12
# Target: 16 CPU cores / 40GB RAM

#############################################################################
# NETWORKS
#############################################################################
networks:
  k2-net:
    driver: bridge
    ipam:
      config:
        - subnet: 172.28.0.0/16

#############################################################################
# VOLUMES
#############################################################################
volumes:
  # Redpanda data
  redpanda-data:
    driver: local

  # ClickHouse data
  clickhouse-data:
    driver: local
  clickhouse-logs:
    driver: local

  # Monitoring data
  prometheus-data:
    driver: local
  grafana-data:
    driver: local

  # Orchestration data
  postgres-data:
    driver: local

  # Object storage data
  minio-data:
    driver: local

  # Iceberg warehouse
  iceberg-warehouse:
    driver: local

#############################################################################
# SERVICES - STREAMING BACKBONE
#############################################################################
services:

  # ───────────────────────────────────────────────────────────────────────
  # Redpanda - Kafka-compatible streaming platform (C++, single binary)
  # ───────────────────────────────────────────────────────────────────────
  redpanda:
    image: docker.redpanda.com/redpandadata/redpanda:v25.3.4
    container_name: k2-redpanda
    hostname: redpanda
    networks:
      k2-net:
        ipv4_address: 172.28.0.10
    ports:
      - "9092:9092"     # Kafka API
      - "9644:9644"     # Admin API
      - "8081:8081"     # Schema Registry
    volumes:
      - redpanda-data:/var/lib/redpanda/data
    command:
      - redpanda
      - start
      - --kafka-addr internal://0.0.0.0:9092,external://0.0.0.0:19092
      - --advertise-kafka-addr internal://redpanda:9092,external://localhost:19092
      - --pandaproxy-addr internal://0.0.0.0:8082,external://0.0.0.0:18082
      - --advertise-pandaproxy-addr internal://redpanda:8082,external://localhost:18082
      - --schema-registry-addr internal://0.0.0.0:8081,external://0.0.0.0:18081
      - --rpc-addr redpanda:33145
      - --advertise-rpc-addr redpanda:33145
      - --mode dev-container
      - --smp 1
      - --memory 1500M
    environment:
      REDPANDA_PANDAPROXY_CLIENT_RETRIES: 5
      REDPANDA_PANDAPROXY_CLIENT_RETRY_BASE_BACKOFF_MS: 100
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "rpk cluster health | grep -q 'Healthy'"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "streaming"
      com.k2.tier: "hot"
      com.k2.version: "v2"

  # ───────────────────────────────────────────────────────────────────────
  # Redpanda Console - Web UI for Redpanda/Kafka
  # ───────────────────────────────────────────────────────────────────────
  redpanda-console:
    image: docker.redpanda.com/redpandadata/console:v3.5.1
    container_name: k2-redpanda-console
    hostname: console
    networks:
      - k2-net
    ports:
      - "8080:8080"
    environment:
      KAFKA_BROKERS: redpanda:9092
      KAFKA_SCHEMAREGISTRY_ENABLED: true
      KAFKA_SCHEMAREGISTRY_URLS: http://redpanda:8081
    depends_on:
      redpanda:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/admin/health"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    labels:
      com.k2.service: "ui"
      com.k2.tier: "management"
      com.k2.version: "v2"

#############################################################################
# SERVICES - WARM STORAGE (ClickHouse)
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # ClickHouse - OLAP database for real-time analytics
  # ───────────────────────────────────────────────────────────────────────
  clickhouse:
    image: clickhouse/clickhouse-server:24.3-alpine
    container_name: k2-clickhouse
    hostname: clickhouse
    networks:
      k2-net:
        ipv4_address: 172.28.0.20
    ports:
      - "8123:8123"     # HTTP interface
      - "9002:9000"     # Native protocol (9002 to avoid MinIO conflict)
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - clickhouse-logs:/var/log/clickhouse-server
      - ./docker/clickhouse/config.xml:/etc/clickhouse-server/config.d/k2-config.xml:ro
      # - ./docker/clickhouse/users.xml:/etc/clickhouse-server/users.d/k2-users.xml:ro  # Removed: using default auth with CLICKHOUSE_PASSWORD env var
      - ./docker/clickhouse/ddl:/docker-entrypoint-initdb.d:ro
    environment:
      CLICKHOUSE_DB: k2
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ${CLICKHOUSE_PASSWORD:-clickhouse}
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1
    deploy:
      resources:
        limits:
          cpus: '4.0'
          memory: 8G
        reservations:
          cpus: '2.0'
          memory: 4G
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    healthcheck:
      test: ["CMD", "clickhouse-client", "--query", "SELECT 1"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "storage"
      com.k2.tier: "warm"
      com.k2.version: "v2"

  # ───────────────────────────────────────────────────────────────────────
  # MinIO - S3-compatible object storage for Iceberg
  # ───────────────────────────────────────────────────────────────────────
  minio:
    image: minio/minio:RELEASE.2024-01-18T22-51-28Z
    container_name: k2-minio
    hostname: minio
    networks:
      k2-net:
        ipv4_address: 172.28.0.21
    ports:
      - "9000:9000"     # API
      - "9001:9001"     # Console
    volumes:
      - minio-data:/data
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    labels:
      com.k2.service: "storage"
      com.k2.tier: "cold"
      com.k2.version: "v2"

#############################################################################
# SERVICES - MONITORING & OBSERVABILITY
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # Prometheus - Metrics collection
  # ───────────────────────────────────────────────────────────────────────
  prometheus:
    image: prom/prometheus:v3.2.0
    container_name: k2-prometheus
    hostname: prometheus
    networks:
      - k2-net
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    labels:
      com.k2.service: "monitoring"
      com.k2.tier: "observability"
      com.k2.version: "v2"

  # ───────────────────────────────────────────────────────────────────────
  # Grafana - Metrics visualization
  # ───────────────────────────────────────────────────────────────────────
  grafana:
    image: grafana/grafana:11.5.0
    container_name: k2-grafana
    hostname: grafana
    networks:
      - k2-net
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./docker/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./docker/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:-admin}
      GF_INSTALL_PLUGINS: ""
      GF_AUTH_ANONYMOUS_ENABLED: false
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_LOG_LEVEL: info
    depends_on:
      prometheus:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: unless-stopped
    labels:
      com.k2.service: "monitoring"
      com.k2.tier: "observability"
      com.k2.version: "v2"

#############################################################################
# SERVICES - ORCHESTRATION LAYER
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # PostgreSQL - Prefect metadata + watermarks
  # ───────────────────────────────────────────────────────────────────────
  prefect-db:
    image: postgres:15-alpine
    container_name: k2-prefect-db
    hostname: prefect-db
    networks:
      k2-net:
        ipv4_address: 172.28.0.30
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres/ddl:/docker-entrypoint-initdb.d:ro
    environment:
      POSTGRES_USER: prefect
      POSTGRES_PASSWORD: prefect
      POSTGRES_DB: prefect
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U prefect"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    restart: unless-stopped
    labels:
      com.k2.service: "orchestration"
      com.k2.tier: "control-plane"
      com.k2.version: "v2"

  # ───────────────────────────────────────────────────────────────────────
  # Prefect Server - Workflow orchestration UI/API (v3.x)
  # ───────────────────────────────────────────────────────────────────────
  prefect-server:
    image: prefecthq/prefect:3-python3.12
    container_name: k2-prefect-server
    hostname: prefect-server
    networks:
      k2-net:
        ipv4_address: 172.28.0.31
    ports:
      - "4200:4200"
    volumes:
      - ./docker/prefect:/root/.prefect
    environment:
      PREFECT_SERVER_API_HOST: 0.0.0.0
      PREFECT_API_URL: http://prefect-server:4200/api
      PREFECT_API_DATABASE_CONNECTION_URL: postgresql+asyncpg://prefect:prefect@prefect-db:5432/prefect
    command: prefect server start --host 0.0.0.0
    depends_on:
      prefect-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4200/api/health')"]
      interval: 15s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "orchestration"
      com.k2.tier: "control-plane"
      com.k2.version: "v2"

  # ───────────────────────────────────────────────────────────────────────
  # Prefect Worker - Workflow executor (v3.x uses workers not agents)
  # ───────────────────────────────────────────────────────────────────────
  prefect-worker:
    image: prefecthq/prefect:3-python3.12
    container_name: k2-prefect-worker
    hostname: prefect-worker
    networks:
      k2-net:
        ipv4_address: 172.28.0.32
    volumes:
      - ./docker/prefect:/root/.prefect
      - ./docker/offload:/opt/prefect/offload
      - ./docker/offload/flows:/opt/prefect/flows
      - /var/run/docker.sock:/var/run/docker.sock  # Docker socket for container orchestration
    environment:
      PREFECT_API_URL: http://prefect-server:4200/api
      PYTHONPATH: /opt/prefect
      DOCKER_API_VERSION: "1.44"
    command: sh -c "apt-get update && apt-get install -y docker.io && pip install psycopg2-binary prometheus-client && prefect worker start --pool iceberg-offload --type process"
    depends_on:
      prefect-server:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped
    labels:
      com.k2.service: "orchestration"
      com.k2.tier: "control-plane"
      com.k2.version: "v2"

#############################################################################
# SERVICES - ETL / OFFLOAD LAYER
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # Spark + Iceberg - ETL engine for cold tier offload
  # ───────────────────────────────────────────────────────────────────────
  spark-iceberg:
    build:
      context: .
      dockerfile: docker/spark/Dockerfile
    image: k2-spark-iceberg:3.5.0_1.4.2  # local tag after build
    container_name: k2-spark-iceberg
    hostname: spark-iceberg
    networks:
      k2-net:
        ipv4_address: 172.28.0.40
    ports:
      - "18080:8080"    # Spark Master UI (mapped to 18080 to avoid conflict)
      - "4040:4040"     # Spark Application UI
      - "8888:8888"     # Jupyter (if needed)
    volumes:
      - iceberg-warehouse:/home/iceberg/warehouse
      - ./docker/iceberg/warehouse:/home/iceberg/warehouse
      - ./docker/offload:/home/iceberg/offload
      - ./docker/offload/flows:/home/iceberg/offload/flows
    environment:
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_REGION=us-east-1
      - AWS_S3_ENDPOINT=http://minio:9000
    depends_on:
      minio:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      prefect-db:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
    restart: unless-stopped
    labels:
      com.k2.service: "etl"
      com.k2.tier: "cold"
      com.k2.version: "v2"

#############################################################################
# SERVICES - TOPIC INITIALIZATION
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # Redpanda Init - One-shot service to create topics with explicit partitions
  # Runs once at startup; feed handlers depend on service_completed_successfully
  # ───────────────────────────────────────────────────────────────────────
  redpanda-init:
    image: docker.redpanda.com/redpandadata/redpanda:v25.3.4
    container_name: k2-redpanda-init
    networks:
      - k2-net
    depends_on:
      redpanda:
        condition: service_healthy
    entrypoint: ["/bin/bash", "-c"]
    command:
      - |
        rpk topic create market.crypto.trades.binance.raw --partitions 40 --brokers redpanda:9092 --if-not-exists &&
        rpk topic create market.crypto.trades.binance     --partitions 40 --brokers redpanda:9092 --if-not-exists &&
        rpk topic create market.crypto.trades.kraken.raw  --partitions 20 --brokers redpanda:9092 --if-not-exists &&
        rpk topic create market.crypto.trades.kraken      --partitions 20 --brokers redpanda:9092 --if-not-exists &&
        echo "✅ All topics initialized"
    restart: "no"
    labels:
      com.k2.service: "init"
      com.k2.tier: "hot"
      com.k2.version: "v2"

#############################################################################
# SERVICES - FEED HANDLERS (DATA INGESTION)
#############################################################################

  # ───────────────────────────────────────────────────────────────────────
  # Binance Feed Handler - Kotlin/Coroutines WebSocket client
  # ───────────────────────────────────────────────────────────────────────
  feed-handler-binance:
    build:
      context: .
      dockerfile: services/feed-handler-kotlin/Dockerfile
    container_name: k2-feed-handler-binance
    hostname: feed-handler-binance
    networks:
      k2-net:
        ipv4_address: 172.28.0.50
    depends_on:
      redpanda:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      redpanda-init:
        condition: service_completed_successfully
    environment:
      - JAVA_OPTS=-Xmx512m -Xms256m
      - K2_EXCHANGE=binance
      - K2_INSTRUMENTS_FILE=/app/config/instruments.yaml
      - K2_SCHEMA_PATH=/app/schemas
      - K2_KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
      - K2_KAFKA_SCHEMA_REGISTRY_URL=http://redpanda:8081
    volumes:
      - ./logs/binance:/app/logs
      - ./config/instruments.yaml:/app/config/instruments.yaml:ro
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "pgrep", "-f", "java"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "feed-handler"
      com.k2.exchange: "binance"
      com.k2.version: "v2"

  # ───────────────────────────────────────────────────────────────────────
  # Kraken Feed Handler - Kotlin/Coroutines WebSocket client
  # ───────────────────────────────────────────────────────────────────────
  feed-handler-kraken:
    build:
      context: .
      dockerfile: services/feed-handler-kotlin/Dockerfile
    container_name: k2-feed-handler-kraken
    hostname: feed-handler-kraken
    networks:
      k2-net:
        ipv4_address: 172.28.0.51
    depends_on:
      redpanda:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
      redpanda-init:
        condition: service_completed_successfully
    environment:
      - JAVA_OPTS=-Xmx512m -Xms256m
      - K2_EXCHANGE=kraken
      - K2_INSTRUMENTS_FILE=/app/config/instruments.yaml
      - K2_SCHEMA_PATH=/app/schemas
      - K2_KAFKA_BOOTSTRAP_SERVERS=redpanda:9092
      - K2_KAFKA_SCHEMA_REGISTRY_URL=http://redpanda:8081
    volumes:
      - ./logs/kraken:/app/logs
      - ./config/instruments.yaml:/app/config/instruments.yaml:ro
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "pgrep", "-f", "java"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped
    labels:
      com.k2.service: "feed-handler"
      com.k2.exchange: "kraken"
      com.k2.version: "v2"

#############################################################################
# RESOURCE SUMMARY (Consolidated - Full Platform)
#############################################################################
# Service              | CPU Limit | Memory Limit | Purpose
# ---------------------|-----------|--------------|---------------------------
# redpanda             | 2.0       | 2G           | Streaming backbone
# redpanda-init        | -         | -            | One-shot topic init (exits)
# redpanda-console     | 0.5       | 256M         | Streaming UI
# clickhouse           | 4.0       | 8G           | Warm storage OLAP
# minio                | 1.0       | 1G           | S3-compatible storage
# prometheus           | 1.0       | 2G           | Metrics collection
# grafana              | 0.5       | 512M         | Visualization
# prefect-db           | 1.0       | 1G           | Metadata + watermarks
# prefect-server       | 1.0       | 1G           | Orchestration UI/API (v3.x)
# prefect-worker       | 0.5       | 512M         | Workflow executor (v3.x)
# spark-iceberg        | 2.0       | 4G           | ETL engine
# feed-handler-binance | 0.5       | 512M         | Binance ingestion (12 pairs)
# feed-handler-kraken  | 0.5       | 512M         | Kraken ingestion (11 pairs)
# ---------------------|-----------|--------------|---------------------------
# TOTAL                | 15.0 CPU  | 21.25 GB     | Full consolidated platform
#
# Remaining budget: 1.0 CPU / 18.75 GB for future expansion
# Target: 16 CPU / 40 GB total
#
# Instrument config: see config/instruments.yaml (single source of truth)
# Last updated: 2026-02-18
#############################################################################
