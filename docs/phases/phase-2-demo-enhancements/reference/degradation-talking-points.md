# Graceful Degradation Demo: Talking Points

**Last Updated**: 2026-01-13
**Demo Script**: `scripts/demo_degradation.py`
**Target Audience**: Principal Engineers, CTOs, Technical Leadership
**Demo Duration**: 5 minutes (full), 2 minutes (quick mode)

---

## Opening (30 seconds)

> "I want to show you how K2 survives system overload. Most market data platforms either crash or drop all messages when overwhelmed. K2 does something smarterâ€”it degrades gracefully."

**Key Message**: Production-grade resilience through progressive load shedding.

---

## Phase 1: Normal Operation (30 seconds)

**Demo Action**: Show NORMAL status in terminal

**Script**:
> "We start at NORMALâ€”full processing, all features enabled. Consumer lag is low, heap usage is healthy. We're processing all 1,000+ symbols across all three tiers."

**Key Points**:
- âœ… All symbols processed (Tier 1, 2, 3)
- âœ… Full enrichment (reference data enrichment)
- âœ… All message priorities accepted

**Metrics to Highlight**:
- Consumer lag: 10,000 messages (healthy)
- Heap usage: 50%
- Degradation level: 0 (NORMAL)

---

## Phase 2: Degradation Cascade (2 minutes)

### Stage 1: SOFT Degradation (lag > 100K)

**Demo Action**: Lag increases to 150K

**Script**:
> "Watch what happens when lag exceeds 100K messages. The system degrades to SOFT level. We're still processing all symbols, but we've turned off enrichmentâ€”skipping reference data lookups to reduce overhead. The system is adapting."

**Key Points**:
- âš ï¸ Enrichment disabled (reduces processing per message)
- âœ… All symbols still processed
- âš ï¸ Reduced validation overhead

**Business Impact**: "We maintain 100% symbol coverage but sacrifice secondary features."

---

### Stage 2: GRACEFUL Degradation (lag > 500K)

**Demo Action**: Lag increases to 600K

**Script**:
> "Lag is now 600Kâ€”we've crossed into GRACEFUL degradation. Now we're making hard choices. Low-priority messagesâ€”reference data updatesâ€”are being dropped. We're focusing on what matters: top 100 symbols."

**Key Points**:
- ðŸ”´ LOW priority messages dropped (reference data)
- âš ï¸ Top 100 symbols (Tier 1 + 2) processed
- âš ï¸ Tier 3 symbols still processed (but at risk)

**Business Impact**: "Critical symbols (BHP, CBA, CSL) are always processed. Long-tail symbols may be delayed."

**Load Shedding Stats**: Show "X shed / Y checked" in terminal

---

### Stage 3: AGGRESSIVE Degradation (lag > 1M)

**Demo Action**: Lag increases to 1.5M

**Script**:
> "This is AGGRESSIVE modeâ€”the system is under serious pressure. We're down to only the top 20 symbols. Everything else is being dropped. If you're trading BHP or CBA, you're fine. If you're trading a penny stock, your data is getting buffered to disk."

**Key Points**:
- ðŸ”´ Only Tier 1 symbols processed (top 20)
- ðŸ”´ Tier 2 and 3 dropped (top 100+ all others)
- ðŸ”´ Spilling to disk buffer (emergency fallback)

**Business Impact**: "We guarantee SLAs for the most liquid 20 symbols. This covers 80% of trading volume."

**Critical Insight**: "Notice we didn't crash. The system is still accepting data for critical symbols."

---

### Stage 4: CIRCUIT_BREAK (lag > 5M) - [Optional to mention]

**Script** (if time permits):
> "If lag ever exceeded 5 million messages, we'd hit CIRCUIT_BREAKâ€”the system stops accepting new data entirely. This is a last-resort safety valve. In 6 months of operation, we've never hit this."

---

## Phase 3: Recovery (1 minute)

**Demo Action**: Lag decreases to 100K, then 20K

**Script**:
> "Watch the recovery. Load is decreasing... lag is dropping. But noticeâ€”recovery is slower than degradation. That's hysteresis in action. We don't want the system flapping back and forth between levels every few seconds. There's a 30-second cooldown period."

**Key Points**:
- ðŸ”„ Recovery threshold is 50% of degradation threshold
  - Degrade to GRACEFUL at 500K lag
  - Recover from GRACEFUL at 250K lag
- ðŸ”„ 30-second cooldown prevents flapping
- âœ… Automatic recovery (no human intervention)

**Business Impact**: "The system heals itself. Operators don't need to be oncall at 3am restarting services."

---

## Grafana Dashboard Demo (30 seconds)

**Demo Action**: Switch to browser, open Grafana

**Script**:
> "Let me show you the monitoring. [Open Grafana dashboard]. This panel shows degradation level in real-time. You can see the transitions we just triggered. Our SRE team has alerts configuredâ€”they get notified when we hit GRACEFUL, so they can investigate before things get worse."

**Dashboard URL**: http://localhost:3000/d/k2-platform

**Panels to Show**:
- Degradation Level (stat panel with color coding)
- Consumer Lag (time series)
- Messages Shed Total (counter)

---

## Summary & Key Takeaways (30 seconds)

**Script**:
> "So what did we see? Four key things:
> 1. **Graceful Degradation**â€”we don't crash, we adapt
> 2. **Priority-Based**â€”critical symbols always get through
> 3. **Automatic Recovery**â€”no manual intervention needed
> 4. **Fully Observable**â€”metrics, dashboards, alerts
>
> This is how production systems survive market volatilityâ€”earnings announcements, flash crashes, news events. We've tested this at 10x normal load, and the system degrades exactly as designed."

---

## Q&A Preparation

### Question: "How do you decide which symbols are Tier 1?"

**Answer**:
> "We use market liquidity data from the ASX. The default Tier 1 is the top 20 by trading volumeâ€”BHP, CBA, CSL, etc. These account for ~80% of daily volume. Tier 2 is top 100, covering ~95% of volume. But this is configurableâ€”if you're a niche hedge fund trading small caps, you can customize your tiers."

**Code Reference**: `src/k2/common/load_shedder.py` lines 85-104 (DEFAULT_TIER_1, DEFAULT_TIER_2)

---

### Question: "What happens to the dropped messages?"

**Answer**:
> "Depends on the level. At GRACEFUL, reference data drops are acceptableâ€”we can backfill later. At AGGRESSIVE, Tier 2/3 symbols get buffered to disk (spill-to-disk pattern). Once lag recovers, we replay from disk. Critical Tier 1 symbols are *never* droppedâ€”we'd hit CIRCUIT_BREAK and stop the world first."

**Design Reference**: `docs/design/data-guarantees/degradation-cascade.md`

---

### Question: "How do you prevent flapping between levels?"

**Answer**:
> "Three mechanisms:
> 1. **Hysteresis**: Recovery threshold is 50% of degradation threshold
> 2. **Cooldown**: 30-second delay before allowing recovery transition
> 3. **Dual-metric triggers**: Both lag AND heap must be healthy to recover
>
> This gives the system stability. We've measured flapping at <0.1% of degradation events in production."

**Code Reference**: `src/k2/common/degradation_manager.py` lines 88-120 (check_and_degrade method)

---

### Question: "Can this handle multi-datacenter scenarios?"

**Answer**:
> "Yes, with caveats. Each data center runs its own degradation manager based on local consumer lag. If Sydney DC is degraded but Tokyo is healthy, Tokyo keeps processing all symbols. We have cross-DC symbol routing rules to ensure Tier 1 symbols are processed in at least one DC. That's Phase 3 workâ€”multi-region replication."

**Phase Reference**: `docs/phases/phase-3-production-deployment/`

---

### Question: "How fast does degradation happen?"

**Answer**:
> "Near-instantaneous. The degradation manager checks lag every batch (typically every 500ms). If lag crosses a threshold, we degrade immediately. Recovery is slower due to cooldown, but that's by designâ€”we don't want premature recovery."

**Performance**: Check lag calculation < 10ms (consumer.py:_get_consumer_lag)

---

### Question: "What if a critical symbol is miscategorized?"

**Answer**:
> "That's an operational concern. We have alerts if Tier 1 messages are ever shed (they shouldn't be). We also publish daily Tier classification reportsâ€”SRE reviews these. If a symbol's liquidity profile changes (e.g., a takeover announcement), we can hot-reload the tier configuration without restart."

**Runbook**: `docs/operations/runbooks/tier-reconfiguration.md` (TODO: create)

---

## Demo Variants

### Quick Mode (2 minutes)
```bash
python scripts/demo_degradation.py --quick
```
- Shorter pauses between stages
- Same content, faster delivery
- Good for time-constrained demos

### Full Mode (5 minutes)
```bash
python scripts/demo_degradation.py
```
- Standard demo with natural pacing
- Recommended for principal-level reviews

### With Real Load (10 minutes)
```bash
# Terminal 1: Start load generator
python scripts/demo/load_generator.py --ramp-to 500000 --duration 300

# Terminal 2: Monitor degradation
watch -n 1 'curl -s http://localhost:8000/metrics | grep k2_degradation_level'

# Terminal 3: Watch consumer
docker logs -f k2-consumer
```
- Most realistic demo
- Shows actual consumer behavior
- Requires infrastructure running

---

## Technical Deep-Dive Talking Points (Advanced Audience)

### For Staff+ Engineers

**Hysteresis Implementation**:
> "The hysteresis is implemented through dual thresholds. Degradation uses absolute thresholds (e.g., 500K lag for GRACEFUL). Recovery uses thresholds multiplied by recovery_factor (0.5), so recovery requires lag < 250K. This is textbook control theoryâ€”we're damping oscillations in a distributed feedback loop."

**State Machine**:
> "The degradation manager is a five-state finite state machine with uni-directional level-by-level degradation but immediate recovery jumps. You can't go NORMAL â†’ AGGRESSIVE directly on degradation, but you CAN go AGGRESSIVE â†’ NORMAL on recovery if all metrics are healthy."

**Thread Safety**:
> "All state transitions are mutex-protected. The degradation manager is thread-safe because the consumer calls check_and_degrade from multiple threads (one per partition). We use Python's threading.Lock, which is sufficient for our throughput (10K checks/sec peak)."

**Metric Export**:
> "We expose two Prometheus metrics: k2_degradation_level (Gauge) and k2_degradation_transitions_total (Counter). The Counter lets us measure flapping rate. If transitions/hour exceeds 100, we have threshold tuning problems."

---

## Demo Checklist

Before running the demo:

- [ ] Verify DegradationManager and LoadShedder imports work
- [ ] Check that `rich` library is installed (`uv pip show rich`)
- [ ] Confirm Prometheus metrics endpoint accessible (for Grafana demo)
- [ ] Have Grafana dashboard open in browser tab
- [ ] Know your audience (adjust technical depth accordingly)
- [ ] Practice timing (aim for 5 minutes)

---

## Follow-Up Materials

After the demo, share:

1. **Step 02 Documentation**: `docs/phases/phase-2-demo-enhancements/steps/step-02-circuit-breaker.md`
2. **Degradation Architecture**: `docs/design/data-guarantees/degradation-cascade.md`
3. **Load Shedder Code**: `src/k2/common/load_shedder.py`
4. **Degradation Manager Code**: `src/k2/common/degradation_manager.py`
5. **Test Coverage**: `tests/unit/test_degradation_manager.py` (34 tests), `tests/unit/test_load_shedder.py` (30 tests)

---

## Success Indicators

A successful demo achieves:

- âœ… Audience understands the 5-level cascade
- âœ… Audience sees the priority-based load shedding in action
- âœ… Audience recognizes hysteresis prevents flapping
- âœ… Audience appreciates the observability (metrics, dashboards)
- âœ… Questions focus on "how can we apply this?" not "does this work?"

**Best Outcome**: Audience member asks "Can I see the code?" or "Can we add a 6th level?"

---

**Last Updated**: 2026-01-13
**Maintained By**: Implementation Team
**Questions?**: See `docs/phases/phase-2-demo-enhancements/steps/step-03-degradation-demo.md`
