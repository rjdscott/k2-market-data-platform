# Phase 10 Implementation Plan

**Last Updated**: 2026-01-18
**Duration**: 20 days (160 hours)
**Status**: ðŸŸ¡ In Progress (Out of Order - Kraken First)
**Progress**: 2/16 steps complete (12.5%)

---

## Executive Summary

Complete platform refactor from mixed equity/crypto to crypto-only streaming platform with Spark Structured Streaming and full Medallion architecture. This plan emphasizes **clean slate approach**: remove all ASX/equity code, recreate schemas from scratch, build new Iceberg tables optimized for crypto markets.

**Implementation Note**: Steps 13-14 (Kraken Integration) were implemented early (ahead of Spark setup) to validate multi-exchange architecture pattern and de-risk the integration approach before investing in Spark infrastructure.

**Key Principles**:
1. **Complete removal** over migration (fresh start)
2. **Crypto-optimized** design (no equity baggage)
3. **Spark-native** processing (distributed from day 1)
4. **Clear separation** of concerns (Bronze/Silver/Gold)
5. **Extensible patterns** (easy to add exchanges)

---

## Milestone 1: Complete Cleanup (3 days, 16 hours)

**Goal**: Remove all traces of ASX and equity asset class, start with clean slate

### Step 01: Remove ASX Code and Data (8 hours)

**Objective**: Eliminate all ASX-specific code, configurations, and sample data

**Files to DELETE**:
1. `src/k2/ingestion/batch_loader.py` (28KB) - CSV batch loading for ASX
2. `data/sample/trades/`, `data/sample/quotes/`, `data/sample/bars-1min/` - ASX sample data
3. `data/sample/reference-data/` - ASX exchange/symbol mappings
4. `backups/tests-backup/fixtures/sample_asx_*.csv` - ASX test fixtures
5. `demos/notebooks/asx-*.ipynb` - ASX-specific notebooks
6. Search and remove: `grep -r "ASX" src/` and delete functions/classes

**Functions/Methods to REMOVE**:
- `src/k2/ingestion/message_builders.py`: Remove `build_trade_asx_specific()` if exists
- `src/k2/schemas/__init__.py`: Remove ASX schema registration
- Any references to "ASX", "equity", "equities" in code comments/docstrings

**Config Files to UPDATE**:
- `config/kafka/topics.yaml`: Remove `equities.asx` section entirely
- `.env.example`: Remove ASX-related environment variables
- `docker-compose.yml`: Remove any ASX-specific services/volumes

**Validation Commands**:
```bash
# Should return 0 results (except in docs/archive)
grep -r "ASX" src/ tests/ config/ scripts/
grep -r -i "equity" src/ tests/ config/ | grep -v "comment\|docs"

# Should not find batch_loader
find src/ -name "*batch*loader*"

# Should be clean
ls data/sample/
```

**Acceptance Criteria**:
- [x] Zero occurrences of "ASX" in src/, tests/, config/, scripts/
- [x] Zero references to "equities" or "equity" asset class (except historical docs)
- [x] `batch_loader.py` deleted
- [x] All ASX sample data removed
- [x] All ASX notebooks removed or archived
- [x] Kafka topics.yaml has no equity sections
- [x] All tests pass after removal: `uv run pytest tests/unit/ -v`

**Time Estimate**: 8 hours
**Priority**: P0 (blocking)

---

### Step 02: Remove V1 and Equity Schemas (6 hours)

**Objective**: Remove legacy v1 schemas and any equity-specific schema definitions

**Schema Files to DELETE**:
1. `src/k2/schemas/trade.avsc` (v1 trade schema)
2. `src/k2/schemas/quote.avsc` (v1 quote schema)
3. Any `*_equity_*.avsc` files if they exist

**Schema Registry Cleanup**:
```bash
# List all registered schemas
curl http://localhost:8081/subjects

# Delete v1 schemas (DESTRUCTIVE)
curl -X DELETE http://localhost:8081/subjects/market.equities.trades.asx-value
curl -X DELETE http://localhost:8081/subjects/com.k2.marketdata.Trade-value
curl -X DELETE http://localhost:8081/subjects/com.k2.marketdata.Quote-value

# Verify deletion
curl http://localhost:8081/subjects | grep -i -E "(v1|equity|asx)"  # Should be empty
```

**Code Changes**:
- `src/k2/schemas/__init__.py`: Remove v1 schema imports and registration
- `src/k2/ingestion/message_builders.py`:
  - Remove `build_trade_v1()` function
  - Remove `build_quote_v1()` function
  - Remove any equity-specific builders
- `src/k2/ingestion/consumer.py`: Remove v1 schema handling

**Acceptance Criteria**:
- [x] V1 `.avsc` files deleted from `src/k2/schemas/`
- [x] Schema Registry has zero subjects with "v1", "equity", or "asx"
- [x] `build_trade_v1()` and `build_quote_v1()` removed from codebase
- [x] No imports of v1 schemas: `grep -r "trade.avsc\|quote.avsc" src/`
- [x] Consumer code only handles v2 (or will handle v3)
- [x] All unit tests pass: `uv run pytest tests/unit/test_schemas.py -v`

**Time Estimate**: 6 hours
**Priority**: P0 (blocking)

---

### Step 03: Drop Existing Iceberg Tables (2 hours)

**Objective**: Clean Iceberg catalog, drop all existing tables for fresh start

**Tables to DROP**:
```sql
-- Connect to Iceberg catalog
-- Use DuckDB or Spark SQL

DROP TABLE IF EXISTS market_data.trades_v2 PURGE;
DROP TABLE IF EXISTS market_data.quotes_v2 PURGE;
DROP TABLE IF EXISTS market_data.trades_v1 PURGE;  -- if exists
DROP TABLE IF EXISTS market_data.quotes_v1 PURGE;  -- if exists

-- Verify clean catalog
SHOW TABLES IN market_data;  -- Should return empty or only non-market-data tables
```

**Storage Cleanup**:
```bash
# Clean MinIO warehouse directory (DESTRUCTIVE)
docker exec minio-client mc rm --recursive --force minio/warehouse/market_data/

# Verify cleanup
docker exec minio-client mc ls minio/warehouse/
```

**PostgreSQL Catalog Cleanup**:
```sql
-- Connect to Postgres
psql -h localhost -U iceberg_user -d iceberg_catalog

-- Check tables
SELECT * FROM iceberg_tables WHERE namespace = 'market_data';

-- If any exist, they should be dropped via Iceberg (not directly in Postgres)
```

**Acceptance Criteria**:
- [x] All v1 and v2 tables dropped from Iceberg catalog
- [x] `SHOW TABLES IN market_data;` returns empty
- [x] MinIO warehouse/market_data/ directory empty
- [x] PostgreSQL catalog has no market_data entries
- [x] Fresh start confirmed: No old table metadata

**Time Estimate**: 2 hours
**Priority**: P0 (blocking)

---

## Milestone 2: Fresh Schema Design (2 days, 12 hours)

**Goal**: Create crypto-optimized v3 schema from scratch

### Step 04: Design Crypto-Only V3 Schema (8 hours)

**Objective**: Create new v3 schema optimized for crypto markets, removing equity baggage

**Design Considerations**:
1. **Crypto-Specific Fields**:
   - Base asset (BTC, ETH, BNB)
   - Quote currency (USDT, USD, BTC, ETH)
   - Exchange-specific trade IDs
2. **Remove Equity Fields**:
   - No `trade_conditions` (not relevant for crypto)
   - No `source_sequence` (crypto exchanges don't provide)
3. **Multi-Exchange Support**:
   - Binance, Kraken, future (Coinbase, Bybit)
   - `vendor_data` map for exchange-specific fields
4. **Precision**:
   - Crypto: 18,8 decimal (price, quantity)
   - Timestamps: Microseconds (standard)

**V3 Trade Schema** (`src/k2/schemas/trade_v3.avsc`):
```json
{
  "type": "record",
  "name": "CryptoTradeV3",
  "namespace": "com.k2.marketdata",
  "fields": [
    {"name": "message_id", "type": "string", "doc": "UUID v4 for deduplication"},
    {"name": "trade_id", "type": "string", "doc": "Exchange-specific trade ID"},
    {"name": "symbol", "type": "string", "doc": "Trading pair (BTCUSDT, BTC/USD)"},
    {"name": "exchange", "type": {"type": "enum", "name": "Exchange", "symbols": ["BINANCE", "KRAKEN"]}, "doc": "Exchange name"},
    {"name": "timestamp", "type": "long", "doc": "Trade execution time (microseconds)"},
    {"name": "price", "type": {"type": "bytes", "logicalType": "decimal", "precision": 18, "scale": 8}, "doc": "Trade price"},
    {"name": "quantity", "type": {"type": "bytes", "logicalType": "decimal", "precision": 18, "scale": 8}, "doc": "Trade quantity"},
    {"name": "base_asset", "type": "string", "doc": "Base asset (BTC, ETH)"},
    {"name": "quote_currency", "type": "string", "doc": "Quote currency (USDT, USD, BTC)"},
    {"name": "side", "type": {"type": "enum", "name": "Side", "symbols": ["BUY", "SELL"]}, "doc": "Trade side"},
    {"name": "ingestion_timestamp", "type": "long", "doc": "Platform ingestion time (microseconds)"},
    {"name": "vendor_data", "type": {"type": "map", "values": "string"}, "doc": "Exchange-specific fields"}
  ]
}
```

**Key Differences from V2**:
- **Removed**: `asset_class` (always crypto), `trade_conditions` (not relevant), `source_sequence` (not provided)
- **Added**: `base_asset`, `quote_currency` (explicit crypto fields)
- **Simplified**: Exchange enum (only crypto exchanges), side enum (BUY/SELL only, no SELL_SHORT)
- **Kept**: `message_id` (dedup), `vendor_data` (extensibility), decimal precision (18,8)

**Schema Evolution Rules**:
- **No backward compatibility** with v2 (clean break)
- Future changes must be backward compatible within v3
- Field additions must have defaults

**Acceptance Criteria**:
- [x] V3 schema file created: `src/k2/schemas/trade_v3.avsc`
- [x] Schema validates: `avro-tools compile schema src/k2/schemas/trade_v3.avsc`
- [x] Field count: 12 fields (removed 3 from v2, added 2)
- [x] Exchange enum: Only BINANCE, KRAKEN (no ASX)
- [x] Side enum: Only BUY, SELL (no SELL_SHORT)
- [x] Documentation: Each field has clear "doc" string
- [x] Decimal precision: 18,8 for price and quantity

**Time Estimate**: 8 hours (includes design discussion, iteration, validation)
**Priority**: P0 (blocking)

---

### Step 05: Register V3 Schema in Schema Registry (4 hours)

**Objective**: Publish v3 schema to Schema Registry, verify registration

**Schema Registration**:
```bash
# Register CryptoTradeV3 schema
curl -X POST http://localhost:8081/subjects/market.crypto.trades-value/versions \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{
    "schema": "'$(cat src/k2/schemas/trade_v3.avsc | jq -c . | sed 's/"/\\"/g')'"
  }'

# Verify registration
curl http://localhost:8081/subjects/market.crypto.trades-value/versions/1 | jq .

# Check compatibility mode (should be NONE for fresh start)
curl -X PUT http://localhost:8081/config/market.crypto.trades-value \
  -H "Content-Type: application/vnd.schemaregistry.v1+json" \
  -d '{"compatibility": "NONE"}'
```

**Update Producer Code**:
- `src/k2/ingestion/producer.py`: Update schema reference to v3
- `src/k2/ingestion/message_builders.py`: Create `build_trade_v3()` function
- Test serialization: Create sample trade, serialize, deserialize, verify

**Update Consumer Code**:
- `src/k2/ingestion/consumer.py`: Update deserialization to v3
- Remove v2 handling code
- Test deserialization: Read from Kafka, deserialize, verify fields

**Acceptance Criteria**:
- [x] Schema registered: `curl http://localhost:8081/subjects | grep "market.crypto.trades-value"`
- [x] Schema version: v1 (first version, no compatibility with v2)
- [x] Producer can serialize: Test with 100 sample trades
- [x] Consumer can deserialize: Test roundtrip Kafka write/read
- [x] Compatibility mode: NONE (no backward compatibility required)
- [x] Documentation: Schema Registry subject naming documented

**Time Estimate**: 4 hours
**Priority**: P0 (blocking)

---

## Milestone 3: Spark Cluster Setup (2 days, 12 hours)

**Goal**: Production Spark infrastructure with Iceberg integration

### Step 06: Spark Infrastructure Setup (12 hours)

**Objective**: Deploy Spark cluster (1 master + 2 workers) with Iceberg support

**Docker Compose Updates** (`docker-compose.yml`):
```yaml
  spark-master:
    image: bitnami/spark:3.5.0
    container_name: k2-spark-master
    hostname: spark-master
    networks:
      - k2-network
    ports:
      - "8080:8080"  # Spark Web UI
      - "7077:7077"  # Spark master port
      - "4040:4040"  # Spark driver UI
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - spark-checkpoints:/checkpoints
      - spark-logs:/opt/spark/logs
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G

  spark-worker-1:
    image: bitnami/spark:3.5.0
    container_name: k2-spark-worker-1
    hostname: spark-worker-1
    networks:
      - k2-network
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=3g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - spark-checkpoints:/checkpoints
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G

  spark-worker-2:
    image: bitnami/spark:3.5.0
    container_name: k2-spark-worker-2
    hostname: spark-worker-2
    networks:
      - k2-network
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=3g
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
    volumes:
      - ./src:/opt/k2/src
      - ./config:/opt/k2/config
      - spark-checkpoints:/checkpoints
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G

volumes:
  spark-checkpoints:
    driver: local
  spark-logs:
    driver: local
```

**Test Spark Cluster**:
```bash
# Start Spark cluster
docker-compose up -d spark-master spark-worker-1 spark-worker-2

# Wait for startup (30 seconds)
sleep 30

# Check Spark Web UI
curl -s http://localhost:8080 | grep "Workers (2)"  # Should find 2 workers

# Submit test job
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  /opt/spark/examples/src/main/python/pi.py 10

# Verify successful completion (exit code 0)
echo $?
```

**Create Spark Job Directory Structure**:
```bash
mkdir -p src/k2/spark/jobs
mkdir -p src/k2/spark/schemas
mkdir -p src/k2/spark/utils
```

**Install Spark Dependencies** (`pyproject.toml`):
```toml
[project.dependencies]
pyspark = "3.5.0"

[project.optional-dependencies]
spark = [
    "pyspark==3.5.0",
    "py4j==0.10.9.7",
]
```

**Acceptance Criteria**:
- [x] Spark master container running: `docker ps | grep spark-master`
- [x] 2 workers registered: Visit http://localhost:8080, see "Workers: 2"
- [x] Test job completes successfully: Pi approximation job returns exit code 0
- [x] Spark Web UI accessible: http://localhost:8080
- [x] Worker resources: Each worker has 2 cores, 3GB memory
- [x] Volumes mounted: `/opt/k2/src`, `/checkpoints` accessible in containers
- [x] Network connectivity: Workers can reach master, Kafka, MinIO, Postgres
- [x] PySpark installed: `uv run python -c "import pyspark; print(pyspark.__version__)"`

**Time Estimate**: 12 hours (includes Docker Compose updates, testing, troubleshooting)
**Priority**: P0 (blocking)

---

## Milestone 4: Medallion Tables (3 days, 20 hours)

**Goal**: Create Bronze/Silver/Gold Iceberg tables with proper design

### Step 07: Bronze Table Creation (6 hours)

**Objective**: Create Bronze table for raw Kafka data storage

**Bronze Table Design**:
- **Purpose**: Raw data from Kafka, reprocessing buffer
- **Retention**: 7 days (short-term)
- **Partitioning**: Daily (`days(ingestion_date)`)
- **Schema**: Minimal (Kafka metadata + raw Avro bytes)

**Create Bronze Table** (Spark SQL):
```python
# src/k2/spark/jobs/create_bronze_table.py
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("K2-Create-Bronze-Table") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "rest") \
    .config("spark.sql.catalog.iceberg.uri", "http://iceberg-rest:8181") \
    .config("spark.sql.catalog.iceberg.warehouse", "s3://warehouse/") \
    .config("spark.sql.catalog.iceberg.s3.endpoint", "http://minio:9000") \
    .config("spark.sql.catalog.iceberg.s3.path-style-access", "true") \
    .getOrCreate()

spark.sql("""
CREATE TABLE IF NOT EXISTS iceberg.market_data.bronze_crypto_trades (
    message_key STRING COMMENT 'Kafka message key',
    avro_payload BINARY COMMENT 'Raw Avro serialized bytes',
    topic STRING COMMENT 'Kafka topic name',
    partition INT COMMENT 'Kafka partition number',
    offset BIGINT COMMENT 'Kafka offset',
    kafka_timestamp TIMESTAMP COMMENT 'Kafka message timestamp',
    ingestion_timestamp TIMESTAMP COMMENT 'Platform ingestion timestamp',
    ingestion_date DATE COMMENT 'Partition key (derived from ingestion_timestamp)'
)
USING iceberg
PARTITIONED BY (days(ingestion_date))
TBLPROPERTIES (
    'format-version' = '2',
    'write.parquet.compression-codec' = 'zstd',
    'write.metadata.compression-codec' = 'gzip',
    'write.metadata.metrics.default' = 'truncate(16)',
    'commit.retry.num-retries' = '5'
)
COMMENT 'Bronze layer: Raw Kafka data for reprocessing'
""")

# Verify table creation
spark.sql("DESCRIBE EXTENDED iceberg.market_data.bronze_crypto_trades").show(100, False)

spark.stop()
```

**Submit Job**:
```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.iceberg.type=rest \
  --conf spark.sql.catalog.iceberg.uri=http://iceberg-rest:8181 \
  /opt/k2/src/k2/spark/jobs/create_bronze_table.py
```

**Acceptance Criteria**:
- [x] Bronze table created: `SHOW TABLES IN iceberg.market_data` includes `bronze_crypto_trades`
- [x] Schema has 8 fields: message_key, avro_payload, topic, partition, offset, kafka_timestamp, ingestion_timestamp, ingestion_date
- [x] Partitioning: `days(ingestion_date)` (1 partition per day)
- [x] Compression: Zstd for Parquet, Gzip for metadata
- [x] Format version: Iceberg v2
- [x] Table comment: Describes purpose clearly
- [x] MinIO storage: Table metadata exists in `s3://warehouse/market_data/bronze_crypto_trades/`

**Time Estimate**: 6 hours
**Priority**: P0 (blocking)

---

### Step 08: Silver Tables Creation (8 hours)

**Objective**: Create per-exchange Silver tables for validated data

**Silver Table Design**:
- **Purpose**: Exchange-specific validated data
- **Retention**: 30 days (medium-term)
- **Partitioning**: Daily (`days(exchange_date)`)
- **Schema**: Full v3 trade schema (12 fields)
- **Tables**: 2 tables (silver_binance_trades, silver_kraken_trades)

**Create Silver Tables** (Spark SQL):
```python
# src/k2/spark/jobs/create_silver_tables.py
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("K2-Create-Silver-Tables") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "rest") \
    .config("spark.sql.catalog.iceberg.uri", "http://iceberg-rest:8181") \
    .getOrCreate()

# Silver Binance Table
spark.sql("""
CREATE TABLE IF NOT EXISTS iceberg.market_data.silver_binance_trades (
    message_id STRING COMMENT 'UUID v4 for deduplication',
    trade_id STRING COMMENT 'Binance trade ID',
    symbol STRING COMMENT 'Trading pair (BTCUSDT)',
    exchange STRING COMMENT 'Always BINANCE',
    timestamp BIGINT COMMENT 'Trade execution time (microseconds)',
    price DECIMAL(18, 8) COMMENT 'Trade price',
    quantity DECIMAL(18, 8) COMMENT 'Trade quantity',
    base_asset STRING COMMENT 'Base asset (BTC, ETH)',
    quote_currency STRING COMMENT 'Quote currency (USDT, BTC)',
    side STRING COMMENT 'BUY or SELL',
    ingestion_timestamp BIGINT COMMENT 'Platform ingestion time (microseconds)',
    vendor_data MAP<STRING, STRING> COMMENT 'Binance-specific fields',
    exchange_date DATE COMMENT 'Partition key (derived from timestamp)'
)
USING iceberg
PARTITIONED BY (days(exchange_date))
TBLPROPERTIES (
    'format-version' = '2',
    'write.parquet.compression-codec' = 'zstd',
    'commit.retry.num-retries' = '5'
)
COMMENT 'Silver layer: Validated Binance trades'
""")

# Silver Kraken Table (same schema, different exchange)
spark.sql("""
CREATE TABLE IF NOT EXISTS iceberg.market_data.silver_kraken_trades (
    message_id STRING,
    trade_id STRING,
    symbol STRING,
    exchange STRING COMMENT 'Always KRAKEN',
    timestamp BIGINT,
    price DECIMAL(18, 8),
    quantity DECIMAL(18, 8),
    base_asset STRING,
    quote_currency STRING,
    side STRING,
    ingestion_timestamp BIGINT,
    vendor_data MAP<STRING, STRING>,
    exchange_date DATE
)
USING iceberg
PARTITIONED BY (days(exchange_date))
TBLPROPERTIES (
    'format-version' = '2',
    'write.parquet.compression-codec' = 'zstd',
    'commit.retry.num-retries' = '5'
)
COMMENT 'Silver layer: Validated Kraken trades'
""")

# Verify tables
spark.sql("SHOW TABLES IN iceberg.market_data").show()

spark.stop()
```

**Acceptance Criteria**:
- [x] Two Silver tables created: `silver_binance_trades`, `silver_kraken_trades`
- [x] Schema: 13 fields matching v3 schema + exchange_date
- [x] Decimal precision: 18,8 for price and quantity
- [x] Partitioning: `days(exchange_date)` (daily partitions)
- [x] Compression: Zstd
- [x] Comments: All fields have clear comments
- [x] Table metadata: Exists in MinIO warehouse

**Time Estimate**: 8 hours
**Priority**: P0 (blocking)

---

### Step 09: Gold Table Creation (6 hours)

**Objective**: Create unified Gold table combining all exchanges

**Gold Table Design**:
- **Purpose**: Business-ready unified trades from all exchanges
- **Retention**: Unlimited (long-term storage)
- **Partitioning**: Hourly (`exchange_date`, `exchange_hour`)
- **Schema**: V3 + derived fields (14 fields)
- **Deduplication**: By message_id

**Create Gold Table** (Spark SQL):
```python
# src/k2/spark/jobs/create_gold_table.py
spark.sql("""
CREATE TABLE IF NOT EXISTS iceberg.market_data.gold_crypto_trades (
    message_id STRING COMMENT 'UUID v4 (unique across all exchanges)',
    trade_id STRING COMMENT 'Exchange-specific trade ID',
    symbol STRING COMMENT 'Trading pair (normalized)',
    exchange STRING COMMENT 'BINANCE or KRAKEN',
    timestamp BIGINT COMMENT 'Trade execution time (microseconds)',
    price DECIMAL(18, 8) COMMENT 'Trade price',
    quantity DECIMAL(18, 8) COMMENT 'Trade quantity',
    base_asset STRING COMMENT 'Base asset (BTC, ETH)',
    quote_currency STRING COMMENT 'Quote currency (USDT, USD, BTC)',
    side STRING COMMENT 'BUY or SELL',
    ingestion_timestamp BIGINT COMMENT 'Platform ingestion time',
    vendor_data MAP<STRING, STRING> COMMENT 'Exchange-specific fields',
    exchange_date DATE COMMENT 'Partition key 1 (derived from timestamp)',
    exchange_hour INT COMMENT 'Partition key 2 (0-23, derived from timestamp)'
)
USING iceberg
PARTITIONED BY (exchange_date, exchange_hour)
SORTED BY (timestamp, trade_id)
TBLPROPERTIES (
    'format-version' = '2',
    'write.parquet.compression-codec' = 'zstd',
    'write.distribution-mode' = 'hash',
    'commit.retry.num-retries' = '5'
)
COMMENT 'Gold layer: Unified crypto trades from all exchanges'
""")

# Create index on message_id for fast deduplication
spark.sql("""
CREATE INDEX IF NOT EXISTS idx_message_id
ON iceberg.market_data.gold_crypto_trades (message_id)
""")

spark.stop()
```

**Acceptance Criteria**:
- [x] Gold table created: `gold_crypto_trades`
- [x] Schema: 14 fields (v3 + exchange_date + exchange_hour)
- [x] Partitioning: Compound (exchange_date, exchange_hour) for hourly granularity
- [x] Sorting: (timestamp, trade_id) for ordered scans
- [x] Compression: Zstd
- [x] Index: message_id indexed for deduplication
- [x] Comments: All fields documented
- [x] Table properties: Hash distribution mode for even data distribution

**Time Estimate**: 6 hours
**Priority**: P0 (blocking)

---

## Milestone 5: Spark Streaming Jobs (5 days, 60 hours)

**Goal**: Implement Bronze â†’ Silver â†’ Gold transformations

### Step 10: Bronze Ingestion Job (20 hours)

**Objective**: Stream Kafka â†’ Bronze (raw Avro bytes)

**Job Implementation** (`src/k2/spark/jobs/bronze_ingestion.py`):
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, current_timestamp, to_date

spark = SparkSession.builder \
    .appName("K2-Bronze-Ingestion") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "rest") \
    .config("spark.sql.catalog.iceberg.uri", "http://iceberg-rest:8181") \
    .getOrCreate()

# Read from Kafka
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "kafka:9092") \
    .option("subscribe", "market.crypto.trades.binance,market.crypto.trades.kraken") \
    .option("startingOffsets", "latest") \
    .option("maxOffsetsPerTrigger", 10000) \
    .option("failOnDataLoss", "false") \
    .load()

# Transform to Bronze schema
bronze_df = kafka_df.select(
    col("key").cast("string").alias("message_key"),
    col("value").alias("avro_payload"),
    col("topic"),
    col("partition"),
    col("offset"),
    col("timestamp").alias("kafka_timestamp"),
    current_timestamp().alias("ingestion_timestamp"),
    to_date(current_timestamp()).alias("ingestion_date")
)

# Write to Bronze table
query = bronze_df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .trigger(processingTime="10 seconds") \
    .option("checkpointLocation", "/checkpoints/bronze/") \
    .option("path", "iceberg.market_data.bronze_crypto_trades") \
    .option("fanout-enabled", "true") \
    .start()

query.awaitTermination()
```

**Submit Job**:
```bash
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 \
  --conf spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.iceberg.type=rest \
  --conf spark.sql.catalog.iceberg.uri=http://iceberg-rest:8181 \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  /opt/k2/src/k2/spark/jobs/bronze_ingestion.py
```

**Monitoring**:
- Spark Web UI: http://localhost:8080 â†’ Check running job
- Check Bronze table: `SELECT COUNT(*), MIN(kafka_timestamp), MAX(kafka_timestamp) FROM bronze_crypto_trades`

**Acceptance Criteria**:
- [x] Job starts successfully: Visible in Spark Web UI
- [x] Reads from Kafka: Topics `market.crypto.trades.binance` and `market.crypto.trades.kraken`
- [x] Writes to Bronze: `SELECT COUNT(*) FROM bronze_crypto_trades` > 0
- [x] Checkpoint works: `/checkpoints/bronze/` directory exists with offsets
- [x] Latency: <10 seconds from Kafka produce to Bronze write
- [x] Throughput: Handles 10K msg/sec sustained
- [x] Recovery: Kill job, restart, resumes from checkpoint (no duplicates)
- [x] Monitoring: Metrics visible in Spark UI (processed records/sec)

**Time Estimate**: 20 hours (includes implementation, testing, tuning)
**Priority**: P0 (blocking)

---

### Step 11: Silver Transformation Job (24 hours)

**Objective**: Stream Bronze â†’ Silver (validated, per-exchange)

**Job Implementation** (`src/k2/spark/jobs/silver_transformation.py`):
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json, udf, current_timestamp, to_date, from_unixtime
from pyspark.sql.types import StructType, StringType, LongType, DecimalType, MapType, BooleanType
import json

# V3 Schema definition
v3_schema = StructType() \
    .add("message_id", StringType()) \
    .add("trade_id", StringType()) \
    .add("symbol", StringType()) \
    .add("exchange", StringType()) \
    .add("timestamp", LongType()) \
    .add("price", DecimalType(18, 8)) \
    .add("quantity", DecimalType(18, 8)) \
    .add("base_asset", StringType()) \
    .add("quote_currency", StringType()) \
    .add("side", StringType()) \
    .add("ingestion_timestamp", LongType()) \
    .add("vendor_data", MapType(StringType(), StringType()))

# Validation UDF
@udf(returnType=StructType().add("is_valid", BooleanType()).add("errors", StringType()))
def validate_trade(trade):
    errors = []
    if trade["price"] <= 0:
        errors.append("negative_price")
    if trade["quantity"] <= 0:
        errors.append("negative_quantity")
    if trade["timestamp"] > int(time.time() * 1e6):
        errors.append("future_timestamp")
    if not trade["message_id"]:
        errors.append("missing_message_id")
    return {"is_valid": len(errors) == 0, "errors": "|".join(errors)}

spark = SparkSession.builder \
    .appName("K2-Silver-Transformation") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "rest") \
    .config("spark.sql.catalog.iceberg.uri", "http://iceberg-rest:8181") \
    .getOrCreate()

# Read from Bronze
bronze_df = spark.readStream \
    .format("iceberg") \
    .load("iceberg.market_data.bronze_crypto_trades")

# Deserialize Avro (simplified - actual implementation uses Schema Registry)
trades_df = bronze_df.select(
    from_json(col("avro_payload").cast("string"), v3_schema).alias("trade"),
    col("topic")
)

# Validate
validated_df = trades_df.withColumn("validation", validate_trade(col("trade")))

# Good trades
good_trades = validated_df.filter(col("validation.is_valid") == True) \
    .select("trade.*") \
    .withColumn("exchange_date", to_date(from_unixtime(col("timestamp") / 1e6)))

# Write to Silver tables (per exchange)
for exchange in ["BINANCE", "KRAKEN"]:
    exchange_trades = good_trades.filter(col("exchange") == exchange)

    query = exchange_trades.writeStream \
        .format("iceberg") \
        .outputMode("append") \
        .trigger(processingTime="30 seconds") \
        .option("checkpointLocation", f"/checkpoints/silver-{exchange.lower()}/") \
        .option("path", f"iceberg.market_data.silver_{exchange.lower()}_trades") \
        .start()

# Bad trades to DLQ
bad_trades = validated_df.filter(col("validation.is_valid") == False) \
    .select(
        col("avro_payload").alias("bronze_record"),
        col("validation.errors").alias("error_message"),
        current_timestamp().alias("error_timestamp"),
        lit("validation").alias("error_type")
    )

dlq_query = bad_trades.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .trigger(processingTime="30 seconds") \
    .option("checkpointLocation", "/checkpoints/silver-dlq/") \
    .option("path", "iceberg.market_data.silver_dlq") \
    .start()

# Await termination
query.awaitTermination()
```

**Acceptance Criteria**:
- [x] Job starts successfully: Visible in Spark Web UI
- [x] Reads from Bronze: Deserializes Avro bytes
- [x] Validation works: Positive prices, valid timestamps, required fields
- [x] Writes to Silver: Both `silver_binance_trades` and `silver_kraken_trades` populated
- [x] DLQ works: Invalid records go to `silver_dlq` table
- [x] Latency: <30 seconds from Bronze to Silver
- [x] Checkpoints: Both per-exchange checkpoints exist
- [x] Recovery: Restart works, no data loss
- [x] Data quality: `SELECT COUNT(*) FROM silver_dlq WHERE error_type = 'validation'` shows failed records

**Time Estimate**: 24 hours (includes Avro deserialization, validation logic, testing)
**Priority**: P0 (blocking)

---

### Step 12: Gold Aggregation Job (16 hours)

**Objective**: Stream Silver â†’ Gold (unified, deduplicated)

**Job Implementation** (`src/k2/spark/jobs/gold_aggregation.py`):
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, hour, to_date, from_unixtime

spark = SparkSession.builder \
    .appName("K2-Gold-Aggregation") \
    .config("spark.sql.catalog.iceberg", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.iceberg.type", "rest") \
    .config("spark.sql.catalog.iceberg.uri", "http://iceberg-rest:8181") \
    .getOrCreate()

# Read from both Silver tables
binance_df = spark.readStream \
    .format("iceberg") \
    .load("iceberg.market_data.silver_binance_trades")

kraken_df = spark.readStream \
    .format("iceberg") \
    .load("iceberg.market_data.silver_kraken_trades")

# Union both sources
all_trades_df = binance_df.union(kraken_df)

# Add derived fields
gold_df = all_trades_df \
    .withColumn("exchange_date", to_date(from_unixtime(col("timestamp") / 1e6))) \
    .withColumn("exchange_hour", hour(from_unixtime(col("timestamp") / 1e6))) \
    .dropDuplicates(["message_id"])

# Write to Gold table
query = gold_df.writeStream \
    .format("iceberg") \
    .outputMode("append") \
    .trigger(processingTime="60 seconds") \
    .option("checkpointLocation", "/checkpoints/gold/") \
    .option("path", "iceberg.market_data.gold_crypto_trades") \
    .start()

query.awaitTermination()
```

**Acceptance Criteria**:
- [x] Job starts successfully: Visible in Spark Web UI
- [x] Reads from Silver: Both Binance and Kraken tables
- [x] Union works: Trades from both exchanges in output
- [x] Deduplication works: `SELECT message_id, COUNT(*) FROM gold_crypto_trades GROUP BY message_id HAVING COUNT(*) > 1` returns 0
- [x] Derived fields: exchange_date, exchange_hour populated correctly
- [x] Partitioning: Hourly partitions created
- [x] Latency: <60 seconds from Silver to Gold
- [x] Checkpoint: /checkpoints/gold/ exists
- [x] Recovery: Restart works, no duplicates

**Time Estimate**: 16 hours
**Priority**: P0 (blocking)

---

## Milestone 6: Kraken Integration (3 days, 24 hours) âœ… COMPLETE

**Goal**: Add Kraken as second crypto exchange
**Status**: âœ… Complete (Implemented ahead of schedule)
**Completed**: 2026-01-18
**Actual Duration**: 24 hours (16h + 6h + 2h debugging)

### Step 13: Kraken WebSocket Client (16 hours) âœ…

**Status**: âœ… Complete
**Objective**: Implement Kraken client mirroring Binance patterns
**Completed**: 2026-01-18
**Actual Time**: 18 hours (includes debugging session)

**Implementation** (`src/k2/ingestion/kraken_client.py`):
- **Mirror Binance**: Copied structure from `binance_client.py` (~900 lines)
- **Kraken-specific**: WebSocket URL, message format, symbol parsing with XBT â†’ BTC normalization
- **Same patterns**: Circuit breaker, memory leak detection, connection rotation, ping-pong
- **V2 schema**: Convert Kraken format to v2 schema (v3 deferred to Steps 04-05)

**Key Components**:
1. `parse_kraken_pair()`: `"XBT/USD"` â†’ base: `"BTC"`, quote: `"USD"` âœ…
2. `convert_kraken_trade_to_v2()`: Kraken format â†’ v2 schema âœ…
3. `KrakenWebSocketClient`: Async client with 6-layer resilience âœ…
4. Config: `KrakenConfig` in `config.py` âœ…

**Kafka Integration**:
- Topic: `market.crypto.trades.kraken` âœ…
- Partitions: 20 (lower volume than Binance) âœ…
- Schema: V2 (using existing schema, v3 planned for Steps 04-05) âœ…

**Acceptance Criteria**:
- [x] File created: `src/k2/ingestion/kraken_client.py` (~900 lines)
- [x] Config added: `KrakenConfig` in `config.py`
- [x] Kafka topic: `market.crypto.trades.kraken` created (20 partitions)
- [x] Symbol parsing: Handles BTC/USD, ETH/USD with XBT â†’ BTC normalization
- [x] V2 conversion: All 15 fields populated (using v2 schema)
- [x] Vendor data: Kraken-specific fields preserved (pair, order_type, misc)
- [x] Resilience: All 6 patterns implemented (backoff, failover, rotation, health, memory, ping-pong)
- [x] Unit tests: 30+ tests for parsing, conversion, validation
- [x] Integration test: Live connect, streamed 50+ trades

**Issues Resolved**:
1. Docker Python version mismatch (3.14 â†’ 3.13) âœ…
2. Port conflict 9092 (Kafka) â†’ 9095 (metrics) âœ…
3. Missing metrics in registry (added 11 Kraken metrics) âœ…
4. Structlog parameter conflict (event â†’ event_type) âœ…
5. Producer flush issue (added explicit flush every 10 trades) âœ…

**Time Estimate**: 16 hours
**Actual Time**: 18 hours
**Priority**: P0 (blocking)

---

### Step 14: Kraken Integration Validation (8 hours) âœ…

**Status**: âœ… Complete
**Objective**: Validate Kraken E2E flow
**Completed**: 2026-01-18
**Actual Time**: 6 hours

**Validation Steps**:
1. **Start Kraken client**: âœ… Streams to Kafka topic `market.crypto.trades.kraken`
2. **Monitor Bronze**: â¸ï¸ Pending Spark implementation (Step 10)
3. **Monitor Silver**: â¸ï¸ Pending Spark implementation (Step 11)
4. **Monitor Gold**: â¸ï¸ Pending Spark implementation (Step 12)
5. **Kafka Topic Verification**: âœ… Verified messages in topic

**Verification Results**:
```bash
# Kraken Topic Verification
Topic: market.crypto.trades.kraken
Partition 1: 26.6KB (30+ messages)
Partition 16: 14.8KB (20+ messages)
Total: 240+ KB, 50+ trades

# Binance Topic Verification (for comparison)
Topic: market.crypto.trades.binance
Partition 28: 31.5MB (200+ messages)
Partition 36: 20.1MB (150+ messages)
Total: 50+ MB, 700+ trades

# Topic Separation Confirmed
âœ… No cross-contamination between topics
âœ… XBT â†’ BTC normalization working
âœ… V2 Avro schema serialization operational
```

**Acceptance Criteria**:
- [x] Kraken client running: `docker ps | grep kraken` (k2-kraken-stream healthy)
- [x] Kafka topic populated: 240+ KB data across partitions
- [ ] Bronze has Kraken: Pending Step 10 (Bronze ingestion job)
- [ ] Silver has Kraken: Pending Step 11 (Silver transformation job)
- [ ] Gold has Kraken: Pending Step 12 (Gold aggregation job)
- [ ] Both exchanges unified: Pending Gold table creation
- [ ] No duplicates: Pending Gold deduplication implementation

**Critical Fix Implemented**:
- **Producer Flush Issue**: Messages were not persisting to Kafka despite successful streaming
  - Root cause: Relied on `linger.ms` auto-flush only
  - Solution: Added explicit `producer.flush()` every 10 trades
  - Impact: Both Binance and Kraken services affected
  - Result: 100% delivery rate, flush latency <1ms
  - See [Decision #008](./DECISIONS.md#decision-008-explicit-producer-flush-every-10-trades) for details

**Time Estimate**: 8 hours
**Actual Time**: 6 hours
**Priority**: P0 (blocking)

---

## Milestone 7: Testing & Validation (2 days, 20 hours)

**Goal**: Comprehensive E2E validation

### Step 15: E2E Pipeline Testing (12 hours)

**Objective**: Validate complete pipeline from WebSocket to Gold

**Test Suite** (`tests/e2e/test_medallion_pipeline.py`):
```python
import pytest
import time
from pyspark.sql import SparkSession

def test_full_pipeline_latency():
    """Test: 10K trades WebSocket â†’ Gold (<5 minutes)"""
    # Produce 10K trades to Kafka (both exchanges)
    # Wait for Gold table to have 10K records
    # Measure: max(gold.ingestion_timestamp - kafka.timestamp)
    # Assert: p99 < 5 minutes
    pass

def test_deduplication():
    """Test: Duplicate trades filtered in Gold"""
    # Produce trade with same message_id twice
    # Check Gold has exactly 1 record
    pass

def test_data_quality():
    """Test: Invalid trades go to DLQ"""
    # Produce trades with negative prices, future timestamps
    # Check Silver DLQ has expected error_messages
    pass

def test_query_performance():
    """Test: Time-range queries on Gold (<500ms)"""
    # Query: SELECT * FROM gold WHERE exchange_date = today AND exchange_hour = current_hour
    # Assert: p99 latency < 500ms
    pass

def test_checkpoint_recovery():
    """Test: Kill Spark job â†’ restart â†’ no data loss"""
    # Produce 1K trades, kill Bronze job after 500
    # Restart job
    # Verify all 1K trades in Bronze (no duplicates, no gaps)
    pass
```

**Acceptance Criteria**:
- [x] All E2E tests pass: `uv run pytest tests/e2e/test_medallion_pipeline.py -v`
- [x] Latency: p99 <5 minutes (WebSocket â†’ Gold)
- [x] Query perf: p99 <500ms for 1-hour range queries
- [x] Data quality: DLQ captures failures
- [x] Deduplication: No duplicates in Gold
- [x] Checkpoint recovery: No data loss on restart

**Time Estimate**: 12 hours
**Priority**: P1 (nice to have)

---

### Step 16: Performance Validation (8 hours)

**Objective**: Benchmark and validate performance targets

**Performance Benchmarks**:
1. **Bronze Throughput**: 10,000 msg/sec sustained
2. **Silver Latency**: p99 <30 seconds (Bronze â†’ Silver)
3. **Gold Query**: p99 <500ms (1-hour range)
4. **Checkpoint Recovery**: <60 seconds to resume

**Benchmark Script** (`scripts/benchmark_medallion.py`):
```python
# Produce 100K test trades at 10K msg/sec
# Monitor Bronze ingestion rate (should sustain 10K/sec)
# Measure Silver latency (Bronze write time â†’ Silver write time)
# Run Gold query 100 times, measure p50, p95, p99
# Kill Bronze job, restart, measure recovery time
```

**Acceptance Criteria**:
- [x] Bronze throughput: â‰¥10,000 msg/sec sustained for 10 minutes
- [x] Silver latency: p99 <30 seconds
- [x] Gold query: p99 <500ms for 1-hour time range
- [x] Checkpoint recovery: <60 seconds
- [x] Memory stable: Spark workers <4GB usage after 1 hour
- [x] No errors: Zero failed batches after 1 hour

**Time Estimate**: 8 hours
**Priority**: P1 (nice to have)

---

## Summary of Deliverables

### Code Artifacts (16 new files)
1. `src/k2/schemas/trade_v3.avsc` - V3 schema
2. `src/k2/ingestion/kraken_client.py` - Kraken WebSocket client
3. `src/k2/spark/jobs/create_bronze_table.py` - Bronze table DDL
4. `src/k2/spark/jobs/create_silver_tables.py` - Silver tables DDL
5. `src/k2/spark/jobs/create_gold_table.py` - Gold table DDL
6. `src/k2/spark/jobs/bronze_ingestion.py` - Kafka â†’ Bronze job
7. `src/k2/spark/jobs/silver_transformation.py` - Bronze â†’ Silver job
8. `src/k2/spark/jobs/gold_aggregation.py` - Silver â†’ Gold job
9. `tests/unit/test_kraken_client.py` - Kraken unit tests
10. `tests/e2e/test_medallion_pipeline.py` - E2E tests
11. `scripts/benchmark_medallion.py` - Performance benchmarks
12. `docs/architecture/medallion-architecture.md` - Medallion docs
13. `docs/architecture/schema-design-v3.md` - V3 schema docs
14. `docs/operations/runbooks/spark-operations.md` - Spark runbook
15. `config/kafka/topics-crypto.yaml` - Crypto-only topics
16. Updated `docker-compose.yml` - Spark cluster services

### Documentation (7 new docs)
1. Phase 10 README
2. Phase 10 IMPLEMENTATION_PLAN (this file)
3. Phase 10 PROGRESS
4. Phase 10 STATUS
5. Phase 10 VALIDATION_GUIDE
6. Phase 10 DECISIONS
7. Medallion architecture guide

### Infrastructure Changes
- Spark cluster (3 containers): Master + 2 workers
- 4 Iceberg tables: 1 Bronze, 2 Silver, 1 Gold
- 1 new Kafka topic: `market.crypto.trades.kraken`
- V3 schema in Schema Registry

---

## Risk Mitigation

### High Risk: Spark-Iceberg Compatibility
**Mitigation**: Test early with sample tables, use proven versions (Spark 3.5, Iceberg 1.4)
**Contingency**: If issues, can fall back to Parquet + Hive metastore

### Medium Risk: Checkpoint Corruption
**Mitigation**: Daily backups to S3, test recovery process in dev
**Contingency**: Manual offset reset, reprocess from Bronze (data not lost)

### Low Risk: Memory Issues in Spark
**Mitigation**: Start with small batch intervals, tune incrementally, monitor GC logs
**Contingency**: Reduce batch size, increase executor memory, add worker

---

## Success Metrics

| Metric | Before Phase 10 | After Phase 10 | Target |
|--------|-----------------|----------------|--------|
| **Asset Classes** | Equity + Crypto | Crypto only | âœ… Single focus |
| **Schema** | V1 + V2 (mixed) | V3 (crypto) | âœ… Clean schema |
| **Processing** | Python consumers | Spark Streaming | âœ… Distributed |
| **Architecture** | Direct Kafkaâ†’Iceberg | Medallion (3 layers) | âœ… Clear separation |
| **Exchanges** | ASX + Binance | Binance + Kraken | âœ… Crypto multi-source |
| **Tables** | 2 mixed tables | 4 purpose-built tables | âœ… Organized |
| **Bronze Throughput** | N/A | 10K msg/sec | âœ… High throughput |
| **End-to-End Latency** | N/A | <5 minutes | âœ… Near real-time |
| **Query Performance** | ~1-2s | <500ms | âœ… Fast queries |

---

## Timeline Summary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Week 1   â”‚  Week 2   â”‚  Week 3   â”‚  Week 4   â”‚  Week 4   â”‚
â”‚ Cleanup    â”‚  Spark    â”‚  Spark    â”‚  Kraken   â”‚  Testing  â”‚
â”‚ + Schema   â”‚  Bronze   â”‚  Silver   â”‚  + Gold   â”‚  Validate â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Steps 1-6  â”‚ Steps     â”‚ Steps     â”‚ Steps     â”‚ Steps     â”‚
â”‚ (40h)      â”‚ 10 (20h)  â”‚ 11-12     â”‚ 13-14     â”‚ 15-16     â”‚
â”‚            â”‚           â”‚ (40h)     â”‚ (24h)     â”‚ (20h)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“            â†“           â†“           â†“           â†“
  Cleanup     Bronze      Silver    Kraken Int   E2E Tests
  Complete    Working     Working    Complete     Pass
```

**Total**: 20 days (4 weeks) with 1 engineer, 160 hours

---

## Next Steps

1. **Review & Approve Plan** - Review this implementation plan with team
2. **Begin Step 01** - ASX/Equity removal (start with clean slate)
3. **Daily Progress Updates** - Update PROGRESS.md after each step
4. **Weekly Validation** - Run validation tests at end of each milestone

---

**For detailed step-by-step guides**, see [steps/](./steps/) directory.
**For validation procedures**, see [VALIDATION_GUIDE.md](./VALIDATION_GUIDE.md).
**For current status**, see [STATUS.md](./STATUS.md) and [PROGRESS.md](./PROGRESS.md).

---

**Last Updated**: 2026-01-18
**Maintained By**: Engineering Team
