# End-of-Day Handoff — 2026-02-18

**Date:** 2026-02-18
**Branch:** `phase-5-prefect-iceberg-offload`
**Engineer:** Staff Data Engineer (Claude)
**Status:** Full medallion pipeline operational, tests green

---

## TL;DR

✅ **Coinbase Advanced Trade added as 3rd exchange** (all layers: feed → ClickHouse → Iceberg)
✅ **Full offload pipeline running**: Bronze ×3 → Silver → Gold ×6 (10/10 tables clean)
✅ **Kotlin unit tests: 16/16 passing** (compiled, ran, all green)
✅ **Schema files synced** to live ClickHouse v2 MVs (Binance, Kraken, Coinbase silver)
✅ **OHLCV watermark bug fixed**: `sequence_col: window_start` → `trade_count`
✅ **Spark container rebuilt** with correct env vars (`PREFECT_DB_*` now baked in)

**Next session**: Prefect 15-min schedule deployment → Spark daily maintenance → warm-cold validation → tag `v2-phase-5-complete`

---

## What Was Accomplished

### 1. Coinbase Exchange Integration (carried over from previous session)

**Root cause of "no Coinbase prices"**: Docker bind mount inode staleness. When `Edit` tool atomically replaces a file (write-then-rename), the container's bind mount still points to the old inode. The container was reading the old `instruments.yaml` (without Coinbase section) and subscribing with Binance symbols that Coinbase rejected.

**Fix**: `docker compose up -d --force-recreate --no-deps feed-handler-coinbase`

**Important**: `docker restart` does NOT fix inode staleness. Must use compose `--force-recreate`.

### 2. Coinbase v2 Bronze Schema

The original Coinbase bronze used exchange-native raw strings (`trade_id String`, `product_id String`, `price String`). Aligned to **v2 pattern** matching Binance/Kraken:

```sql
CREATE TABLE IF NOT EXISTS k2.bronze_trades_coinbase (
    exchange_timestamp  DateTime64(3),
    sequence_number     UInt64,
    symbol              String,   -- "BTCUSD" (dash removed)
    price               Decimal(18, 8),
    quantity            Decimal(18, 8),
    quote_volume        Decimal(18, 8),
    event_time          DateTime64(3),
    kafka_offset        UInt64,
    kafka_partition     UInt16,
    ingestion_timestamp DateTime DEFAULT now()
) ENGINE = MergeTree() ...
```

Files updated: `docker/clickhouse/schema/11-bronze-coinbase.sql`, `12-silver-coinbase.sql`

### 3. Silver and Gold Layers

Created all ClickHouse layers that were missing from the live instance:
- `k2.silver_trades` (MergeTree, all 3 exchanges via MVs)
- `k2.ohlcv_{1m,5m,15m,30m,1h,1d}` (SummingMergeTree, reading from `silver_trades`)
- 3 silver MVs: `bronze_{binance,kraken,coinbase}_to_silver_mv`
- 6 OHLCV MVs, one per timeframe

**TTL fix**: `TTL toDateTime(timestamp) + INTERVAL 30 DAY` — `DateTime64` must be cast with `toDateTime()` in TTL clauses.

### 4. Iceberg Cold Storage (9 tables)

All 9 Iceberg tables in MinIO `cold.*` namespace:

| Layer | Table | Rows offloaded |
|-------|-------|---------------|
| Bronze | `bronze_trades_binance` | 747,783 |
| Bronze | `bronze_trades_kraken` | 8,728 |
| Bronze | `bronze_trades_coinbase` | 20,983 |
| Silver | `silver_trades` | 538,641 |
| Gold | `gold_ohlcv_1m` | 2,251 |
| Gold | `gold_ohlcv_5m` | 534 |
| Gold | `gold_ohlcv_15m` | 195 |
| Gold | `gold_ohlcv_30m` | 104 |
| Gold | `gold_ohlcv_1h` | 69 |
| Gold | `gold_ohlcv_1d` | 61 |

### 5. Spark JDBC Fixes (Iceberg offload)

Three ClickHouse → Spark JDBC incompatibilities discovered and fixed:

| ClickHouse type | JDBC error | Fix |
|-----------------|-----------|-----|
| `Array(String)` | `UNRECOGNIZED_SQL_TYPE ARRAY` | Drop column from Iceberg; exclude from SELECT |
| `Map(String,String)` | `UNRECOGNIZED_SQL_TYPE` | Same as above |
| `DateTime64(6, 'UTC')` | `TIMESTAMP_WITH_TIMEZONE` | `ALTER TABLE MODIFY COLUMN timestamp DateTime64(6)` |
| Nullable(UInt64) all NULL | `null value violates NOT NULL` (watermark) | Change `sequence_col` to non-nullable column |

Silver columns excluded from Iceberg (dropped from `cold.silver_trades`): `trade_conditions`, `vendor_data`, `validation_errors`

### 6. Offload Flow Updates (`iceberg_offload_flow.py`)

- OHLCV `sequence_col`: `window_start` (timestamp) → `trade_count` (UInt32/BIGINT-compatible)
- Enabled full pipeline in `iceberg_offload_main()`: Bronze → Silver → Gold (was Bronze-only with TODO)
- Added `_SILVER_TRADES_COLUMNS` constant (explicit column list, excludes Array/Map types)
- Version bumped to `3.1.0`

### 7. Kotlin Unit Tests

**Results**: 16/16 PASSED, 0 failed, 0 skipped

| Suite | Tests | Result |
|-------|-------|--------|
| `TradeNormalizerTest` | 7 | ✅ All passed |
| `InstrumentsLoaderTest` | 9 | ✅ All passed |

**Bug fixed**: `InstrumentsLoaderTest` had a compilation error — `File(File)` constructor does not exist in Java. The canonical YAML path lookup wrapped its `File` result in another `File(...)`. Fix: remove the redundant outer wrapper.

```kotlin
// BEFORE (does not compile — no File(File) constructor)
val canonicalYaml = File(
    System.getProperty("user.dir").let { dir ->
        File(dir).parentFile?.parentFile?.resolve("config/instruments.yaml") ?: ...
    }
)

// AFTER (correct — let block already returns File)
val canonicalYaml = System.getProperty("user.dir").let { dir ->
    File(dir).parentFile?.parentFile?.resolve("config/instruments.yaml") ?: ...
}
```

**How to run tests** (no `gradlew` in project — use Docker):
```bash
docker run --rm \
  -v /home/rjdscott/Documents/projects/k2-market-data-platform:/project \
  -w /project/services/feed-handler-kotlin \
  -e GRADLE_USER_HOME=/tmp/.gradle \
  gradle:8.12-jdk21 \
  gradle test --no-daemon --rerun-tasks
```

JUnit XML results: `services/feed-handler-kotlin/build/test-results/test/`

### 8. Spark Container Rebuild

The `k2-spark-iceberg` container was missing `PREFECT_DB_*` env vars (started before they were added to docker-compose). Rebuilt with `--build --no-deps` so the JDBC jar (from Dockerfile) and env vars (from `.env`) are permanently baked in:

```bash
docker compose -f docker-compose.v2.yml up -d --build --no-deps spark-iceberg
```

---

## Schema File Changes

| File | Change |
|------|--------|
| `docker/clickhouse/schema/09-silver-kraken-to-v2.sql` | Rewritten to v2 (sequence_number, Decimal, kafka vendor_data) |
| `docker/clickhouse/schema/10-silver-binance.sql` | Rewritten to v2 (same pattern as kraken/coinbase) |
| `docker/clickhouse/schema/11-bronze-coinbase.sql` | Rewritten to v2 Decimal schema |
| `docker/clickhouse/schema/12-silver-coinbase.sql` | New file: Coinbase → silver_trades MV |
| `docker/iceberg/ddl/02-bronze-tables.sql` | Updated all 3 tables to v2 column names |
| `docker/offload/flows/iceberg_offload_flow.py` | sequence_col fix + full pipeline + v3.1.0 |
| `services/feed-handler-kotlin/src/test/kotlin/.../InstrumentsLoaderTest.kt` | Fixed `File(File)` compilation bug |

---

## What's Next (Priority Order)

### 1. Prefect Schedule Deployment (Phase 5 Step 3)
Deploy the offload flow as a Prefect deployment with 15-minute schedule:
```bash
# Deploy via Prefect CLI
prefect deployment build docker/offload/flows/iceberg_offload_flow.py:iceberg_offload_main \
  --name "k2-iceberg-offload" --interval 900
prefect deployment apply iceberg_offload_main-deployment.yaml
```
Or configure via Prefect UI at `http://localhost:4200`.

### 2. Spark Daily Maintenance (Phase 5 Step 4)
Create a daily Spark job for:
- **02:00 UTC**: File compaction (merge small Parquet files → optimal ~128MB)
- **02:20 UTC**: Snapshot expiry (clean up old Iceberg metadata, 7-day retention)
- **02:30 UTC**: Row count audit (verify ClickHouse vs Iceberg parity)

### 3. Warm-Cold Consistency Validation (Phase 5 Step 5)
Compare row counts across tiers. Sample query:
```sql
-- ClickHouse (warm)
SELECT 'binance' AS exchange, count() FROM k2.bronze_trades_binance;

-- Iceberg (cold) — via Spark
spark.read.format("iceberg").load("cold.bronze_trades_binance").count()
```
Gate criteria: counts match within 1 TTL cycle (7 days for bronze).

### 4. Tag and PR
Once consistency validated:
```bash
git tag v2-phase-5-complete
git push origin v2-phase-5-complete
# Open PR: phase-5-prefect-iceberg-offload → main
```

---

## System State

### Running Services (all healthy)
All 12 v2 services: redpanda, redpanda-init (completed), clickhouse, minio, iceberg-rest, prefect-db, prefect-server, prefect-worker, spark-iceberg, feed-handler-{binance,kraken,coinbase}

### ClickHouse Data (approx, as of session end)
- `bronze_trades_binance`: ~1.5M+ rows (growing)
- `bronze_trades_kraken`: ~40K+ rows (growing)
- `bronze_trades_coinbase`: ~50K+ rows (growing)
- `silver_trades`: all 3 exchanges unified
- `ohlcv_*`: aggregated OHLCV across all timeframes

### Iceberg Watermarks (PostgreSQL `offload_watermarks`)
All 10 tables in `success` status. On next Prefect run, only data newer than last watermark will be offloaded (incremental).

---

**Branch**: `phase-5-prefect-iceberg-offload` (ahead of main by ~25 commits)
**Last commit**: `fix(phase-5): sync silver schema files to v2 + enable full offload pipeline`
