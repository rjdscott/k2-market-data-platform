# Custom Flink Docker Image with Iceberg, Kafka, and S3 Connectors
FROM flink:1.18.1-scala_2.12-java11

# Install jq for JSON parsing in job submission scripts (best practice)
USER root
RUN apt-get update && apt-get install -y jq curl && rm -rf /var/lib/apt/lists/*
USER flink

# Copy Kafka and Iceberg connectors to Flink lib directory
COPY flink-jars/flink-sql-connector-kafka-*.jar /opt/flink/lib/
COPY flink-jars/iceberg-flink-runtime-*.jar /opt/flink/lib/
COPY flink-jars/flink-metrics-prometheus-*.jar /opt/flink/lib/
COPY flink-jars/flink-sql-avro-confluent-registry-*.jar /opt/flink/lib/

# Copy Hadoop client libraries (required by Iceberg connector)
COPY flink-jars/hadoop-client-*.jar /opt/flink/lib/

# Copy commons-logging (required by Hadoop FileSystem)
COPY flink-jars/commons-logging-*.jar /opt/flink/lib/

# Copy S3 filesystem to lib (not plugins) for Iceberg compatibility
# Iceberg needs S3AFileSystem in the main classloader, not isolated in plugins
COPY flink-jars/flink-s3-fs-hadoop-*.jar /opt/flink/lib/

# Copy Hadoop configuration (core-site.xml with S3A credentials)
COPY config/hadoop/core-site.xml /opt/flink/conf/

# Copy Flink SQL job definitions into image (avoid volume mount caching issues)
RUN mkdir -p /opt/flink/sql
COPY config/flink-sql/*.sql /opt/flink/sql/

# Copy job submission script (SQL Gateway best practice)
USER root
RUN mkdir -p /opt/flink/scripts && chown flink:flink /opt/flink/scripts
USER flink
COPY --chown=flink:flink scripts/submit_bronze_job.sh /opt/flink/scripts/
RUN chmod +x /opt/flink/scripts/submit_bronze_job.sh

# Verify JARs and SQL files were copied
RUN echo "=== Flink lib directory ===" && \
    ls -lh /opt/flink/lib/*.jar && \
    echo "=== Flink SQL jobs ===" && \
    ls -lh /opt/flink/sql/ && \
    echo "Flink connector JARs loaded successfully"
